{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c643090b",
   "metadata": {},
   "source": [
    "# 权限配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f20a94c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::847380964353:role/spot-bot-SpotSageMakerExecutionRole-TP8BLT3Z5JJL\n",
      "sagemaker bucket: sagemaker-us-west-2-847380964353\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86540a82",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2967feee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-847380964353/datasets/zhenyun'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset used\n",
    "dataset_name = 'zhenyun'\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'datasets/zhenyun'\n",
    "WORK_DIRECTORY = './data/'\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=s3_prefix)\n",
    "data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc18e5",
   "metadata": {},
   "source": [
    "# 模型参数指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8970e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "# hyperparameters which are passed to the training job\n",
    "hyperparameters={'num_train_epochs': 10,\n",
    "                 'train_file':'/opt/ml/input/data/train/100014.csv',\n",
    "                 'validation_file':'/opt/ml/input/data/test/100014.csv',\n",
    "                 'output_dir':'/opt/ml/model',\n",
    "                 'max_seq_length': 128,\n",
    "                 'model_name_or_path': 'bert-base-chinese',\n",
    "                 'learning_rate': 2e-5,\n",
    "                 'num_train_epochs': 1,\n",
    "                 'per_device_train_batch_size': 32,\n",
    "                 'save_strategy':'epoch',\n",
    "                 'save_total_limit':1,\n",
    "                 }\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='run_glue.py',\n",
    "        source_dir='./scripts',\n",
    "        instance_type='ml.p3.2xlarge',\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        transformers_version='4.6',\n",
    "        pytorch_version='1.7',\n",
    "        py_version='py36',\n",
    "        hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e26474",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bbf4afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-28 14:23:07 Starting - Starting the training job...\n",
      "2022-04-28 14:23:35 Starting - Preparing the instances for trainingProfilerReport-1651155787: InProgress\n",
      ".........\n",
      "2022-04-28 14:24:53 Downloading - Downloading input data...\n",
      "2022-04-28 14:25:31 Training - Downloading the training image..................\n",
      "2022-04-28 14:28:32 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-04-28 14:28:28,857 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-04-28 14:28:28,882 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-04-28 14:28:28,890 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-04-28 14:28:29,238 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting accelerate\n",
      "  Downloading accelerate-0.6.2-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.1.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (3.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->-r requirements.txt (line 5)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r requirements.txt (line 2)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from accelerate->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.1.3->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.1.3->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: accelerate\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.6.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-04-28 14:28:32,435 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"learning_rate\": 2e-05,\n",
      "        \"max_seq_length\": 128,\n",
      "        \"model_name_or_path\": \"bert-base-chinese\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_train_batch_size\": 32,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"save_total_limit\": 1,\n",
      "        \"train_file\": \"/opt/ml/input/data/train/100014.csv\",\n",
      "        \"validation_file\": \"/opt/ml/input/data/test/100014.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-04-28-14-23-06-725\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-847380964353/huggingface-pytorch-training-2022-04-28-14-23-06-725/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_glue\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_glue.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"learning_rate\":2e-05,\"max_seq_length\":128,\"model_name_or_path\":\"bert-base-chinese\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"per_device_train_batch_size\":32,\"save_strategy\":\"epoch\",\"save_total_limit\":1,\"train_file\":\"/opt/ml/input/data/train/100014.csv\",\"validation_file\":\"/opt/ml/input/data/test/100014.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_glue.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_glue\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-847380964353/huggingface-pytorch-training-2022-04-28-14-23-06-725/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"learning_rate\":2e-05,\"max_seq_length\":128,\"model_name_or_path\":\"bert-base-chinese\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"per_device_train_batch_size\":32,\"save_strategy\":\"epoch\",\"save_total_limit\":1,\"train_file\":\"/opt/ml/input/data/train/100014.csv\",\"validation_file\":\"/opt/ml/input/data/test/100014.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-04-28-14-23-06-725\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-847380964353/huggingface-pytorch-training-2022-04-28-14-23-06-725/source/sourcedir.tar.gz\",\"module_name\":\"run_glue\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_glue.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--learning_rate\",\"2e-05\",\"--max_seq_length\",\"128\",\"--model_name_or_path\",\"bert-base-chinese\",\"--num_train_epochs\",\"1\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_train_batch_size\",\"32\",\"--save_strategy\",\"epoch\",\"--save_total_limit\",\"1\",\"--train_file\",\"/opt/ml/input/data/train/100014.csv\",\"--validation_file\",\"/opt/ml/input/data/test/100014.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=bert-base-chinese\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=1\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/train/100014.csv\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FILE=/opt/ml/input/data/test/100014.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 run_glue.py --learning_rate 2e-05 --max_seq_length 128 --model_name_or_path bert-base-chinese --num_train_epochs 1 --output_dir /opt/ml/model --per_device_train_batch_size 32 --save_strategy epoch --save_total_limit 1 --train_file /opt/ml/input/data/train/100014.csv --validation_file /opt/ml/input/data/test/100014.csv\u001b[0m\n",
      "\u001b[34m04/28/2022 14:28:37 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m04/28/2022 14:28:37 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr28_14-28-37_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.EPOCH, save_steps=500, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\u001b[0m\n",
      "\u001b[34m04/28/2022 14:28:37 - INFO - __main__ -   load a local file for train: /opt/ml/input/data/train/100014.csv\u001b[0m\n",
      "\u001b[34m04/28/2022 14:28:37 - INFO - __main__ -   load a local file for validation: /opt/ml/input/data/test/100014.csv\u001b[0m\n",
      "\u001b[34m04/28/2022 14:28:37 - WARNING - datasets.builder -   Using custom data configuration default-ee434320104dc5fc\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-ee434320104dc5fc/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-ee434320104dc5fc/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mtotal labels: ['EPP泡沫', 'T其它', 'T存储', 'T服务器', 'T电脑', 'T网络', 'T网络布线(办公)', '书写工具', '会议桌', '传感器', '低压柜', '保护帽', '保洁', '信号发生器', '光学检测设备 AOI改造和升级', '光谱仪', '其它', '其它 五金件其它', '其它(机械其它）', '其它(气动其它）', '其它(电气其它）', '其它（工具其它）', '其它（非金属材料其它）', '内衬', '冰箱', '冲压设备', '冷却设备', '切片及产品验证', '办公桌', '办公椅', '办公用纸', '功率测试设备', '加热设备', '包材模具相关', '包装设备', '医疗、健康', '印刷品', '压力测量仪', '叉车', '叉车备件', '变压器', '地面', '垫板、垫片', '塑料周转塑料箱', '塑料袋', '外箱', '存储(文件)', '存储(系统)', '实验室工作台', '密封和润滑', '封箱带', '工作台', '工作服装鞋帽', '工作椅', '常用电气', '弹簧', '影像', '性能试验', '恒温恒湿机', '手动', '扎带', '打包带', '打印设备', '投影仪', '排风机', '接插件', '控制器', '支撑板', '数字万用表', '整形设备', '文件储存', '日用杂品', '更衣柜', '服务器(项目)', '机器人', '机床附件和焊接器材', '机械切平设备', '机械脉动试验', '材料', '标签', '气动', '气动执行元件', '气动控制阀', '气动附件', '水槽', '水泵', '洗碗机', '流量测量仪', '测量', '消火栓', '润滑脂', '液压执行元件', '液压控制阀', '液压泵', '液压附件', '清洗类', '清洗设备专用备件', '温度冲击', '温湿度传感器', '灭火器', '灭火用品', '灯具', '炉', '特殊桌面设备', '电动', '电机', '电气仪表', '电气性能测试设备', '电源', '电磁接触器', '电线和电缆', '电话会议设备', '电话系统', '监控探头', '硬度计', '碎纸机', '示波器', '礼品', '空压机', '空调', '空调箱', '立体仓库备件', '管路连接件', '粗糙度仪', '紧固件', '纸护角', '缠绕膜', '耗材', '蒸箱', '螺旋, 带链及齿轮传动件', '衡器', '表面激光成型设备', '计量', '财务用品', '起重, 液压和运输', '车床', '车辆使用的周边设备', '车险', '轴及连接', '轴承', '轴法兰', '运输和起重件', '配电箱', '酸碱溶液', '金属周转器具', '钢材', '钻床', '门窗和家具配件', '阀门', '防冻液', '防护用品', '隔档', '食堂日杂', '马桶', '高位货架备件', '高分子材料分析']\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-04-28 14:28:38,400 >> https://huggingface.co/bert-base-chinese/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4zg0u7mv\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-28 14:28:38,713 >> storing https://huggingface.co/bert-base-chinese/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-28 14:28:38,713 >> creating metadata file for /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:517] 2022-04-28 14:28:38,714 >> loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2022-04-28 14:28:38,715 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\",\n",
      "    \"73\": \"LABEL_73\",\n",
      "    \"74\": \"LABEL_74\",\n",
      "    \"75\": \"LABEL_75\",\n",
      "    \"76\": \"LABEL_76\",\n",
      "    \"77\": \"LABEL_77\",\n",
      "    \"78\": \"LABEL_78\",\n",
      "    \"79\": \"LABEL_79\",\n",
      "    \"80\": \"LABEL_80\",\n",
      "    \"81\": \"LABEL_81\",\n",
      "    \"82\": \"LABEL_82\",\n",
      "    \"83\": \"LABEL_83\",\n",
      "    \"84\": \"LABEL_84\",\n",
      "    \"85\": \"LABEL_85\",\n",
      "    \"86\": \"LABEL_86\",\n",
      "    \"87\": \"LABEL_87\",\n",
      "    \"88\": \"LABEL_88\",\n",
      "    \"89\": \"LABEL_89\",\n",
      "    \"90\": \"LABEL_90\",\n",
      "    \"91\": \"LABEL_91\",\n",
      "    \"92\": \"LABEL_92\",\n",
      "    \"93\": \"LABEL_93\",\n",
      "    \"94\": \"LABEL_94\",\n",
      "    \"95\": \"LABEL_95\",\n",
      "    \"96\": \"LABEL_96\",\n",
      "    \"97\": \"LABEL_97\",\n",
      "    \"98\": \"LABEL_98\",\n",
      "    \"99\": \"LABEL_99\",\n",
      "    \"100\": \"LABEL_100\",\n",
      "    \"101\": \"LABEL_101\",\n",
      "    \"102\": \"LABEL_102\",\n",
      "    \"103\": \"LABEL_103\",\n",
      "    \"104\": \"LABEL_104\",\n",
      "    \"105\": \"LABEL_105\",\n",
      "    \"106\": \"LABEL_106\",\n",
      "    \"107\": \"LABEL_107\",\n",
      "    \"108\": \"LABEL_108\",\n",
      "    \"109\": \"LABEL_109\",\n",
      "    \"110\": \"LABEL_110\",\n",
      "    \"111\": \"LABEL_111\",\n",
      "    \"112\": \"LABEL_112\",\n",
      "    \"113\": \"LABEL_113\",\n",
      "    \"114\": \"LABEL_114\",\n",
      "    \"115\": \"LABEL_115\",\n",
      "    \"116\": \"LABEL_116\",\n",
      "    \"117\": \"LABEL_117\",\n",
      "    \"118\": \"LABEL_118\",\n",
      "    \"119\": \"LABEL_119\",\n",
      "    \"120\": \"LABEL_120\",\n",
      "    \"121\": \"LABEL_121\",\n",
      "    \"122\": \"LABEL_122\",\n",
      "    \"123\": \"LABEL_123\",\n",
      "    \"124\": \"LABEL_124\",\n",
      "    \"125\": \"LABEL_125\",\n",
      "    \"126\": \"LABEL_126\",\n",
      "    \"127\": \"LABEL_127\",\n",
      "    \"128\": \"LABEL_128\",\n",
      "    \"129\": \"LABEL_129\",\n",
      "    \"130\": \"LABEL_130\",\n",
      "    \"131\": \"LABEL_131\",\n",
      "    \"132\": \"LABEL_132\",\n",
      "    \"133\": \"LABEL_133\",\n",
      "    \"134\": \"LABEL_134\",\n",
      "    \"135\": \"LABEL_135\",\n",
      "    \"136\": \"LABEL_136\",\n",
      "    \"137\": \"LABEL_137\",\n",
      "    \"138\": \"LABEL_138\",\n",
      "    \"139\": \"LABEL_139\",\n",
      "    \"140\": \"LABEL_140\",\n",
      "    \"141\": \"LABEL_141\",\n",
      "    \"142\": \"LABEL_142\",\n",
      "    \"143\": \"LABEL_143\",\n",
      "    \"144\": \"LABEL_144\",\n",
      "    \"145\": \"LABEL_145\",\n",
      "    \"146\": \"LABEL_146\",\n",
      "    \"147\": \"LABEL_147\",\n",
      "    \"148\": \"LABEL_148\",\n",
      "    \"149\": \"LABEL_149\",\n",
      "    \"150\": \"LABEL_150\",\n",
      "    \"151\": \"LABEL_151\",\n",
      "    \"152\": \"LABEL_152\",\n",
      "    \"153\": \"LABEL_153\",\n",
      "    \"154\": \"LABEL_154\",\n",
      "    \"155\": \"LABEL_155\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_100\": 100,\n",
      "    \"LABEL_101\": 101,\n",
      "    \"LABEL_102\": 102,\n",
      "    \"LABEL_103\": 103,\n",
      "    \"LABEL_104\": 104,\n",
      "    \"LABEL_105\": 105,\n",
      "    \"LABEL_106\": 106,\n",
      "    \"LABEL_107\": 107,\n",
      "    \"LABEL_108\": 108,\n",
      "    \"LABEL_109\": 109,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_110\": 110,\n",
      "    \"LABEL_111\": 111,\n",
      "    \"LABEL_112\": 112,\n",
      "    \"LABEL_113\": 113,\n",
      "    \"LABEL_114\": 114,\n",
      "    \"LABEL_115\": 115,\n",
      "    \"LABEL_116\": 116,\n",
      "    \"LABEL_117\": 117,\n",
      "    \"LABEL_118\": 118,\n",
      "    \"LABEL_119\": 119,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_120\": 120,\n",
      "    \"LABEL_121\": 121,\n",
      "    \"LABEL_122\": 122,\n",
      "    \"LABEL_123\": 123,\n",
      "    \"LABEL_124\": 124,\n",
      "    \"LABEL_125\": 125,\n",
      "    \"LABEL_126\": 126,\n",
      "    \"LABEL_127\": 127,\n",
      "    \"LABEL_128\": 128,\n",
      "    \"LABEL_129\": 129,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_130\": 130,\n",
      "    \"LABEL_131\": 131,\n",
      "    \"LABEL_132\": 132,\n",
      "    \"LABEL_133\": 133,\n",
      "    \"LABEL_134\": 134,\n",
      "    \"LABEL_135\": 135,\n",
      "    \"LABEL_136\": 136,\n",
      "    \"LABEL_137\": 137,\n",
      "    \"LABEL_138\": 138,\n",
      "    \"LABEL_139\": 139,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_140\": 140,\n",
      "    \"LABEL_141\": 141,\n",
      "    \"LABEL_142\": 142,\n",
      "    \"LABEL_143\": 143,\n",
      "    \"LABEL_144\": 144,\n",
      "    \"LABEL_145\": 145,\n",
      "    \"LABEL_146\": 146,\n",
      "    \"LABEL_147\": 147,\n",
      "    \"LABEL_148\": 148,\n",
      "    \"LABEL_149\": 149,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_150\": 150,\n",
      "    \"LABEL_151\": 151,\n",
      "    \"LABEL_152\": 152,\n",
      "    \"LABEL_153\": 153,\n",
      "    \"LABEL_154\": 154,\n",
      "    \"LABEL_155\": 155,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_73\": 73,\n",
      "    \"LABEL_74\": 74,\n",
      "    \"LABEL_75\": 75,\n",
      "    \"LABEL_76\": 76,\n",
      "    \"LABEL_77\": 77,\n",
      "    \"LABEL_78\": 78,\n",
      "    \"LABEL_79\": 79,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_80\": 80,\n",
      "    \"LABEL_81\": 81,\n",
      "    \"LABEL_82\": 82,\n",
      "    \"LABEL_83\": 83,\n",
      "    \"LABEL_84\": 84,\n",
      "    \"LABEL_85\": 85,\n",
      "    \"LABEL_86\": 86,\n",
      "    \"LABEL_87\": 87,\n",
      "    \"LABEL_88\": 88,\n",
      "    \"LABEL_89\": 89,\n",
      "    \"LABEL_9\": 9,\n",
      "    \"LABEL_90\": 90,\n",
      "    \"LABEL_91\": 91,\n",
      "    \"LABEL_92\": 92,\n",
      "    \"LABEL_93\": 93,\n",
      "    \"LABEL_94\": 94,\n",
      "    \"LABEL_95\": 95,\n",
      "    \"LABEL_96\": 96,\n",
      "    \"LABEL_97\": 97,\n",
      "    \"LABEL_98\": 98,\n",
      "    \"LABEL_99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:517] 2022-04-28 14:28:39,015 >> loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2022-04-28 14:28:39,016 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-04-28 14:28:39,324 >> https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzbcq62ai\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-28 14:28:39,777 >> storing https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-28 14:28:39,777 >> creating metadata file for /root/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-04-28 14:28:40,081 >> https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbfjfervh\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-28 14:28:40,666 >> storing https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-28 14:28:40,666 >> creating metadata file for /root/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-04-28 14:28:41,579 >> https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9vog46a2\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-28 14:28:41,897 >> storing https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-28 14:28:41,897 >> creating metadata file for /root/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-28 14:28:41,898 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-28 14:28:41,898 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-28 14:28:41,898 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-28 14:28:41,898 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-28 14:28:41,898 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-04-28 14:28:42,206 >> https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdas1hk3z\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-28 14:28:49,998 >> storing https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-28 14:28:49,998 >> creating metadata file for /root/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1155] 2022-04-28 14:28:49,998 >> loading weights file https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1331] 2022-04-28 14:28:52,199 >> Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1342] 2022-04-28 14:28:52,199 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m04/28/2022 14:28:53 - INFO - __main__ -   Sample 5238 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 7392, 5606, 3808, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 93, 'sentence1': '隔膜泵', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\u001b[0m\n",
      "\u001b[34m04/28/2022 14:28:53 - INFO - __main__ -   Sample 912 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1461, 1429, 3322, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 35, 'sentence1': '呼吸机', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\u001b[0m\n",
      "\u001b[34m04/28/2022 14:28:53 - INFO - __main__ -   Sample 204 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 3344, 1072, 1947, 6163, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 51, 'sentence1': '杯具套装', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:516] 2022-04-28 14:28:57,459 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1156] 2022-04-28 14:28:57,476 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1157] 2022-04-28 14:28:57,476 >>   Num examples = 7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1158] 2022-04-28 14:28:57,476 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1159] 2022-04-28 14:28:57,476 >>   Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1160] 2022-04-28 14:28:57,477 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1161] 2022-04-28 14:28:57,477 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1162] 2022-04-28 14:28:57,477 >>   Total optimization steps = 225\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:57.760 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:57.918 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:57.919 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:57.919 algo-1:32 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:57.921 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:57.921 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.129 algo-1:32 INFO hook.py:591] name:bert.embeddings.word_embeddings.weight count_params:16226304\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.129 algo-1:32 INFO hook.py:591] name:bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.129 algo-1:32 INFO hook.py:591] name:bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.130 algo-1:32 INFO hook.py:591] name:bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.130 algo-1:32 INFO hook.py:591] name:bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.130 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.130 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.130 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.130 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.130 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.131 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.131 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.131 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.131 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.131 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.132 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.132 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.132 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.132 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.132 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.132 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.133 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.133 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.133 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.133 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.133 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.134 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.134 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.134 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.134 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.135 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.135 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.135 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.135 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.135 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.136 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.136 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.136 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.136 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.137 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.137 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.137 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.137 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.137 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.138 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.138 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.138 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.138 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.138 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.138 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.138 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.138 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.138 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.138 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.139 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.139 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.139 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.139 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.139 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.139 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.140 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.140 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.140 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.140 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.140 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.140 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.140 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.140 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.140 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.141 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.141 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.141 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.141 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.141 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.141 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.141 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.141 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.141 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.142 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.142 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.142 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.142 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.142 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.142 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.142 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.143 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.143 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.143 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.143 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.143 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.143 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.143 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.144 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.144 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.144 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.144 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.144 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.144 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.144 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.144 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.144 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.145 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.145 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.145 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.145 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.145 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.145 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.145 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.146 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.146 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.146 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.146 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.146 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.147 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.147 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.147 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.147 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.147 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.148 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.148 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.148 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.148 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.148 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.148 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.148 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.148 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.148 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.149 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.149 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.149 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.149 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.149 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.149 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.149 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.149 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.149 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.150 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.150 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.150 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.150 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.150 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.150 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.150 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.150 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.151 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.151 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.151 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.151 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.151 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.152 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.152 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.152 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.152 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.152 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.152 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.152 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.152 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.152 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.153 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.153 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.153 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.153 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.153 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.153 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.153 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.153 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.154 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.154 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.154 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.154 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.154 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.154 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.154 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.154 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.155 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.155 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.155 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.155 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.155 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.155 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.155 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.156 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.156 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.156 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.156 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.156 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.156 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.157 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.157 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.157 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.157 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.157 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.158 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.158 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.158 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.158 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.158 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.159 algo-1:32 INFO hook.py:591] name:bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.159 algo-1:32 INFO hook.py:591] name:bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.159 algo-1:32 INFO hook.py:591] name:classifier.weight count_params:119808\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.159 algo-1:32 INFO hook.py:591] name:classifier.bias count_params:156\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.159 algo-1:32 INFO hook.py:593] Total Trainable Params: 102387612\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.159 algo-1:32 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-04-28 14:28:58.163 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-04-28 14:30:36,378 >> Saving model checkpoint to /opt/ml/model/checkpoint-225\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-28 14:30:36,381 >> Configuration saved in /opt/ml/model/checkpoint-225/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-28 14:30:37,338 >> Model weights saved in /opt/ml/model/checkpoint-225/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-28 14:30:37,340 >> tokenizer config file saved in /opt/ml/model/checkpoint-225/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-28 14:30:37,340 >> Special tokens file saved in /opt/ml/model/checkpoint-225/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1352] 2022-04-28 14:30:39,309 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 101.8316, 'train_samples_per_second': 2.21, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-04-28 14:30:39,515 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-28 14:30:39,517 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-28 14:30:40,464 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-28 14:30:40,465 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-28 14:30:40,465 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:907] 2022-04-28 14:30:40,498 >> ***** train metrics *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,498 >>   epoch                      =        1.0\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,498 >>   init_mem_cpu_alloc_delta   =      819MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,498 >>   init_mem_cpu_peaked_delta  =      388MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,498 >>   init_mem_gpu_alloc_delta   =      391MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,498 >>   init_mem_gpu_peaked_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,498 >>   train_mem_cpu_alloc_delta  =      438MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,499 >>   train_mem_cpu_peaked_delta =      185MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,499 >>   train_mem_gpu_alloc_delta  =     1231MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,499 >>   train_mem_gpu_peaked_delta =     3393MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,499 >>   train_runtime              = 0:01:41.83\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,499 >>   train_samples              =       7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,499 >>   train_samples_per_second   =       2.21\u001b[0m\n",
      "\u001b[34m04/28/2022 14:30:40 - INFO - __main__ -   *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:516] 2022-04-28 14:30:40,582 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2115] 2022-04-28 14:30:40,587 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2117] 2022-04-28 14:30:40,587 >>   Num examples = 7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2120] 2022-04-28 14:30:40,587 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:907] 2022-04-28 14:31:38,569 >> ***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   epoch                     =        1.0\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_accuracy             =     0.4642\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_loss                 =     2.7924\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_mem_cpu_alloc_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_mem_cpu_peaked_delta =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_mem_gpu_alloc_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_mem_gpu_peaked_delta =       38MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_runtime              = 0:00:57.89\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_samples              =       7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,571 >>   eval_samples_per_second   =    123.829\u001b[0m\n",
      "\u001b[34m#0150 tables [00:00, ? tables/s]#0151 tables [00:00,  8.80 tables/s]#015                                #015#0150 tables [00:00, ? tables/s]#015                            #015[INFO|file_utils.py:1532] 2022-04-28 14:28:38,400 >> https://huggingface.co/bert-base-chinese/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4zg0u7mv\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/624 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 624/624 [00:00<00:00, 403kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-28 14:28:38,713 >> storing https://huggingface.co/bert-base-chinese/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-28 14:28:38,713 >> creating metadata file for /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:517] 2022-04-28 14:28:38,714 >> loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2022-04-28 14:28:38,715 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\u001b[0m\n",
      "\u001b[34m2022-04-28 14:31:39,498 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\",\n",
      "    \"73\": \"LABEL_73\",\n",
      "    \"74\": \"LABEL_74\",\n",
      "    \"75\": \"LABEL_75\",\n",
      "    \"76\": \"LABEL_76\",\n",
      "    \"77\": \"LABEL_77\",\n",
      "    \"78\": \"LABEL_78\",\n",
      "    \"79\": \"LABEL_79\",\n",
      "    \"80\": \"LABEL_80\",\n",
      "    \"81\": \"LABEL_81\",\n",
      "    \"82\": \"LABEL_82\",\n",
      "    \"83\": \"LABEL_83\",\n",
      "    \"84\": \"LABEL_84\",\n",
      "    \"85\": \"LABEL_85\",\n",
      "    \"86\": \"LABEL_86\",\n",
      "    \"87\": \"LABEL_87\",\n",
      "    \"88\": \"LABEL_88\",\n",
      "    \"89\": \"LABEL_89\",\n",
      "    \"90\": \"LABEL_90\",\n",
      "    \"91\": \"LABEL_91\",\n",
      "    \"92\": \"LABEL_92\",\n",
      "    \"93\": \"LABEL_93\",\n",
      "    \"94\": \"LABEL_94\",\n",
      "    \"95\": \"LABEL_95\",\n",
      "    \"96\": \"LABEL_96\",\n",
      "    \"97\": \"LABEL_97\",\n",
      "    \"98\": \"LABEL_98\",\n",
      "    \"99\": \"LABEL_99\",\n",
      "    \"100\": \"LABEL_100\",\n",
      "    \"101\": \"LABEL_101\",\n",
      "    \"102\": \"LABEL_102\",\n",
      "    \"103\": \"LABEL_103\",\n",
      "    \"104\": \"LABEL_104\",\n",
      "    \"105\": \"LABEL_105\",\n",
      "    \"106\": \"LABEL_106\",\n",
      "    \"107\": \"LABEL_107\",\n",
      "    \"108\": \"LABEL_108\",\n",
      "    \"109\": \"LABEL_109\",\n",
      "    \"110\": \"LABEL_110\",\n",
      "    \"111\": \"LABEL_111\",\n",
      "    \"112\": \"LABEL_112\",\n",
      "    \"113\": \"LABEL_113\",\n",
      "    \"114\": \"LABEL_114\",\n",
      "    \"115\": \"LABEL_115\",\n",
      "    \"116\": \"LABEL_116\",\n",
      "    \"117\": \"LABEL_117\",\n",
      "    \"118\": \"LABEL_118\",\n",
      "    \"119\": \"LABEL_119\",\n",
      "    \"120\": \"LABEL_120\",\n",
      "    \"121\": \"LABEL_121\",\n",
      "    \"122\": \"LABEL_122\",\n",
      "    \"123\": \"LABEL_123\",\n",
      "    \"124\": \"LABEL_124\",\n",
      "    \"125\": \"LABEL_125\",\n",
      "    \"126\": \"LABEL_126\",\n",
      "    \"127\": \"LABEL_127\",\n",
      "    \"128\": \"LABEL_128\",\n",
      "    \"129\": \"LABEL_129\",\n",
      "    \"130\": \"LABEL_130\",\n",
      "    \"131\": \"LABEL_131\",\n",
      "    \"132\": \"LABEL_132\",\n",
      "    \"133\": \"LABEL_133\",\n",
      "    \"134\": \"LABEL_134\",\n",
      "    \"135\": \"LABEL_135\",\n",
      "    \"136\": \"LABEL_136\",\n",
      "    \"137\": \"LABEL_137\",\n",
      "    \"138\": \"LABEL_138\",\n",
      "    \"139\": \"LABEL_139\",\n",
      "    \"140\": \"LABEL_140\",\n",
      "    \"141\": \"LABEL_141\",\n",
      "    \"142\": \"LABEL_142\",\n",
      "    \"143\": \"LABEL_143\",\n",
      "    \"144\": \"LABEL_144\",\n",
      "    \"145\": \"LABEL_145\",\n",
      "    \"146\": \"LABEL_146\",\n",
      "    \"147\": \"LABEL_147\",\n",
      "    \"148\": \"LABEL_148\",\n",
      "    \"149\": \"LABEL_149\",\n",
      "    \"150\": \"LABEL_150\",\n",
      "    \"151\": \"LABEL_151\",\n",
      "    \"152\": \"LABEL_152\",\n",
      "    \"153\": \"LABEL_153\",\n",
      "    \"154\": \"LABEL_154\",\n",
      "    \"155\": \"LABEL_155\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_100\": 100,\n",
      "    \"LABEL_101\": 101,\n",
      "    \"LABEL_102\": 102,\n",
      "    \"LABEL_103\": 103,\n",
      "    \"LABEL_104\": 104,\n",
      "    \"LABEL_105\": 105,\n",
      "    \"LABEL_106\": 106,\n",
      "    \"LABEL_107\": 107,\n",
      "    \"LABEL_108\": 108,\n",
      "    \"LABEL_109\": 109,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_110\": 110,\n",
      "    \"LABEL_111\": 111,\n",
      "    \"LABEL_112\": 112,\n",
      "    \"LABEL_113\": 113,\n",
      "    \"LABEL_114\": 114,\n",
      "    \"LABEL_115\": 115,\n",
      "    \"LABEL_116\": 116,\n",
      "    \"LABEL_117\": 117,\n",
      "    \"LABEL_118\": 118,\n",
      "    \"LABEL_119\": 119,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_120\": 120,\n",
      "    \"LABEL_121\": 121,\n",
      "    \"LABEL_122\": 122,\n",
      "    \"LABEL_123\": 123,\n",
      "    \"LABEL_124\": 124,\n",
      "    \"LABEL_125\": 125,\n",
      "    \"LABEL_126\": 126,\n",
      "    \"LABEL_127\": 127,\n",
      "    \"LABEL_128\": 128,\n",
      "    \"LABEL_129\": 129,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_130\": 130,\n",
      "    \"LABEL_131\": 131,\n",
      "    \"LABEL_132\": 132,\n",
      "    \"LABEL_133\": 133,\n",
      "    \"LABEL_134\": 134,\n",
      "    \"LABEL_135\": 135,\n",
      "    \"LABEL_136\": 136,\n",
      "    \"LABEL_137\": 137,\n",
      "    \"LABEL_138\": 138,\n",
      "    \"LABEL_139\": 139,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_140\": 140,\n",
      "    \"LABEL_141\": 141,\n",
      "    \"LABEL_142\": 142,\n",
      "    \"LABEL_143\": 143,\n",
      "    \"LABEL_144\": 144,\n",
      "    \"LABEL_145\": 145,\n",
      "    \"LABEL_146\": 146,\n",
      "    \"LABEL_147\": 147,\n",
      "    \"LABEL_148\": 148,\n",
      "    \"LABEL_149\": 149,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_150\": 150,\n",
      "    \"LABEL_151\": 151,\n",
      "    \"LABEL_152\": 152,\n",
      "    \"LABEL_153\": 153,\n",
      "    \"LABEL_154\": 154,\n",
      "    \"LABEL_155\": 155,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_73\": 73,\n",
      "    \"LABEL_74\": 74,\n",
      "    \"LABEL_75\": 75,\n",
      "    \"LABEL_76\": 76,\n",
      "    \"LABEL_77\": 77,\n",
      "    \"LABEL_78\": 78,\n",
      "    \"LABEL_79\": 79,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_80\": 80,\n",
      "    \"LABEL_81\": 81,\n",
      "    \"LABEL_82\": 82,\n",
      "    \"LABEL_83\": 83,\n",
      "    \"LABEL_84\": 84,\n",
      "    \"LABEL_85\": 85,\n",
      "    \"LABEL_86\": 86,\n",
      "    \"LABEL_87\": 87,\n",
      "    \"LABEL_88\": 88,\n",
      "    \"LABEL_89\": 89,\n",
      "    \"LABEL_9\": 9,\n",
      "    \"LABEL_90\": 90,\n",
      "    \"LABEL_91\": 91,\n",
      "    \"LABEL_92\": 92,\n",
      "    \"LABEL_93\": 93,\n",
      "    \"LABEL_94\": 94,\n",
      "    \"LABEL_95\": 95,\n",
      "    \"LABEL_96\": 96,\n",
      "    \"LABEL_97\": 97,\n",
      "    \"LABEL_98\": 98,\n",
      "    \"LABEL_99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:517] 2022-04-28 14:28:39,015 >> loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2022-04-28 14:28:39,016 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-04-28 14:28:39,324 >> https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzbcq62ai\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]#015Downloading:  83%|████████▎ | 91.1k/110k [00:00<00:00, 771kB/s]#015Downloading: 100%|██████████| 110k/110k [00:00<00:00, 907kB/s] \u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-28 14:28:39,777 >> storing https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-28 14:28:39,777 >> creating metadata file for /root/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-04-28 14:28:40,081 >> https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbfjfervh\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/269k [00:00<?, ?B/s]#015Downloading:  37%|███▋      | 98.3k/269k [00:00<00:00, 732kB/s]#015Downloading: 100%|██████████| 269k/269k [00:00<00:00, 1.31MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-28 14:28:40,666 >> storing https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-28 14:28:40,666 >> creating metadata file for /root/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-04-28 14:28:41,579 >> https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9vog46a2\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 15.7kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-28 14:28:41,897 >> storing https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-28 14:28:41,897 >> creating metadata file for /root/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-28 14:28:41,898 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-28 14:28:41,898 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-28 14:28:41,898 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-28 14:28:41,898 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-28 14:28:41,898 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-04-28 14:28:42,206 >> https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdas1hk3z\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/412M [00:00<?, ?B/s]#015Downloading:   1%|          | 4.22M/412M [00:00<00:09, 42.2MB/s]#015Downloading:   2%|▏         | 9.19M/412M [00:00<00:09, 44.2MB/s]#015Downloading:   3%|▎         | 14.3M/412M [00:00<00:08, 46.1MB/s]#015Downloading:   5%|▍         | 19.3M/412M [00:00<00:08, 47.2MB/s]#015Downloading:   6%|▌         | 24.6M/412M [00:00<00:07, 48.8MB/s]#015Downloading:   7%|▋         | 29.6M/412M [00:00<00:07, 49.1MB/s]#015Downloading:   9%|▊         | 35.0M/412M [00:00<00:07, 50.4MB/s]#015Downloading:  10%|▉         | 40.8M/412M [00:00<00:07, 52.5MB/s]#015Downloading:  11%|█▏        | 46.7M/412M [00:00<00:06, 54.2MB/s]#015Downloading:  13%|█▎        | 52.5M/412M [00:01<00:06, 55.2MB/s]#015Downloading:  14%|█▍        | 58.2M/412M [00:01<00:06, 55.9MB/s]#015Downloading:  15%|█▌        | 63.8M/412M [00:01<00:06, 55.2MB/s]#015Downloading:  17%|█▋        | 69.2M/412M [00:01<00:06, 54.7MB/s]#015Downloading:  18%|█▊        | 74.7M/412M [00:01<00:06, 54.3MB/s]#015Downloading:  19%|█▉        | 80.1M/412M [00:01<00:06, 54.3MB/s]#015Downloading:  21%|██        | 85.5M/412M [00:01<00:06, 54.3MB/s]#015Downloading:  22%|██▏       | 91.0M/412M [00:01<00:05, 54.0MB/s]#015Downloading:  23%|██▎       | 96.4M/412M [00:01<00:05, 54.0MB/s]#015Downloading:  25%|██▍       | 102M/412M [00:01<00:05, 54.0MB/s] #015Downloading:  26%|██▌       | 107M/412M [00:02<00:05, 54.2MB/s]#015Downloading:  27%|██▋       | 113M/412M [00:02<00:05, 54.1MB/s]#015Downloading:  29%|██▊       | 118M/412M [00:02<00:05, 54.0MB/s]#015Downloading:  30%|███       | 123M/412M [00:02<00:05, 54.2MB/s]#015Downloading:  31%|███▏      | 129M/412M [00:02<00:05, 54.4MB/s]#015Downloading:  33%|███▎      | 134M/412M [00:02<00:05, 54.6MB/s]#015Downloading:  34%|███▍      | 140M/412M [00:02<00:04, 54.6MB/s]#015Downloading:  35%|███▌      | 145M/412M [00:02<00:04, 53.4MB/s]#015Downloading:  37%|███▋      | 151M/412M [00:02<00:04, 53.3MB/s]#015Downloading:  38%|███▊      | 156M/412M [00:02<00:05, 49.3MB/s]#015Downloading:  39%|███▉      | 161M/412M [00:03<00:04, 50.5MB/s]#015Downloading:  41%|████      | 167M/412M [00:03<00:04, 51.5MB/s]#015Downloading:  42%|████▏     | 172M/412M [00:03<00:04, 52.3MB/s]#015Downloading:  43%|████▎     | 178M/412M [00:03<00:04, 52.8MB/s]#015Downloading:  44%|████▍     | 183M/412M [00:03<00:04, 53.3MB/s]#015Downloading:  46%|████▌     | 189M/412M [00:03<00:04, 53.7MB/s]#015Downloading:  47%|████▋     | 194M/412M [00:03<00:04, 53.5MB/s]#015Downloading:  49%|████▊     | 200M/412M [00:03<00:03, 55.2MB/s]#015Downloading:  50%|█████     | 206M/412M [00:03<00:03, 56.9MB/s]#015Downloading:  52%|█████▏    | 212M/412M [00:03<00:03, 57.5MB/s]#015Downloading:  53%|█████▎    | 218M/412M [00:04<00:03, 58.1MB/s]#015Downloading:  54%|█████▍    | 224M/412M [00:04<00:03, 58.4MB/s]#015Downloading:  56%|█████▌    | 230M/412M [00:04<00:03, 58.7MB/s]#015Downloading:  57%|█████▋    | 236M/412M [00:04<00:03, 58.4MB/s]#015Downloading:  59%|█████▊    | 242M/412M [00:04<00:02, 58.8MB/s]#015Downloading:  60%|██████    | 248M/412M [00:04<00:03, 48.1MB/s]#015Downloading:  61%|██████▏   | 253M/412M [00:04<00:03, 49.7MB/s]#015Downloading:  63%|██████▎   | 258M/412M [00:04<00:03, 50.9MB/s]#015Downloading:  64%|██████▍   | 264M/412M [00:04<00:03, 49.3MB/s]#015Downloading:  65%|██████▌   | 269M/412M [00:05<00:02, 48.7MB/s]#015Downloading:  67%|██████▋   | 274M/412M [00:05<00:02, 49.6MB/s]#015Downloading:  68%|██████▊   | 279M/412M [00:05<00:02, 51.1MB/s]#015Downloading:  69%|██████▉   | 285M/412M [00:05<00:02, 49.8MB/s]#015Downloading:  70%|███████   | 290M/412M [00:05<00:02, 50.8MB/s]#015Downloading:  72%|███████▏  | 295M/412M [00:05<00:02, 51.9MB/s]#015Downloading:  73%|███████▎  | 301M/412M [00:05<00:02, 51.4MB/s]#015Downloading:  74%|███████▍  | 306M/412M [00:05<00:02, 50.1MB/s]#015Downloading:  76%|███████▌  | 311M/412M [00:05<00:02, 50.0MB/s]#015Downloading:  77%|███████▋  | 317M/412M [00:05<00:01, 52.5MB/s]#015Downloading:  78%|███████▊  | 323M/412M [00:06<00:01, 54.1MB/s]#015Downloading:  80%|███████▉  | 328M/412M [00:06<00:01, 55.4MB/s]#015Downloading:  81%|████████  | 334M/412M [00:06<00:01, 56.6MB/s]#015Downloading:  83%|████████▎ | 340M/412M [00:06<00:01, 54.4MB/s]#015Downloading:  84%|████████▍ | 346M/412M [00:06<00:01, 54.5MB/s]#015Downloading:  85%|████████▌ | 351M/412M [00:06<00:01, 54.6MB/s]#015Downloading:  87%|████████▋ | 357M/412M [00:06<00:01, 53.0MB/s]#015Downloading:  88%|████████▊ | 362M/412M [00:06<00:00, 52.5MB/s]#015Downloading:  89%|████████▉ | 367M/412M [00:06<00:00, 53.1MB/s]#015Downloading:  91%|█████████ | 373M/412M [00:06<00:00, 53.6MB/s]#015Downloading:  92%|█████████▏| 378M/412M [00:07<00:00, 54.0MB/s]#015Downloading:  93%|█████████▎| 384M/412M [00:07<00:00, 54.2MB/s]#015Downloading:  95%|█████████▍| 389M/412M [00:07<00:00, 49.4MB/s]#015Downloading:  96%|█████████▌| 395M/412M [00:07<00:00, 51.0MB/s]#015Downloading:  97%|█████████▋| 400M/412M [00:07<00:00, 52.2MB/s]#015Downloading:  99%|█████████▊| 406M/412M [00:07<00:00, 52.9MB/s]#015Downloading: 100%|█████████▉| 411M/412M [00:07<00:00, 53.4MB/s]#015Downloading: 100%|██████████| 412M/412M [00:07<00:00, 53.2MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-28 14:28:49,998 >> storing https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-28 14:28:49,998 >> creating metadata file for /root/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1155] 2022-04-28 14:28:49,998 >> loading weights file https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1331] 2022-04-28 14:28:52,199 >> Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1342] 2022-04-28 14:28:52,199 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/8 [00:00<?, ?ba/s]#015 12%|█▎        | 1/8 [00:00<00:00,  7.20ba/s]#015 38%|███▊      | 3/8 [00:00<00:00,  8.86ba/s]#015 75%|███████▌  | 6/8 [00:00<00:00, 10.66ba/s]#015100%|██████████| 8/8 [00:00<00:00, 17.88ba/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/8 [00:00<?, ?ba/s]#015 25%|██▌       | 2/8 [00:00<00:00, 16.67ba/s]#015 62%|██████▎   | 5/8 [00:00<00:00, 15.67ba/s]#015100%|██████████| 8/8 [00:00<00:00, 18.28ba/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:516] 2022-04-28 14:28:57,459 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1156] 2022-04-28 14:28:57,476 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1157] 2022-04-28 14:28:57,476 >>   Num examples = 7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1158] 2022-04-28 14:28:57,476 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1159] 2022-04-28 14:28:57,476 >>   Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1160] 2022-04-28 14:28:57,477 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1161] 2022-04-28 14:28:57,477 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1162] 2022-04-28 14:28:57,477 >>   Total optimization steps = 225\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/225 [00:00<?, ?it/s]#015  0%|          | 1/225 [00:02<08:58,  2.40s/it]#015  1%|          | 2/225 [00:02<06:49,  1.84s/it]#015  1%|▏         | 3/225 [00:03<05:14,  1.42s/it]#015  2%|▏         | 4/225 [00:03<04:08,  1.12s/it]#015  2%|▏         | 5/225 [00:04<03:26,  1.06it/s]#015  3%|▎         | 6/225 [00:05<03:26,  1.06it/s]#015  3%|▎         | 7/225 [00:06<03:25,  1.06it/s]#015  4%|▎         | 8/225 [00:06<02:53,  1.25it/s]#015  4%|▍         | 9/225 [00:07<02:30,  1.43it/s]#015  4%|▍         | 10/225 [00:07<02:18,  1.55it/s]#015  5%|▍         | 11/225 [00:08<02:06,  1.69it/s]#015  5%|▌         | 12/225 [00:08<01:58,  1.80it/s]#015  6%|▌         | 13/225 [00:09<01:49,  1.94it/s]#015  6%|▌         | 14/225 [00:09<01:41,  2.07it/s]#015  7%|▋         | 15/225 [00:09<01:36,  2.17it/s]#015  7%|▋         | 16/225 [00:10<01:41,  2.06it/s]#015  8%|▊         | 17/225 [00:10<01:35,  2.17it/s]#015  8%|▊         | 18/225 [00:11<01:31,  2.25it/s]#015  8%|▊         | 19/225 [00:11<01:30,  2.29it/s]#015  9%|▉         | 20/225 [00:12<01:30,  2.26it/s]#015  9%|▉         | 21/225 [00:12<01:28,  2.30it/s]#015 10%|▉         | 22/225 [00:12<01:26,  2.34it/s]#015 10%|█         | 23/225 [00:13<01:25,  2.36it/s]#015 11%|█         | 24/225 [00:13<01:25,  2.36it/s]#015 11%|█         | 25/225 [00:14<01:23,  2.38it/s]#015 12%|█▏        | 26/225 [00:14<01:23,  2.39it/s]#015 12%|█▏        | 27/225 [00:14<01:23,  2.38it/s]#015 12%|█▏        | 28/225 [00:15<01:22,  2.38it/s]#015 13%|█▎        | 29/225 [00:15<01:21,  2.39it/s]#015 13%|█▎        | 30/225 [00:16<01:21,  2.38it/s]#015 14%|█▍        | 31/225 [00:16<01:23,  2.34it/s]#015 14%|█▍        | 32/225 [00:17<01:22,  2.34it/s]#015 15%|█▍        | 33/225 [00:17<01:23,  2.30it/s]#015 15%|█▌        | 34/225 [00:17<01:22,  2.31it/s]#015 16%|█▌        | 35/225 [00:18<01:24,  2.24it/s]#015 16%|█▌        | 36/225 [00:18<01:22,  2.28it/s]#015 16%|█▋        | 37/225 [00:19<01:21,  2.30it/s]#015 17%|█▋        | 38/225 [00:19<01:20,  2.33it/s]#015 17%|█▋        | 39/225 [00:20<01:19,  2.33it/s]#015 18%|█▊        | 40/225 [00:20<01:17,  2.37it/s]#015 18%|█▊        | 41/225 [00:21<01:20,  2.29it/s]#015 19%|█▊        | 42/225 [00:21<01:21,  2.25it/s]#015 19%|█▉        | 43/225 [00:21<01:19,  2.29it/s]#015 20%|█▉        | 44/225 [00:22<01:21,  2.22it/s]#015 20%|██        | 45/225 [00:22<01:18,  2.30it/s]#015 20%|██        | 46/225 [00:23<01:17,  2.31it/s]#015 21%|██        | 47/225 [00:23<01:16,  2.32it/s]#015 21%|██▏       | 48/225 [00:24<01:15,  2.35it/s]#015 22%|██▏       | 49/225 [00:24<01:13,  2.39it/s]#015 22%|██▏       | 50/225 [00:24<01:12,  2.40it/s]#015 23%|██▎       | 51/225 [00:25<01:14,  2.33it/s]#015 23%|██▎       | 52/225 [00:25<01:14,  2.31it/s]#015 24%|██▎       | 53/225 [00:26<01:13,  2.34it/s]#015 24%|██▍       | 54/225 [00:26<01:13,  2.34it/s]#015 24%|██▍       | 55/225 [00:27<01:10,  2.40it/s]#015 25%|██▍       | 56/225 [00:27<01:09,  2.42it/s]#015 25%|██▌       | 57/225 [00:27<01:08,  2.44it/s]#015 26%|██▌       | 58/225 [00:28<01:08,  2.44it/s]#015 26%|██▌       | 59/225 [00:28<01:08,  2.43it/s]#015 27%|██▋       | 60/225 [00:29<01:07,  2.43it/s]#015 27%|██▋       | 61/225 [00:29<01:07,  2.43it/s]#015 28%|██▊       | 62/225 [00:29<01:07,  2.42it/s]#015 28%|██▊       | 63/225 [00:30<01:06,  2.45it/s]#015 28%|██▊       | 64/225 [00:30<01:05,  2.47it/s]#015 29%|██▉       | 65/225 [00:31<01:06,  2.41it/s]#015 29%|██▉       | 66/225 [00:31<01:05,  2.43it/s]#015 30%|██▉       | 67/225 [00:31<01:04,  2.45it/s]#015 30%|███       | 68/225 [00:32<01:04,  2.43it/s]#015 31%|███       | 69/225 [00:32<01:07,  2.31it/s]#015 31%|███       | 70/225 [00:33<01:06,  2.33it/s]#015 32%|███▏      | 71/225 [00:33<01:04,  2.39it/s]#015 32%|███▏      | 72/225 [00:34<01:04,  2.38it/s]#015 32%|███▏      | 73/225 [00:34<01:04,  2.34it/s]#015 33%|███▎      | 74/225 [00:34<01:04,  2.34it/s]#015 33%|███▎      | 75/225 [00:35<01:04,  2.32it/s]#015 34%|███▍      | 76/225 [00:35<01:03,  2.34it/s]#015 34%|███▍      | 77/225 [00:36<01:03,  2.34it/s]#015 35%|███▍      | 78/225 [00:36<01:02,  2.35it/s]#015 35%|███▌      | 79/225 [00:37<01:02,  2.35it/s]#015 36%|███▌      | 80/225 [00:37<01:02,  2.33it/s]#015 36%|███▌      | 81/225 [00:37<01:00,  2.36it/s]#015 36%|███▋      | 82/225 [00:38<00:59,  2.40it/s]#015 37%|███▋      | 83/225 [00:38<01:00,  2.36it/s]#015 37%|███▋      | 84/225 [00:39<01:00,  2.33it/s]#015 38%|███▊      | 85/225 [00:39<01:01,  2.27it/s]#015 38%|███▊      | 86/225 [00:40<01:00,  2.30it/s]#015 39%|███▊      | 87/225 [00:40<00:59,  2.32it/s]#015 39%|███▉      | 88/225 [00:40<00:57,  2.37it/s]#015 40%|███▉      | 89/225 [00:41<00:58,  2.33it/s]#015 40%|████      | 90/225 [00:41<00:57,  2.35it/s]#015 40%|████      | 91/225 [00:42<00:56,  2.38it/s]#015 41%|████      | 92/225 [00:42<00:56,  2.37it/s]#015 41%|████▏     | 93/225 [00:43<00:55,  2.38it/s]#015 42%|████▏     | 94/225 [00:43<00:55,  2.38it/s]#015 42%|████▏     | 95/225 [00:43<00:54,  2.39it/s]#015 43%|████▎     | 96/225 [00:44<00:54,  2.36it/s]#015 43%|████▎     | 97/225 [00:44<00:54,  2.37it/s]#015 44%|████▎     | 98/225 [00:45<00:52,  2.41it/s]#015 44%|████▍     | 99/225 [00:45<00:52,  2.39it/s]#015 44%|████▍     | 100/225 [00:45<00:53,  2.35it/s]#015 45%|████▍     | 101/225 [00:46<00:52,  2.36it/s]#015 45%|████▌     | 102/225 [00:46<00:51,  2.39it/s]#015 46%|████▌     | 103/225 [00:47<00:50,  2.41it/s]#015 46%|████▌     | 104/225 [00:47<00:50,  2.39it/s]#015 47%|████▋     | 105/225 [00:48<00:49,  2.42it/s]#015 47%|████▋     | 106/225 [00:48<00:49,  2.38it/s]#015 48%|████▊     | 107/225 [00:48<00:49,  2.40it/s]#015 48%|████▊     | 108/225 [00:49<00:49,  2.36it/s]#015 48%|████▊     | 109/225 [00:49<00:50,  2.29it/s]#015 49%|████▉     | 110/225 [00:50<00:50,  2.27it/s]#015 49%|████▉     | 111/225 [00:50<00:49,  2.30it/s]#015 50%|████▉     | 112/225 [00:51<00:48,  2.35it/s]#015 50%|█████     | 113/225 [00:51<00:47,  2.37it/s]#015 51%|█████     | 114/225 [00:51<00:46,  2.36it/s]#015 51%|█████     | 115/225 [00:52<00:46,  2.38it/s]#015 52%|█████▏    | 116/225 [00:52<00:45,  2.37it/s]#015 52%|█████▏    | 117/225 [00:53<00:44,  2.43it/s]#015 52%|█████▏    | 118/225 [00:53<00:44,  2.43it/s]#015 53%|█████▎    | 119/225 [00:53<00:44,  2.39it/s]#015 53%|█████▎    | 120/225 [00:54<00:44,  2.36it/s]#015 54%|█████▍    | 121/225 [00:54<00:44,  2.36it/s]#015 54%|█████▍    | 122/225 [00:55<00:43,  2.36it/s]#015 55%|█████▍    | 123/225 [00:55<00:43,  2.37it/s]#015 55%|█████▌    | 124/225 [00:56<00:42,  2.38it/s]#015 56%|█████▌    | 125/225 [00:56<00:41,  2.39it/s]#015 56%|█████▌    | 126/225 [00:56<00:40,  2.42it/s]#015 56%|█████▋    | 127/225 [00:57<00:40,  2.43it/s]#015 57%|█████▋    | 128/225 [00:57<00:39,  2.45it/s]#015 57%|█████▋    | 129/225 [00:58<00:39,  2.41it/s]#015 58%|█████▊    | 130/225 [00:58<00:39,  2.42it/s]#015 58%|█████▊    | 131/225 [00:58<00:38,  2.42it/s]#015 59%|█████▊    | 132/225 [00:59<00:40,  2.32it/s]#015 59%|█████▉    | 133/225 [00:59<00:39,  2.33it/s]#015 60%|█████▉    | 134/225 [01:00<00:39,  2.31it/s]#015 60%|██████    | 135/225 [01:00<00:38,  2.33it/s]#015 60%|██████    | 136/225 [01:01<00:38,  2.30it/s]#015 61%|██████    | 137/225 [01:01<00:37,  2.33it/s]#015 61%|██████▏   | 138/225 [01:01<00:36,  2.36it/s]#015 62%|██████▏   | 139/225 [01:02<00:36,  2.38it/s]#015 62%|██████▏   | 140/225 [01:02<00:36,  2.33it/s]#015 63%|██████▎   | 141/225 [01:03<00:36,  2.33it/s]#015 63%|██████▎   | 142/225 [01:03<00:35,  2.35it/s]#015 64%|██████▎   | 143/225 [01:04<00:35,  2.30it/s]#015 64%|██████▍   | 144/225 [01:04<00:34,  2.36it/s]#015 64%|██████▍   | 145/225 [01:04<00:33,  2.38it/s]#015 65%|██████▍   | 146/225 [01:05<00:33,  2.34it/s]#015 65%|██████▌   | 147/225 [01:05<00:33,  2.30it/s]#015 66%|██████▌   | 148/225 [01:06<00:33,  2.32it/s]#015 66%|██████▌   | 149/225 [01:06<00:33,  2.28it/s]#015 67%|██████▋   | 150/225 [01:07<00:32,  2.29it/s]#015 67%|██████▋   | 151/225 [01:07<00:32,  2.25it/s]#015 68%|██████▊   | 152/225 [01:08<00:32,  2.27it/s]#015 68%|██████▊   | 153/225 [01:08<00:31,  2.31it/s]#015 68%|██████▊   | 154/225 [01:08<00:30,  2.34it/s]#015 69%|██████▉   | 155/225 [01:09<00:29,  2.36it/s]#015 69%|██████▉   | 156/225 [01:09<00:29,  2.37it/s]#015 70%|██████▉   | 157/225 [01:10<00:28,  2.37it/s]#015 70%|███████   | 158/225 [01:10<00:28,  2.39it/s]#015 71%|███████   | 159/225 [01:10<00:27,  2.39it/s]#015 71%|███████   | 160/225 [01:11<00:27,  2.39it/s]#015 72%|███████▏  | 161/225 [01:11<00:26,  2.38it/s]#015 72%|███████▏  | 162/225 [01:12<00:26,  2.39it/s]#015 72%|███████▏  | 163/225 [01:12<00:25,  2.42it/s]#015 73%|███████▎  | 164/225 [01:13<00:25,  2.35it/s]#015 73%|███████▎  | 165/225 [01:13<00:25,  2.34it/s]#015 74%|███████▍  | 166/225 [01:13<00:24,  2.40it/s]#015 74%|███████▍  | 167/225 [01:14<00:23,  2.43it/s]#015 75%|███████▍  | 168/225 [01:14<00:23,  2.42it/s]#015 75%|███████▌  | 169/225 [01:15<00:23,  2.40it/s]#015 76%|███████▌  | 170/225 [01:15<00:23,  2.30it/s]#015 76%|███████▌  | 171/225 [01:16<00:23,  2.28it/s]#015 76%|███████▋  | 172/225 [01:16<00:22,  2.32it/s]#015 77%|███████▋  | 173/225 [01:16<00:22,  2.33it/s]#015 77%|███████▋  | 174/225 [01:17<00:21,  2.34it/s]#015 78%|███████▊  | 175/225 [01:17<00:21,  2.36it/s]#015 78%|███████▊  | 176/225 [01:18<00:20,  2.41it/s]#015 79%|███████▊  | 177/225 [01:18<00:19,  2.43it/s]#015 79%|███████▉  | 178/225 [01:18<00:19,  2.41it/s]#015 80%|███████▉  | 179/225 [01:19<00:19,  2.40it/s]#015 80%|████████  | 180/225 [01:19<00:18,  2.41it/s]#015 80%|████████  | 181/225 [01:20<00:18,  2.38it/s]#015 81%|████████  | 182/225 [01:20<00:17,  2.42it/s]#015 81%|████████▏ | 183/225 [01:21<00:18,  2.30it/s]#015 82%|████████▏ | 184/225 [01:21<00:17,  2.33it/s]#015 82%|████████▏ | 185/225 [01:22<00:17,  2.28it/s]#015 83%|████████▎ | 186/225 [01:22<00:16,  2.31it/s]#015 83%|████████▎ | 187/225 [01:22<00:16,  2.36it/s]#015 84%|████████▎ | 188/225 [01:23<00:15,  2.38it/s]#015 84%|████████▍ | 189/225 [01:23<00:15,  2.35it/s]#015 84%|████████▍ | 190/225 [01:24<00:15,  2.27it/s]#015 85%|████████▍ | 191/225 [01:24<00:14,  2.28it/s]#015 85%|████████▌ | 192/225 [01:25<00:14,  2.28it/s]#015 86%|████████▌ | 193/225 [01:25<00:13,  2.31it/s]#015 86%|████████▌ | 194/225 [01:25<00:13,  2.32it/s]#015 87%|████████▋ | 195/225 [01:26<00:12,  2.37it/s]#015 87%|████████▋ | 196/225 [01:26<00:12,  2.37it/s]#015 88%|████████▊ | 197/225 [01:27<00:12,  2.27it/s]#015 88%|████████▊ | 198/225 [01:27<00:11,  2.31it/s]#015 88%|████████▊ | 199/225 [01:28<00:11,  2.35it/s]#015 89%|████████▉ | 200/225 [01:28<00:10,  2.37it/s]#015 89%|████████▉ | 201/225 [01:28<00:10,  2.36it/s]#015 90%|████████▉ | 202/225 [01:29<00:09,  2.34it/s]#015 90%|█████████ | 203/225 [01:29<00:09,  2.38it/s]#015 91%|█████████ | 204/225 [01:30<00:08,  2.36it/s]#015 91%|█████████ | 205/225 [01:30<00:08,  2.40it/s]#015 92%|█████████▏| 206/225 [01:30<00:07,  2.40it/s]#015 92%|█████████▏| 207/225 [01:31<00:07,  2.43it/s]#015 92%|█████████▏| 208/225 [01:31<00:06,  2.43it/s]#015 93%|█████████▎| 209/225 [01:32<00:06,  2.44it/s]#015 93%|█████████▎| 210/225 [01:32<00:06,  2.45it/s]#015 94%|█████████▍| 211/225 [01:33<00:05,  2.36it/s]#015 94%|█████████▍| 212/225 [01:33<00:05,  2.32it/s]#015 95%|█████████▍| 213/225 [01:33<00:05,  2.37it/s]#015 95%|█████████▌| 214/225 [01:34<00:04,  2.37it/s]#015 96%|█████████▌| 215/225 [01:34<00:04,  2.38it/s]#015 96%|█████████▌| 216/225 [01:35<00:03,  2.38it/s]#015 96%|█████████▋| 217/225 [01:35<00:03,  2.33it/s]#015 97%|█████████▋| 218/225 [01:36<00:03,  2.32it/s]#015 97%|█████████▋| 219/225 [01:36<00:02,  2.34it/s]#015 98%|█████████▊| 220/225 [01:36<00:02,  2.35it/s]#015 98%|█████████▊| 221/225 [01:37<00:01,  2.36it/s]#015 99%|█████████▊| 222/225 [01:37<00:01,  2.33it/s]#015 99%|█████████▉| 223/225 [01:38<00:00,  2.30it/s]#015100%|█████████▉| 224/225 [01:38<00:00,  2.32it/s]#015100%|██████████| 225/225 [01:38<00:00,  2.52it/s][INFO|trainer.py:1885] 2022-04-28 14:30:36,378 >> Saving model checkpoint to /opt/ml/model/checkpoint-225\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-28 14:30:36,381 >> Configuration saved in /opt/ml/model/checkpoint-225/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-28 14:30:37,338 >> Model weights saved in /opt/ml/model/checkpoint-225/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-28 14:30:37,340 >> tokenizer config file saved in /opt/ml/model/checkpoint-225/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-28 14:30:37,340 >> Special tokens file saved in /opt/ml/model/checkpoint-225/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1352] 2022-04-28 14:30:39,309 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m#015                                                 #015#015100%|██████████| 225/225 [01:41<00:00,  2.52it/s]#015100%|██████████| 225/225 [01:41<00:00,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-04-28 14:30:39,515 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-28 14:30:39,517 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-28 14:30:40,464 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-28 14:30:40,465 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-28 14:30:40,465 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:907] 2022-04-28 14:30:40,498 >> ***** train metrics *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,498 >>   epoch                      =        1.0\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,498 >>   init_mem_cpu_alloc_delta   =      819MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,498 >>   init_mem_cpu_peaked_delta  =      388MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,498 >>   init_mem_gpu_alloc_delta   =      391MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,498 >>   init_mem_gpu_peaked_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,498 >>   train_mem_cpu_alloc_delta  =      438MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,499 >>   train_mem_cpu_peaked_delta =      185MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,499 >>   train_mem_gpu_alloc_delta  =     1231MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,499 >>   train_mem_gpu_peaked_delta =     3393MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,499 >>   train_runtime              = 0:01:41.83\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,499 >>   train_samples              =       7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:30:40,499 >>   train_samples_per_second   =       2.21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:516] 2022-04-28 14:30:40,582 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2115] 2022-04-28 14:30:40,587 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2117] 2022-04-28 14:30:40,587 >>   Num examples = 7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2120] 2022-04-28 14:30:40,587 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/897 [00:00<?, ?it/s]#015  0%|          | 3/897 [00:00<00:39, 22.46it/s]#015  1%|          | 5/897 [00:00<00:45, 19.47it/s]#015  1%|          | 7/897 [00:00<00:47, 18.68it/s]#015  1%|          | 9/897 [00:00<00:48, 18.15it/s]#015  1%|          | 11/897 [00:00<00:49, 17.73it/s]#015  1%|▏         | 13/897 [00:00<00:51, 17.09it/s]#015  2%|▏         | 15/897 [00:00<00:52, 16.88it/s]#015  2%|▏         | 17/897 [00:01<00:56, 15.44it/s]#015  2%|▏         | 19/897 [00:01<01:00, 14.40it/s]#015  2%|▏         | 21/897 [00:01<00:59, 14.62it/s]#015  3%|▎         | 23/897 [00:01<00:58, 14.88it/s]#015  3%|▎         | 25/897 [00:01<00:57, 15.12it/s]#015  3%|▎         | 27/897 [00:01<00:56, 15.28it/s]#015  3%|▎         | 29/897 [00:01<00:56, 15.25it/s]#015  3%|▎         | 31/897 [00:01<00:57, 15.07it/s]#015  4%|▎         | 33/897 [00:02<00:56, 15.33it/s]#015  4%|▍         | 35/897 [00:02<00:56, 15.28it/s]#015  4%|▍         | 37/897 [00:02<00:55, 15.41it/s]#015  4%|▍         | 39/897 [00:02<00:55, 15.43it/s]#015  5%|▍         | 41/897 [00:02<00:54, 15.71it/s]#015  5%|▍         | 43/897 [00:02<00:54, 15.61it/s]#015  5%|▌         | 45/897 [00:02<00:55, 15.31it/s]#015  5%|▌         | 47/897 [00:03<00:55, 15.37it/s]#015  5%|▌         | 49/897 [00:03<00:53, 15.87it/s]#015  6%|▌         | 51/897 [00:03<00:51, 16.30it/s]#015  6%|▌         | 53/897 [00:03<00:51, 16.36it/s]#015  6%|▌         | 55/897 [00:03<00:51, 16.38it/s]#015  6%|▋         | 57/897 [00:03<00:52, 16.14it/s]#015  7%|▋         | 59/897 [00:03<00:50, 16.56it/s]#015  7%|▋         | 61/897 [00:03<00:51, 16.36it/s]#015  7%|▋         | 63/897 [00:03<00:50, 16.48it/s]#015  7%|▋         | 65/897 [00:04<00:50, 16.46it/s]#015  7%|▋         | 67/897 [00:04<00:50, 16.54it/s]#015  8%|▊         | 69/897 [00:04<00:52, 15.82it/s]#015  8%|▊         | 71/897 [00:04<00:53, 15.47it/s]#015  8%|▊         | 73/897 [00:04<00:53, 15.46it/s]#015  8%|▊         | 75/897 [00:04<00:52, 15.65it/s]#015  9%|▊         | 77/897 [00:04<00:55, 14.87it/s]#015  9%|▉         | 79/897 [00:05<00:56, 14.57it/s]#015  9%|▉         | 81/897 [00:05<00:55, 14.78it/s]#015  9%|▉         | 83/897 [00:05<00:54, 14.85it/s]#015  9%|▉         | 85/897 [00:05<00:53, 15.29it/s]#015 10%|▉         | 87/897 [00:05<00:51, 15.65it/s]#015 10%|▉         | 89/897 [00:05<00:51, 15.81it/s]#015 10%|█         | 91/897 [00:05<00:50, 15.90it/s]#015 10%|█         | 93/897 [00:05<00:52, 15.31it/s]#015 11%|█         | 95/897 [00:06<00:55, 14.47it/s]#015 11%|█         | 97/897 [00:06<00:57, 13.99it/s]#015 11%|█         | 99/897 [00:06<00:56, 14.25it/s]#015 11%|█▏        | 101/897 [00:06<00:52, 15.03it/s]#015 11%|█▏        | 103/897 [00:06<00:51, 15.34it/s]#015 12%|█▏        | 105/897 [00:06<00:50, 15.67it/s]#015 12%|█▏        | 107/897 [00:06<00:50, 15.68it/s]#015 12%|█▏        | 109/897 [00:06<00:51, 15.20it/s]#015 12%|█▏        | 111/897 [00:07<00:49, 15.77it/s]#015 13%|█▎        | 113/897 [00:07<00:48, 16.07it/s]#015 13%|█▎        | 115/897 [00:07<00:49, 15.91it/s]#015 13%|█▎        | 117/897 [00:07<00:47, 16.26it/s]#015 13%|█▎        | 119/897 [00:07<00:48, 16.11it/s]#015 13%|█▎        | 121/897 [00:07<00:48, 15.90it/s]#015 14%|█▎        | 123/897 [00:07<00:49, 15.56it/s]#015 14%|█▍        | 125/897 [00:07<00:49, 15.70it/s]#015 14%|█▍        | 127/897 [00:08<00:48, 16.02it/s]#015 14%|█▍        | 129/897 [00:08<00:48, 15.92it/s]#015 15%|█▍        | 131/897 [00:08<00:47, 16.06it/s]#015 15%|█▍        | 133/897 [00:08<00:47, 15.95it/s]#015 15%|█▌        | 135/897 [00:08<00:47, 15.94it/s]#015 15%|█▌        | 137/897 [00:08<00:48, 15.61it/s]#015 15%|█▌        | 139/897 [00:08<00:48, 15.75it/s]#015 16%|█▌        | 141/897 [00:08<00:47, 15.83it/s]#015 16%|█▌        | 143/897 [00:09<00:47, 15.92it/s]#015 16%|█▌        | 145/897 [00:09<00:45, 16.38it/s]#015 16%|█▋        | 147/897 [00:09<00:45, 16.60it/s]#015 17%|█▋        | 149/897 [00:09<00:45, 16.62it/s]#015 17%|█▋        | 151/897 [00:09<00:45, 16.49it/s]#015 17%|█▋        | 153/897 [00:09<00:45, 16.39it/s]#015 17%|█▋        | 155/897 [00:09<00:45, 16.20it/s]#015 18%|█▊        | 157/897 [00:09<00:45, 16.09it/s]#015 18%|█▊        | 159/897 [00:10<00:46, 15.97it/s]#015 18%|█▊        | 161/897 [00:10<00:47, 15.40it/s]#015 18%|█▊        | 163/897 [00:10<00:51, 14.29it/s]#015 18%|█▊        | 165/897 [00:10<00:53, 13.74it/s]#015 19%|█▊        | 167/897 [00:10<00:52, 13.84it/s]#015 19%|█▉        | 169/897 [00:10<00:51, 14.23it/s]#015 19%|█▉        | 171/897 [00:10<00:50, 14.29it/s]#015 19%|█▉        | 173/897 [00:11<00:48, 14.78it/s]#015 20%|█▉        | 175/897 [00:11<00:48, 14.96it/s]#015 20%|█▉        | 177/897 [00:11<00:46, 15.40it/s]#015 20%|█▉        | 179/897 [00:11<00:47, 15.22it/s]#015 20%|██        | 181/897 [00:11<00:46, 15.39it/s]#015 20%|██        | 183/897 [00:11<00:45, 15.79it/s]#015 21%|██        | 185/897 [00:11<00:44, 16.05it/s]#015 21%|██        | 187/897 [00:11<00:46, 15.39it/s]#015 21%|██        | 189/897 [00:12<00:44, 15.78it/s]#015 21%|██▏       | 191/897 [00:12<00:43, 16.25it/s]#015 22%|██▏       | 193/897 [00:12<00:43, 16.22it/s]#015 22%|██▏       | 195/897 [00:12<00:43, 15.96it/s]#015 22%|██▏       | 197/897 [00:12<00:44, 15.75it/s]#015 22%|██▏       | 199/897 [00:12<00:44, 15.86it/s]#015 22%|██▏       | 201/897 [00:12<00:42, 16.29it/s]#015 23%|██▎       | 203/897 [00:12<00:41, 16.55it/s]#015 23%|██▎       | 205/897 [00:13<00:41, 16.50it/s]#015 23%|██▎       | 207/897 [00:13<00:41, 16.70it/s]#015 23%|██▎       | 209/897 [00:13<00:41, 16.75it/s]#015 24%|██▎       | 211/897 [00:13<00:41, 16.69it/s]#015 24%|██▎       | 213/897 [00:13<00:40, 16.77it/s]#015 24%|██▍       | 215/897 [00:13<00:44, 15.40it/s]#015 24%|██▍       | 217/897 [00:13<00:44, 15.22it/s]#015 24%|██▍       | 219/897 [00:13<00:42, 15.84it/s]#015 25%|██▍       | 221/897 [00:14<00:42, 16.02it/s]#015 25%|██▍       | 223/897 [00:14<00:41, 16.35it/s]#015 25%|██▌       | 225/897 [00:14<00:40, 16.50it/s]#015 25%|██▌       | 227/897 [00:14<00:40, 16.52it/s]#015 26%|██▌       | 229/897 [00:14<00:40, 16.41it/s]#015 26%|██▌       | 231/897 [00:14<00:41, 16.09it/s]#015 26%|██▌       | 233/897 [00:14<00:41, 15.93it/s]#015 26%|██▌       | 235/897 [00:14<00:42, 15.54it/s]#015 26%|██▋       | 237/897 [00:15<00:42, 15.68it/s]#015 27%|██▋       | 239/897 [00:15<00:40, 16.06it/s]#015 27%|██▋       | 241/897 [00:15<00:41, 15.98it/s]#015 27%|██▋       | 243/897 [00:15<00:41, 15.77it/s]#015 27%|██▋       | 245/897 [00:15<00:40, 15.97it/s]#015 28%|██▊       | 247/897 [00:15<00:40, 16.23it/s]#015 28%|██▊       | 249/897 [00:15<00:39, 16.30it/s]#015 28%|██▊       | 251/897 [00:15<00:41, 15.59it/s]#015 28%|██▊       | 253/897 [00:16<00:41, 15.34it/s]#015 28%|██▊       | 255/897 [00:16<00:42, 15.12it/s]#015 29%|██▊       | 257/897 [00:16<00:42, 15.21it/s]#015 29%|██▉       | 259/897 [00:16<00:41, 15.23it/s]#015 29%|██▉       | 261/897 [00:16<00:41, 15.48it/s]#015 29%|██▉       | 263/897 [00:16<00:40, 15.79it/s]#015 30%|██▉       | 265/897 [00:16<00:40, 15.75it/s]#015 30%|██▉       | 267/897 [00:17<00:41, 15.36it/s]#015 30%|██▉       | 269/897 [00:17<00:39, 15.82it/s]#015 30%|███       | 271/897 [00:17<00:40, 15.55it/s]#015 30%|███       | 273/897 [00:17<00:40, 15.22it/s]#015 31%|███       | 275/897 [00:17<00:40, 15.42it/s]#015 31%|███       | 277/897 [00:17<00:39, 15.66it/s]#015 31%|███       | 279/897 [00:17<00:39, 15.84it/s]#015 31%|███▏      | 281/897 [00:17<00:38, 16.01it/s]#015 32%|███▏      | 283/897 [00:18<00:38, 15.94it/s]#015 32%|███▏      | 285/897 [00:18<00:39, 15.32it/s]#015 32%|███▏      | 287/897 [00:18<00:39, 15.30it/s]#015 32%|███▏      | 289/897 [00:18<00:39, 15.55it/s]#015 32%|███▏      | 291/897 [00:18<00:38, 15.81it/s]#015 33%|███▎      | 293/897 [00:18<00:37, 16.12it/s]#015 33%|███▎      | 295/897 [00:18<00:38, 15.65it/s]#015 33%|███▎      | 297/897 [00:18<00:37, 16.13it/s]#015 33%|███▎      | 299/897 [00:19<00:37, 16.10it/s]#015 34%|███▎      | 301/897 [00:19<00:36, 16.43it/s]#015 34%|███▍      | 303/897 [00:19<00:36, 16.14it/s]#015 34%|███▍      | 305/897 [00:19<00:37, 15.72it/s]#015 34%|███▍      | 307/897 [00:19<00:38, 15.14it/s]#015 34%|███▍      | 309/897 [00:19<00:38, 15.46it/s]#015 35%|███▍      | 311/897 [00:19<00:37, 15.44it/s]#015 35%|███▍      | 313/897 [00:19<00:38, 15.33it/s]#015 35%|███▌      | 315/897 [00:20<00:37, 15.36it/s]#015 35%|███▌      | 317/897 [00:20<00:37, 15.32it/s]#015 36%|███▌      | 319/897 [00:20<00:37, 15.26it/s]#015 36%|███▌      | 321/897 [00:20<00:38, 15.04it/s]#015 36%|███▌      | 323/897 [00:20<00:38, 14.84it/s]#015 36%|███▌      | 325/897 [00:20<00:39, 14.57it/s]#015 36%|███▋      | 327/897 [00:20<00:39, 14.50it/s]#015 37%|███▋      | 329/897 [00:21<00:39, 14.46it/s]#015 37%|███▋      | 331/897 [00:21<00:38, 14.57it/s]#015 37%|███▋      | 333/897 [00:21<00:37, 15.22it/s]#015 37%|███▋      | 335/897 [00:21<00:35, 15.76it/s]#015 38%|███▊      | 337/897 [00:21<00:34, 16.24it/s]#015 38%|███▊      | 339/897 [00:21<00:34, 16.13it/s]#015 38%|███▊      | 341/897 [00:21<00:35, 15.87it/s]#015 38%|███▊      | 343/897 [00:21<00:34, 16.09it/s]#015 38%|███▊      | 345/897 [00:22<00:34, 15.95it/s]#015 39%|███▊      | 347/897 [00:22<00:35, 15.51it/s]#015 39%|███▉      | 349/897 [00:22<00:35, 15.38it/s]#015 39%|███▉      | 351/897 [00:22<00:36, 14.92it/s]#015 39%|███▉      | 353/897 [00:22<00:37, 14.68it/s]#015 40%|███▉      | 355/897 [00:22<00:38, 14.04it/s]#015 40%|███▉      | 357/897 [00:22<00:37, 14.24it/s]#015 40%|████      | 359/897 [00:22<00:36, 14.68it/s]#015 40%|████      | 361/897 [00:23<00:35, 15.26it/s]#015 40%|████      | 363/897 [00:23<00:33, 15.76it/s]#015 41%|████      | 365/897 [00:23<00:34, 15.32it/s]#015 41%|████      | 367/897 [00:23<00:35, 14.74it/s]#015 41%|████      | 369/897 [00:23<00:37, 14.19it/s]#015 41%|████▏     | 371/897 [00:23<00:36, 14.34it/s]#015 42%|████▏     | 373/897 [00:23<00:38, 13.76it/s]#015 42%|████▏     | 375/897 [00:24<00:37, 14.03it/s]#015 42%|████▏     | 377/897 [00:24<00:37, 13.84it/s]#015 42%|████▏     | 379/897 [00:24<00:37, 13.79it/s]#015 42%|████▏     | 381/897 [00:24<00:38, 13.54it/s]#015 43%|████▎     | 383/897 [00:24<00:36, 14.22it/s]#015 43%|████▎     | 385/897 [00:24<00:34, 14.90it/s]#015 43%|████▎     | 387/897 [00:24<00:33, 15.42it/s]#015 43%|████▎     | 389/897 [00:25<00:32, 15.76it/s]#015 44%|████▎     | 391/897 [00:25<00:31, 16.18it/s]#015 44%|████▍     | 393/897 [00:25<00:30, 16.58it/s]#015 44%|████▍     | 395/897 [00:25<00:30, 16.71it/s]#015 44%|████▍     | 397/897 [00:25<00:29, 16.77it/s]#015 44%|████▍     | 399/897 [00:25<00:29, 16.71it/s]#015 45%|████▍     | 401/897 [00:25<00:30, 16.37it/s]#015 45%|████▍     | 403/897 [00:25<00:30, 16.45it/s]#015 45%|████▌     | 405/897 [00:25<00:29, 16.45it/s]#015 45%|████▌     | 407/897 [00:26<00:29, 16.48it/s]#015 46%|████▌     | 409/897 [00:26<00:30, 16.24it/s]#015 46%|████▌     | 411/897 [00:26<00:29, 16.25it/s]#015 46%|████▌     | 413/897 [00:26<00:30, 15.72it/s]#015 46%|████▋     | 415/897 [00:26<00:31, 15.30it/s]#015 46%|████▋     | 417/897 [00:26<00:31, 15.28it/s]#015 47%|████▋     | 419/897 [00:26<00:31, 15.31it/s]#015 47%|████▋     | 421/897 [00:27<00:31, 15.19it/s]#015 47%|████▋     | 423/897 [00:27<00:31, 15.25it/s]#015 47%|████▋     | 425/897 [00:27<00:30, 15.31it/s]#015 48%|████▊     | 427/897 [00:27<00:33, 14.10it/s]#015 48%|████▊     | 429/897 [00:27<00:33, 14.03it/s]#015 48%|████▊     | 431/897 [00:27<00:31, 14.66it/s]#015 48%|████▊     | 433/897 [00:27<00:32, 14.24it/s]#015 48%|████▊     | 435/897 [00:28<00:31, 14.64it/s]#015 49%|████▊     | 437/897 [00:28<00:30, 15.04it/s]#015 49%|████▉     | 439/897 [00:28<00:29, 15.33it/s]#015 49%|████▉     | 441/897 [00:28<00:29, 15.54it/s]#015 49%|████▉     | 443/897 [00:28<00:28, 15.72it/s]#015 50%|████▉     | 445/897 [00:28<00:29, 15.39it/s]#015 50%|████▉     | 447/897 [00:28<00:29, 15.28it/s]#015 50%|█████     | 449/897 [00:28<00:28, 15.53it/s]#015 50%|█████     | 451/897 [00:29<00:28, 15.61it/s]#015 51%|█████     | 453/897 [00:29<00:29, 14.82it/s]#015 51%|█████     | 455/897 [00:29<00:29, 14.90it/s]#015 51%|█████     | 457/897 [00:29<00:29, 15.05it/s]#015 51%|█████     | 459/897 [00:29<00:29, 14.88it/s]#015 51%|█████▏    | 461/897 [00:29<00:30, 14.26it/s]#015 52%|█████▏    | 463/897 [00:29<00:30, 14.09it/s]#015 52%|█████▏    | 465/897 [00:30<00:31, 13.57it/s]#015 52%|█████▏    | 467/897 [00:30<00:31, 13.45it/s]#015 52%|█████▏    | 469/897 [00:30<00:29, 14.28it/s]#015 53%|█████▎    | 471/897 [00:30<00:28, 15.06it/s]#015 53%|█████▎    | 473/897 [00:30<00:27, 15.54it/s]#015 53%|█████▎    | 475/897 [00:30<00:26, 15.76it/s]#015 53%|█████▎    | 477/897 [00:30<00:26, 15.88it/s]#015 53%|█████▎    | 479/897 [00:30<00:25, 16.28it/s]#015 54%|█████▎    | 481/897 [00:31<00:26, 15.86it/s]#015 54%|█████▍    | 483/897 [00:31<00:25, 15.97it/s]#015 54%|█████▍    | 485/897 [00:31<00:25, 15.93it/s]#015 54%|█████▍    | 487/897 [00:31<00:26, 15.61it/s]#015 55%|█████▍    | 489/897 [00:31<00:25, 15.70it/s]#015 55%|█████▍    | 491/897 [00:31<00:26, 15.32it/s]#015 55%|█████▍    | 493/897 [00:31<00:26, 15.23it/s]#015 55%|█████▌    | 495/897 [00:31<00:26, 15.40it/s]#015 55%|█████▌    | 497/897 [00:32<00:25, 15.60it/s]#015 56%|█████▌    | 499/897 [00:32<00:26, 15.27it/s]#015 56%|█████▌    | 501/897 [00:32<00:26, 15.09it/s]#015 56%|█████▌    | 503/897 [00:32<00:26, 14.97it/s]#015 56%|█████▋    | 505/897 [00:32<00:26, 14.92it/s]#015 57%|█████▋    | 507/897 [00:32<00:25, 15.13it/s]#015 57%|█████▋    | 509/897 [00:32<00:25, 15.15it/s]#015 57%|█████▋    | 511/897 [00:32<00:24, 15.46it/s]#015 57%|█████▋    | 513/897 [00:33<00:24, 15.40it/s]#015 57%|█████▋    | 515/897 [00:33<00:25, 15.28it/s]#015 58%|█████▊    | 517/897 [00:33<00:24, 15.63it/s]#015 58%|█████▊    | 519/897 [00:33<00:23, 15.86it/s]#015 58%|█████▊    | 521/897 [00:33<00:23, 15.94it/s]#015 58%|█████▊    | 523/897 [00:33<00:23, 16.16it/s]#015 59%|█████▊    | 525/897 [00:33<00:23, 15.73it/s]#015 59%|█████▉    | 527/897 [00:33<00:22, 16.14it/s]#015 59%|█████▉    | 529/897 [00:34<00:22, 16.01it/s]#015 59%|█████▉    | 531/897 [00:34<00:22, 16.16it/s]#015 59%|█████▉    | 533/897 [00:34<00:22, 16.17it/s]#015 60%|█████▉    | 535/897 [00:34<00:22, 16.25it/s]#015 60%|█████▉    | 537/897 [00:34<00:22, 15.72it/s]#015 60%|██████    | 539/897 [00:34<00:22, 15.59it/s]#015 60%|██████    | 541/897 [00:34<00:23, 15.48it/s]#015 61%|██████    | 543/897 [00:35<00:22, 15.58it/s]#015 61%|██████    | 545/897 [00:35<00:22, 15.89it/s]#015 61%|██████    | 547/897 [00:35<00:22, 15.75it/s]#015 61%|██████    | 549/897 [00:35<00:22, 15.64it/s]#015 61%|██████▏   | 551/897 [00:35<00:22, 15.46it/s]#015 62%|██████▏   | 553/897 [00:35<00:22, 15.24it/s]#015 62%|██████▏   | 555/897 [00:35<00:23, 14.42it/s]#015 62%|██████▏   | 557/897 [00:35<00:25, 13.57it/s]#015 62%|██████▏   | 559/897 [00:36<00:25, 13.20it/s]#015 63%|██████▎   | 561/897 [00:36<00:24, 13.85it/s]#015 63%|██████▎   | 563/897 [00:36<00:23, 14.50it/s]#015 63%|██████▎   | 565/897 [00:36<00:22, 14.99it/s]#015 63%|██████▎   | 567/897 [00:36<00:22, 14.86it/s]#015 63%|██████▎   | 569/897 [00:36<00:21, 15.07it/s]#015 64%|██████▎   | 571/897 [00:36<00:21, 14.84it/s]#015 64%|██████▍   | 573/897 [00:37<00:21, 15.10it/s]#015 64%|██████▍   | 575/897 [\u001b[0m\n",
      "\u001b[34m00:37<00:20, 15.58it/s]#015 64%|██████▍   | 577/897 [00:37<00:20, 15.71it/s]#015 65%|██████▍   | 579/897 [00:37<00:20, 15.45it/s]#015 65%|██████▍   | 581/897 [00:37<00:20, 15.65it/s]#015 65%|██████▍   | 583/897 [00:37<00:19, 15.77it/s]#015 65%|██████▌   | 585/897 [00:37<00:20, 15.51it/s]#015 65%|██████▌   | 587/897 [00:37<00:20, 15.32it/s]#015 66%|██████▌   | 589/897 [00:38<00:21, 14.59it/s]#015 66%|██████▌   | 591/897 [00:38<00:21, 14.11it/s]#015 66%|██████▌   | 593/897 [00:38<00:21, 14.02it/s]#015 66%|██████▋   | 595/897 [00:38<00:22, 13.43it/s]#015 67%|██████▋   | 597/897 [00:38<00:21, 14.09it/s]#015 67%|██████▋   | 599/897 [00:38<00:20, 14.89it/s]#015 67%|██████▋   | 601/897 [00:38<00:19, 15.49it/s]#015 67%|██████▋   | 603/897 [00:39<00:18, 15.82it/s]#015 67%|██████▋   | 605/897 [00:39<00:18, 15.62it/s]#015 68%|██████▊   | 607/897 [00:39<00:18, 15.44it/s]#015 68%|██████▊   | 609/897 [00:39<00:18, 15.93it/s]#015 68%|██████▊   | 611/897 [00:39<00:17, 16.10it/s]#015 68%|██████▊   | 613/897 [00:39<00:18, 15.57it/s]#015 69%|██████▊   | 615/897 [00:39<00:18, 15.39it/s]#015 69%|██████▉   | 617/897 [00:39<00:18, 15.31it/s]#015 69%|██████▉   | 619/897 [00:40<00:17, 15.45it/s]#015 69%|██████▉   | 621/897 [00:40<00:17, 15.74it/s]#015 69%|██████▉   | 623/897 [00:40<00:17, 15.37it/s]#015 70%|██████▉   | 625/897 [00:40<00:17, 15.76it/s]#015 70%|██████▉   | 627/897 [00:40<00:16, 16.21it/s]#015 70%|███████   | 629/897 [00:40<00:16, 16.44it/s]#015 70%|███████   | 631/897 [00:40<00:16, 16.00it/s]#015 71%|███████   | 633/897 [00:40<00:16, 15.98it/s]#015 71%|███████   | 635/897 [00:41<00:16, 15.72it/s]#015 71%|███████   | 637/897 [00:41<00:16, 15.53it/s]#015 71%|███████   | 639/897 [00:41<00:16, 15.59it/s]#015 71%|███████▏  | 641/897 [00:41<00:16, 15.81it/s]#015 72%|███████▏  | 643/897 [00:41<00:16, 15.74it/s]#015 72%|███████▏  | 645/897 [00:41<00:15, 16.02it/s]#015 72%|███████▏  | 647/897 [00:41<00:15, 16.19it/s]#015 72%|███████▏  | 649/897 [00:41<00:15, 15.58it/s]#015 73%|███████▎  | 651/897 [00:42<00:16, 15.04it/s]#015 73%|███████▎  | 653/897 [00:42<00:15, 15.48it/s]#015 73%|███████▎  | 655/897 [00:42<00:15, 15.95it/s]#015 73%|███████▎  | 657/897 [00:42<00:15, 15.97it/s]#015 73%|███████▎  | 659/897 [00:42<00:14, 16.35it/s]#015 74%|███████▎  | 661/897 [00:42<00:14, 16.63it/s]#015 74%|███████▍  | 663/897 [00:42<00:14, 16.65it/s]#015 74%|███████▍  | 665/897 [00:42<00:14, 16.25it/s]#015 74%|███████▍  | 667/897 [00:43<00:14, 16.23it/s]#015 75%|███████▍  | 669/897 [00:43<00:13, 16.42it/s]#015 75%|███████▍  | 671/897 [00:43<00:14, 16.07it/s]#015 75%|███████▌  | 673/897 [00:43<00:13, 16.25it/s]#015 75%|███████▌  | 675/897 [00:43<00:13, 15.93it/s]#015 75%|███████▌  | 677/897 [00:43<00:14, 15.59it/s]#015 76%|███████▌  | 679/897 [00:43<00:13, 15.61it/s]#015 76%|███████▌  | 681/897 [00:43<00:13, 15.69it/s]#015 76%|███████▌  | 683/897 [00:44<00:14, 14.94it/s]#015 76%|███████▋  | 685/897 [00:44<00:14, 14.23it/s]#015 77%|███████▋  | 687/897 [00:44<00:15, 13.69it/s]#015 77%|███████▋  | 689/897 [00:44<00:15, 13.48it/s]#015 77%|███████▋  | 691/897 [00:44<00:14, 13.97it/s]#015 77%|███████▋  | 693/897 [00:44<00:13, 14.65it/s]#015 77%|███████▋  | 695/897 [00:44<00:13, 15.12it/s]#015 78%|███████▊  | 697/897 [00:45<00:12, 15.62it/s]#015 78%|███████▊  | 699/897 [00:45<00:12, 16.08it/s]#015 78%|███████▊  | 701/897 [00:45<00:12, 16.22it/s]#015 78%|███████▊  | 703/897 [00:45<00:12, 16.06it/s]#015 79%|███████▊  | 705/897 [00:45<00:11, 16.17it/s]#015 79%|███████▉  | 707/897 [00:45<00:12, 15.38it/s]#015 79%|███████▉  | 709/897 [00:45<00:12, 14.98it/s]#015 79%|███████▉  | 711/897 [00:45<00:12, 15.33it/s]#015 79%|███████▉  | 713/897 [00:46<00:12, 15.06it/s]#015 80%|███████▉  | 715/897 [00:46<00:12, 15.10it/s]#015 80%|███████▉  | 717/897 [00:46<00:11, 15.03it/s]#015 80%|████████  | 719/897 [00:46<00:12, 14.83it/s]#015 80%|████████  | 721/897 [00:46<00:12, 14.19it/s]#015 81%|████████  | 723/897 [00:46<00:11, 14.51it/s]#015 81%|████████  | 725/897 [00:46<00:11, 14.88it/s]#015 81%|████████  | 727/897 [00:47<00:11, 14.85it/s]#015 81%|████████▏ | 729/897 [00:47<00:11, 15.01it/s]#015 81%|████████▏ | 731/897 [00:47<00:10, 15.42it/s]#015 82%|████████▏ | 733/897 [00:47<00:10, 15.54it/s]#015 82%|████████▏ | 735/897 [00:47<00:10, 15.39it/s]#015 82%|████████▏ | 737/897 [00:47<00:10, 14.55it/s]#015 82%|████████▏ | 739/897 [00:47<00:11, 13.94it/s]#015 83%|████████▎ | 741/897 [00:48<00:11, 14.03it/s]#015 83%|████████▎ | 743/897 [00:48<00:10, 14.44it/s]#015 83%|████████▎ | 745/897 [00:48<00:10, 15.10it/s]#015 83%|████████▎ | 747/897 [00:48<00:09, 15.37it/s]#015 84%|████████▎ | 749/897 [00:48<00:09, 15.54it/s]#015 84%|████████▎ | 751/897 [00:48<00:09, 15.78it/s]#015 84%|████████▍ | 753/897 [00:48<00:09, 15.94it/s]#015 84%|████████▍ | 755/897 [00:48<00:09, 15.56it/s]#015 84%|████████▍ | 757/897 [00:49<00:08, 15.75it/s]#015 85%|████████▍ | 759/897 [00:49<00:08, 15.77it/s]#015 85%|████████▍ | 761/897 [00:49<00:08, 15.99it/s]#015 85%|████████▌ | 763/897 [00:49<00:08, 15.38it/s]#015 85%|████████▌ | 765/897 [00:49<00:08, 15.76it/s]#015 86%|████████▌ | 767/897 [00:49<00:08, 15.90it/s]#015 86%|████████▌ | 769/897 [00:49<00:08, 15.89it/s]#015 86%|████████▌ | 771/897 [00:49<00:07, 16.07it/s]#015 86%|████████▌ | 773/897 [00:50<00:07, 15.96it/s]#015 86%|████████▋ | 775/897 [00:50<00:07, 16.30it/s]#015 87%|████████▋ | 777/897 [00:50<00:07, 16.05it/s]#015 87%|████████▋ | 779/897 [00:50<00:07, 16.41it/s]#015 87%|████████▋ | 781/897 [00:50<00:07, 16.51it/s]#015 87%|████████▋ | 783/897 [00:50<00:06, 16.56it/s]#015 88%|████████▊ | 785/897 [00:50<00:06, 16.50it/s]#015 88%|████████▊ | 787/897 [00:50<00:06, 16.21it/s]#015 88%|████████▊ | 789/897 [00:51<00:06, 16.08it/s]#015 88%|████████▊ | 791/897 [00:51<00:06, 15.66it/s]#015 88%|████████▊ | 793/897 [00:51<00:06, 15.92it/s]#015 89%|████████▊ | 795/897 [00:51<00:06, 16.05it/s]#015 89%|████████▉ | 797/897 [00:51<00:06, 15.89it/s]#015 89%|████████▉ | 799/897 [00:51<00:06, 15.51it/s]#015 89%|████████▉ | 801/897 [00:51<00:06, 15.65it/s]#015 90%|████████▉ | 803/897 [00:51<00:06, 15.37it/s]#015 90%|████████▉ | 805/897 [00:52<00:06, 15.28it/s]#015 90%|████████▉ | 807/897 [00:52<00:05, 15.50it/s]#015 90%|█████████ | 809/897 [00:52<00:05, 15.23it/s]#015 90%|█████████ | 811/897 [00:52<00:05, 15.80it/s]#015 91%|█████████ | 813/897 [00:52<00:05, 16.11it/s]#015 91%|█████████ | 815/897 [00:52<00:05, 16.14it/s]#015 91%|█████████ | 817/897 [00:52<00:04, 16.30it/s]#015 91%|█████████▏| 819/897 [00:52<00:04, 16.44it/s]#015 92%|█████████▏| 821/897 [00:53<00:04, 16.19it/s]#015 92%|█████████▏| 823/897 [00:53<00:04, 16.12it/s]#015 92%|█████████▏| 825/897 [00:53<00:04, 16.36it/s]#015 92%|█████████▏| 827/897 [00:53<00:04, 15.57it/s]#015 92%|█████████▏| 829/897 [00:53<00:04, 15.73it/s]#015 93%|█████████▎| 831/897 [00:53<00:04, 15.38it/s]#015 93%|█████████▎| 833/897 [00:53<00:04, 15.66it/s]#015 93%|█████████▎| 835/897 [00:53<00:03, 15.89it/s]#015 93%|█████████▎| 837/897 [00:54<00:03, 15.81it/s]#015 94%|█████████▎| 839/897 [00:54<00:03, 16.07it/s]#015 94%|█████████▍| 841/897 [00:54<00:03, 16.32it/s]#015 94%|█████████▍| 843/897 [00:54<00:03, 16.44it/s]#015 94%|█████████▍| 845/897 [00:54<00:03, 16.20it/s]#015 94%|█████████▍| 847/897 [00:54<00:03, 15.83it/s]#015 95%|█████████▍| 849/897 [00:54<00:02, 16.07it/s]#015 95%|█████████▍| 851/897 [00:54<00:02, 15.97it/s]#015 95%|█████████▌| 853/897 [00:55<00:02, 16.06it/s]#015 95%|█████████▌| 855/897 [00:55<00:02, 16.07it/s]#015 96%|█████████▌| 857/897 [00:55<00:02, 16.27it/s]#015 96%|█████████▌| 859/897 [00:55<00:02, 15.91it/s]#015 96%|█████████▌| 861/897 [00:55<00:02, 16.27it/s]#015 96%|█████████▌| 863/897 [00:55<00:02, 16.37it/s]#015 96%|█████████▋| 865/897 [00:55<00:01, 16.36it/s]#015 97%|█████████▋| 867/897 [00:55<00:01, 16.76it/s]#015 97%|█████████▋| 869/897 [00:56<00:01, 16.59it/s]#015 97%|█████████▋| 871/897 [00:56<00:01, 16.28it/s]#015 97%|█████████▋| 873/897 [00:56<00:01, 15.40it/s]#015 98%|█████████▊| 875/897 [00:56<00:01, 15.75it/s]#015 98%|█████████▊| 877/897 [00:56<00:01, 15.77it/s]#015 98%|█████████▊| 879/897 [00:56<00:01, 15.84it/s]#015 98%|█████████▊| 881/897 [00:56<00:01, 15.50it/s]#015 98%|█████████▊| 883/897 [00:56<00:00, 15.71it/s]#015 99%|█████████▊| 885/897 [00:57<00:00, 15.60it/s]#015 99%|█████████▉| 887/897 [00:57<00:00, 15.64it/s]#015 99%|█████████▉| 889/897 [00:57<00:00, 14.99it/s]#015 99%|█████████▉| 891/897 [00:57<00:00, 15.09it/s]#015100%|█████████▉| 893/897 [00:57<00:00, 15.48it/s]#015100%|█████████▉| 895/897 [00:57<00:00, 15.64it/s]#015100%|██████████| 897/897 [00:57<00:00, 15.68it/s]#015100%|██████████| 897/897 [00:57<00:00, 15.51it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:907] 2022-04-28 14:31:38,569 >> ***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   epoch                     =        1.0\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_accuracy             =     0.4642\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_loss                 =     2.7924\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_mem_cpu_alloc_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_mem_cpu_peaked_delta =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_mem_gpu_alloc_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_mem_gpu_peaked_delta =       38MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_runtime              = 0:00:57.89\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,570 >>   eval_samples              =       7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-04-28 14:31:38,571 >>   eval_samples_per_second   =    123.829\u001b[0m\n",
      "\n",
      "2022-04-28 14:32:13 Uploading - Uploading generated training model\n",
      "2022-04-28 14:35:14 Completed - Training job completed\n",
      "Training seconds: 614\n",
      "Billable seconds: 614\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(\n",
    "  {'train': data_location+'/100014.csv',\n",
    "   'test': data_location+'/100014.csv'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f9f68",
   "metadata": {},
   "source": [
    "# 模型部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "881ecfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34bf47e",
   "metadata": {},
   "source": [
    "# 模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3723058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel, HuggingFacePredictor\n",
    "import sagemaker\n",
    "\n",
    "huggingface_predictor=HuggingFacePredictor(\n",
    "    endpoint_name='huggingface-pytorch-training-2022-04-20-07-39-48-503',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cbce2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_36', 'score': 0.9989555478096008}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_predictor.predict({'inputs': \"连环画\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12d2e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs=['EPP泡沫', 'T其它', 'T存储', 'T服务器', 'T电脑', 'T网络', 'T网络布线(办公)', '书写工具', '会议桌', '传感器', '低压柜', '保护帽', '保洁', '信号发生器', '光学检测设备 AOI改造和升级', '光谱仪', '其它', '其它 五金件其它', '其它(机械其它）', '其它(气动其它）', '其它(电气其它）', '其它（工具其它）', '其它（非金属材料其它）', '内衬', '冰箱', '冲压设备', '冷却设备', '切片及产品验证', '办公桌', '办公椅', '办公用纸', '功率测试设备', '加热设备', '包材模具相关', '包装设备', '医疗、健康', '印刷品', '压力测量仪', '叉车', '叉车备件', '变压器', '地面', '垫板、垫片', '塑料周转塑料箱', '塑料袋', '外箱', '存储(文件)', '存储(系统)', '实验室工作台', '密封和润滑', '封箱带', '工作台', '工作服装鞋帽', '工作椅', '常用电气', '弹簧', '影像', '性能试验', '恒温恒湿机', '手动', '扎带', '打包带', '打印设备', '投影仪', '排风机', '接插件', '控制器', '支撑板', '数字万用表', '整形设备', '文件储存', '日用杂品', '更衣柜', '服务器(项目)', '机器人', '机床附件和焊接器材', '机械切平设备', '机械脉动试验', '材料', '标签', '气动', '气动执行元件', '气动控制阀', '气动附件', '水槽', '水泵', '洗碗机', '流量测量仪', '测量', '消火栓', '润滑脂', '液压执行元件', '液压控制阀', '液压泵', '液压附件', '清洗类', '清洗设备专用备件', '温度冲击', '温湿度传感器', '灭火器', '灭火用品', '灯具', '炉', '特殊桌面设备', '电动', '电机', '电气仪表', '电气性能测试设备', '电源', '电磁接触器', '电线和电缆', '电话会议设备', '电话系统', '监控探头', '硬度计', '碎纸机', '示波器', '礼品', '空压机', '空调', '空调箱', '立体仓库备件', '管路连接件', '粗糙度仪', '紧固件', '纸护角', '缠绕膜', '耗材', '蒸箱', '螺旋, 带链及齿轮传动件', '衡器', '表面激光成型设备', '计量', '财务用品', '起重, 液压和运输', '车床', '车辆使用的周边设备', '车险', '轴及连接', '轴承', '轴法兰', '运输和起重件', '配电箱', '酸碱溶液', '金属周转器具', '钢材', '钻床', '门窗和家具配件', '阀门', '防冻液', '防护用品', '隔档', '食堂日杂', '马桶', '高位货架备件', '高分子材料分析']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61aaf156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'印刷品'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labs[36]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
