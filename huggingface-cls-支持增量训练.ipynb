{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1862451d",
   "metadata": {},
   "source": [
    "# 权限配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233bf26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws-cn:iam::015708445809:role/service-role/AmazonSageMaker-ExecutionRole-20211021T095375\n",
      "sagemaker bucket: sagemaker-cn-north-1-015708445809\n",
      "sagemaker session region: cn-north-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5099c8d3",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851f9621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-cn-north-1-015708445809/datasets/zhenyun'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset used\n",
    "dataset_name = 'zhenyun'\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'datasets/zhenyun'\n",
    "WORK_DIRECTORY = './data/'\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=s3_prefix)\n",
    "data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb5bc50",
   "metadata": {},
   "source": [
    "# 镜像准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1fc9dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1B02706667: Loading layer  65.61MB/65.61MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1Bdf553184: Loading layer  15.87kB/15.87kB\n",
      "\u001b[1B4df0ad6c: Loading layer  3.072kB/3.072kB\n",
      "\u001b[1Be116c0c0: Loading layer   17.1MB/17.1MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1B30bcc944: Loading layer   30.5MB/30.5MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1B4dce1444: Loading layer  22.02kB/22.02kB\n",
      "\u001b[1Ba7c9e3d1: Loading layer  1.825GB/1.825GB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1B18b890fc: Loading layer  1.808GB/1.808GB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1B00c31be3: Loading layer  3.676GB/3.676GB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1Bc5356ba1: Loading layer    462MB/462MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1Bcdf6d5f2: Loading layer  274.6MB/274.6MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KLoading layer  35.65MB/274.6MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1B0bcc6897: Loading layer  13.39MB/13.39MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1Ba9c23850: Loading layer  1.752GB/1.752GB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1Bce2a51ec: Loading layer  27.14kB/27.14kB\n",
      "\u001b[1B1bae5d0c: Loading layer  1.086GB/1.086GB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1B306d4955: Loading layer   2.56kB/2.56kB\u001b[1A\u001b[2K\n",
      "\u001b[1B0f52b878: Loading layer   2.56kB/2.56kB\n",
      "\u001b[1B8a030441: Loading layer  3.584kB/3.584kB\n",
      "\u001b[1B11ff6420: Loading layer  1.993GB/1.993GB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KLoading layer  348.2MB/1.993GB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KLoading layer  1.396GB/1.993GB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1Bc1b6f753: Loading layer  776.2kB/776.2kB\u001b[1A\u001b[2K\n",
      "\u001b[1B1a00d0a6: Loading layer  276.5kB/276.5kB\u001b[1A\u001b[2K\n",
      "\u001b[1B355c3b7e: Loading layer  153.2MB/153.2MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1B4e98599b: Loading layer  65.21MB/65.21MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1B1d1bff9f: Loading layer  9.728kB/9.728kB\n",
      "\u001b[1Bb1eec580: Loading layer  10.99MB/10.99MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1B4f900f31: Loading layer   5.12kB/5.12kB\n",
      "\u001b[1Bf702dd6c: Loading layer  8.192kB/8.192kB\u001b[1A\u001b[2K\n",
      "\u001b[1B55e0088c: Loading layer  43.28MB/43.28MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1B621c0763: Loading layer  17.39MB/17.39MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1B62d7b191: Loading layer  11.26kB/11.26kB\n",
      "\u001b[1Baa20b119: Loading layer  12.29kB/12.29kB\n",
      "\u001b[1Be81c04ac: Loading layer  7.907MB/7.907MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1B426c5cb3: Loading layer  4.608kB/4.608kB\n",
      "\u001b[1B6d0d93dd: Loading layer  40.76MB/40.76MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1Bb98fd235: Loading layer  9.551MB/9.551MB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "\u001b[1Bae3310ec: Loading layer  7.325MB/7.325MB\u001b[1A\u001b[2K\u001b[1A\u001b[2KLoaded image ID: sha256:579fc0526e3023647c47e9e8e94d52b86445350b9971d44640a503a952c23a9a\n",
      "load_and_push.sh: line 3: hf46.tar: No such file or directory\n",
      "set -e\n",
      "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
      "# by SageMaker.\n",
      "\n",
      "# The argument to this script is the image name. This will be used as the image on the local\n",
      "# machine and combined with the account and region to form the repository name for ECR.\n",
      "image=$1\n",
      "\n",
      "if [ \"$image\" == \"\" ]\n",
      "then\n",
      "    echo \"Use image name hf64\"\n",
      "    image=\"hf64\"\n",
      "fi\n",
      "Use image name hf64\n",
      "\n",
      "# Get the account number associated with the current IAM credentials\n",
      "account=$(aws sts get-caller-identity --query Account --output text)\n",
      "\n",
      "if [ $? -ne 0 ]\n",
      "then\n",
      "    exit 255\n",
      "fi\n",
      "\n",
      "# Get the region defined in the current configuration\n",
      "region=$(aws configure get region)\n",
      "#regions=$(aws ec2 describe-regions --all-regions --query \"Regions[].{Name:RegionName}\" --output text)\n",
      "\n",
      "#for region in $regions; do\n",
      "\n",
      "#aws s3 cp s3://aws-solutions-${region}/spot-bot-models/cars/model.tar.gz ./\n",
      "#tar zxvf model.tar.gz\n",
      "# TODO: update regional location based on https://amazonaws-china.com/releasenotes/available-deep-learning-containers-images/\n",
      "\n",
      "if [[ $region =~ ^cn.* ]]\n",
      "then\n",
      "    fullname=\"${account}.dkr.ecr.${region}.amazonaws.com.cn/${image}:latest\"\n",
      "    registry_id=\"727897471807\"\n",
      "    registry_uri=\"${registry_id}.dkr.ecr.${region}.amazonaws.com.cn\"\n",
      "elif [[ $region = \"ap-east-1\" ]]\n",
      "then\n",
      "    fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${image}:latest\"\n",
      "    registry_id=\"871362719292\"\n",
      "    registry_uri=\"${registry_id}.dkr.ecr.${region}.amazonaws.com\"\n",
      "else\n",
      "    fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${image}:latest\"\n",
      "    registry_id=\"763104351884\"\n",
      "    registry_uri=\"${registry_id}.dkr.ecr.${region}.amazonaws.com\"\n",
      "fi\n",
      "\n",
      "echo ${fullname}\n",
      "015708445809.dkr.ecr.cn-north-1.amazonaws.com.cn/hf64:latest\n",
      "\n",
      "# If the repository doesn't exist in ECR, create it.\n",
      "aws ecr describe-repositories --repository-names \"${image}\" --region ${region} || aws ecr create-repository --repository-name \"${image}\" --region ${region}\n",
      "{\n",
      "    \"repositories\": [\n",
      "        {\n",
      "            \"repositoryArn\": \"arn:aws-cn:ecr:cn-north-1:015708445809:repository/hf64\",\n",
      "            \"registryId\": \"015708445809\",\n",
      "            \"repositoryName\": \"hf64\",\n",
      "            \"repositoryUri\": \"015708445809.dkr.ecr.cn-north-1.amazonaws.com.cn/hf64\",\n",
      "            \"createdAt\": 1652673263.0,\n",
      "            \"imageTagMutability\": \"MUTABLE\",\n",
      "            \"imageScanningConfiguration\": {\n",
      "                \"scanOnPush\": false\n",
      "            },\n",
      "            \"encryptionConfiguration\": {\n",
      "                \"encryptionType\": \"AES256\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "\n",
      "# Get the login command from ECR and execute it directly\n",
      "$(aws ecr get-login --registry-ids ${account} --region ${region} --no-include-email)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "$(aws ecr get-login --registry-ids ${registry_id} --region ${region} --no-include-email)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "# Build the docker image, tag with full name and then push it to ECR\n",
      "# docker build -t ${image} -f Dockerfile . --build-arg REGISTRY_URI=${registry_uri}\n",
      "docker tag ${image} ${fullname}\n",
      "docker push ${fullname}\n",
      "The push refers to repository [015708445809.dkr.ecr.cn-north-1.amazonaws.com.cn/hf64]\n",
      "\n",
      "\u001b[1Bae3310ec: Preparing \n",
      "\u001b[1Bb98fd235: Preparing \n",
      "\u001b[1B6d0d93dd: Preparing \n",
      "\u001b[1B426c5cb3: Preparing \n",
      "\u001b[1Be81c04ac: Preparing \n",
      "\u001b[1Baa20b119: Preparing \n",
      "\u001b[1B62d7b191: Preparing \n",
      "\u001b[1B621c0763: Preparing \n",
      "\u001b[1B55e0088c: Preparing \n",
      "\u001b[1Bf702dd6c: Preparing \n",
      "\u001b[1B4f900f31: Preparing \n",
      "\u001b[1Bb1eec580: Preparing \n",
      "\u001b[1B1d1bff9f: Preparing \n",
      "\u001b[9Baa20b119: Waiting g \n",
      "\u001b[1B355c3b7e: Preparing \n",
      "\u001b[10B2d7b191: Waiting g \n",
      "\u001b[10B21c0763: Waiting g \n",
      "\u001b[1B11ff6420: Preparing \n",
      "\u001b[11B5e0088c: Waiting g \n",
      "\u001b[11B702dd6c: Waiting g \n",
      "\u001b[11Bf900f31: Waiting g \n",
      "\u001b[1B1bae5d0c: Preparing \n",
      "\u001b[1Bce2a51ec: Preparing \n",
      "\u001b[13B1eec580: Waiting g \n",
      "\u001b[13Bd1bff9f: Waiting g \n",
      "\u001b[1Bcdf6d5f2: Preparing \n",
      "\u001b[14Be98599b: Waiting g \n",
      "\u001b[14B55c3b7e: Waiting g \n",
      "\u001b[1B18b890fc: Preparing \n",
      "\u001b[15Ba00d0a6: Waiting g \n",
      "\u001b[15B1b6f753: Waiting g \n",
      "\u001b[1B30bcc944: Preparing \n",
      "\u001b[16B1ff6420: Waiting g \n",
      "\u001b[16Ba030441: Waiting g \n",
      "\u001b[15B06d4955: Waiting g \n",
      "\u001b[9B00c31be3: Pushed   3.676GB/3.676GB\u001b[32A\u001b[2K\u001b[34A\u001b[2K\u001b[32A\u001b[2K\u001b[34A\u001b[2K\u001b[32A\u001b[2K\u001b[34A\u001b[2K\u001b[32A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[31A\u001b[2K\u001b[35A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[34A\u001b[2K\u001b[32A\u001b[2K\u001b[34A\u001b[2K\u001b[30A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[30A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[27A\u001b[2K\u001b[34A\u001b[2KPushing  3.192MB/17.17MB\u001b[28A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[29A\u001b[2K\u001b[28A\u001b[2K\u001b[29A\u001b[2K\u001b[27A\u001b[2K\u001b[29A\u001b[2K\u001b[26A\u001b[2K\u001b[34A\u001b[2K\u001b[28A\u001b[2K\u001b[34A\u001b[2K\u001b[28A\u001b[2K\u001b[29A\u001b[2K\u001b[34A\u001b[2K\u001b[26A\u001b[2K\u001b[25A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[34A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[23A\u001b[2K\u001b[22A\u001b[2K\u001b[23A\u001b[2K\u001b[22A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[21A\u001b[2K\u001b[22A\u001b[2K\u001b[23A\u001b[2K\u001b[22A\u001b[2K\u001b[23A\u001b[2K\u001b[21A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[23A\u001b[2K\u001b[23A\u001b[2K\u001b[23A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[23A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[22A\u001b[2K\u001b[23A\u001b[2K\u001b[22A\u001b[2K\u001b[23A\u001b[2K\u001b[22A\u001b[2K\u001b[19A\u001b[2K\u001b[16A\u001b[2K\u001b[22A\u001b[2K\u001b[23A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[23A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[22A\u001b[2K\u001b[19A\u001b[2K\u001b[22A\u001b[2K\u001b[13A\u001b[2K\u001b[22A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[22A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[19A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[22A\u001b[2K\u001b[19A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[22A\u001b[2K\u001b[13A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[15A\u001b[2K\u001b[22A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2KPushing  204.3MB/1.071GB\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[36A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[15A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[15A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[15A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[6A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2KPushing  381.5MB/1.825GB\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[19A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[8A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[4A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[4A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[4A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[4A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[3A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[3A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[2A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[2A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[19A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2KPushing  1.934GB/3.676GB\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2Klatest: digest: sha256:924fb42da805ff98a1f229def9b052851635dad61dfea2f91881081e46f1e254 size: 7892\n",
      "\n",
      "#done\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://datalab2022.s3.us-west-2.amazonaws.com/hf46.tar?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEML%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDmFwLW5vcnRoZWFzdC0yIkcwRQIgNTzvX7%2BuWMOZShK7BZEPzw20Z07o3b6bYr%2BpDXoQx1ICIQCam6gbYSexAKwy5rOfP2PAQrPRM1Y%2BzLnWgfercZOfzSrvAgib%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAIaDDg0NzM4MDk2NDM1MyIMKUo2PAXGe93m%2BLGOKsMC4zQRK9VbvsLZHwpJ%2FnNNuAa36QLcskbtXsG73wa7fLk8sVK7X5B8N8Xh6oxF9wge1tOi2c%2BaNV0RWUucWOdKBzhBR082BGJYMnEsY%2FHCKiX75vVyDFbyhl7sZE7oDMkhptuEOvDLXMzBNsvNCYvUJR57ovoScGnj9i01LrTjqxXohTZv1QH2Os%2FD1qXlh4IIXMaYDJ3HDEavzAl9GjbOQ%2BJAXC%2BpQZnZCzZRbUszI%2BhTAc78AbkY5xO1MWKPkI0G8fnn4DTq9tbgy5b9afIgloBgB0WUKMzB0OWij6rkETW0eDAD2wTcQ8ljK4K8VKh7heWOz%2Fu8g04EMEEFqFwIwdPMtkNAOccpANtW3fQyarizLFrzGpmBjZ0tQ0W%2B0XA2FgONNRUkf1kAYhbfbFSgX8rxkJ6ZiKhpTHuqm4OQnyGab7Ewy9yGlAY6hwI2Oi9VO6QPWjfUP9LXQXTfVq%2B410viabaTjOFK67O9ZSvc3Ti3Fn%2BFnhUsG3bxC%2B4dXgNoUiPo8oNod7UqS1hFjsBzMx8FtS0z8eJlATZzW8edLwe25AlE%2BcBcfU3EUuyYt1Nn%2BBrlUUgIL71FeRFOwjQEMhI5jPQVBCEgxCFT6sdCndAdfBO30xZ0bvFa%2BbDhDSNLo0S3%2Fut2xdp85ZvwziKxBTa2C64a0jTMB8iHWpUr8b0%2BEYvnGMylNfBuPxTswiNxWZAWgmLHO5HQ60CC14TO%2FJ6%2F7HAZjgjJOP4IjK%2Baeb4DEgPnGeXFa5xTJApgLV6HlUxmZNQ%2FanLdtQnSiTkSJH36hg%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220516T024340Z&X-Amz-SignedHeaders=host&X-Amz-Expires=43200&X-Amz-Credential=ASIA4KS6XMQA5OKSGDQM%2F20220516%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=3bfe5ba71ccb5e98adb02c73273411f8714c5bfd4b250d92908058299cc44bd0\" -O ./hf64.tar\n",
    "!docker load < /home/ec2-user/SageMaker/text-classification-master/hf64.tar\n",
    "!docker tag 579fc0526e30 hf64:latest\n",
    "!sh load_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f527e6",
   "metadata": {},
   "source": [
    "# 模型参数指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "052117c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "# hyperparameters which are passed to the training job\n",
    "hyperparameters={'num_train_epochs': 10,\n",
    "                 'train_file':'/opt/ml/input/data/train/100014.csv',\n",
    "                 'validation_file':'/opt/ml/input/data/test/100014.csv',\n",
    "                 'output_dir':'/opt/ml/model',\n",
    "                 'max_seq_length': 128,\n",
    "                 'model_name_or_path': 'bert-base-chinese',\n",
    "                 'learning_rate': 2e-5,\n",
    "                 'num_train_epochs': 1,\n",
    "                 'per_device_train_batch_size': 32,\n",
    "                 'save_strategy':'epoch',\n",
    "                 'save_total_limit':1,\n",
    "                 }\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='run_glue.py',\n",
    "        source_dir='./scripts',\n",
    "        instance_type='ml.p3.2xlarge',\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        image_uri='015708445809.dkr.ecr.cn-north-1.amazonaws.com.cn/hf64',\n",
    "#         transformers_version='4.6',\n",
    "#         pytorch_version='1.7',\n",
    "        py_version='py36',\n",
    "        hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3efe2",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10149ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-16 05:28:16 Starting - Starting the training job...\n",
      "2022-05-16 05:28:43 Starting - Preparing the instances for trainingProfilerReport-1652678896: InProgress\n",
      ".........\n",
      "2022-05-16 05:30:03 Downloading - Downloading input data...\n",
      "2022-05-16 05:30:43 Training - Downloading the training image.......................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-05-16 05:34:22,075 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-05-16 05:34:22,077 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-16 05:34:22,087 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-05-16 05:34:22,093 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-05-16 05:34:22,546 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting accelerate\u001b[0m\n",
      "\u001b[34m  Downloading accelerate-0.8.0-py3-none-any.whl (114 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.1.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (3.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->-r requirements.txt (line 5)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r requirements.txt (line 2)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from accelerate->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.1.3->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.1.3->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: accelerate\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.8.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-05-16 05:34:27,312 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-16 05:34:27,324 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-16 05:34:27,336 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-16 05:34:27,346 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"learning_rate\": 2e-05,\n",
      "        \"max_seq_length\": 128,\n",
      "        \"model_name_or_path\": \"bert-base-chinese\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_train_batch_size\": 32,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"save_total_limit\": 1,\n",
      "        \"train_file\": \"/opt/ml/input/data/train/100014.csv\",\n",
      "        \"validation_file\": \"/opt/ml/input/data/test/100014.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"hf64-2022-05-16-05-28-16-097\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-cn-north-1-015708445809/hf64-2022-05-16-05-28-16-097/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_glue\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_glue.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"learning_rate\":2e-05,\"max_seq_length\":128,\"model_name_or_path\":\"bert-base-chinese\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"per_device_train_batch_size\":32,\"save_strategy\":\"epoch\",\"save_total_limit\":1,\"train_file\":\"/opt/ml/input/data/train/100014.csv\",\"validation_file\":\"/opt/ml/input/data/test/100014.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_glue.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_glue\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-cn-north-1-015708445809/hf64-2022-05-16-05-28-16-097/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"learning_rate\":2e-05,\"max_seq_length\":128,\"model_name_or_path\":\"bert-base-chinese\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"per_device_train_batch_size\":32,\"save_strategy\":\"epoch\",\"save_total_limit\":1,\"train_file\":\"/opt/ml/input/data/train/100014.csv\",\"validation_file\":\"/opt/ml/input/data/test/100014.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"hf64-2022-05-16-05-28-16-097\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-cn-north-1-015708445809/hf64-2022-05-16-05-28-16-097/source/sourcedir.tar.gz\",\"module_name\":\"run_glue\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_glue.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--learning_rate\",\"2e-05\",\"--max_seq_length\",\"128\",\"--model_name_or_path\",\"bert-base-chinese\",\"--num_train_epochs\",\"1\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_train_batch_size\",\"32\",\"--save_strategy\",\"epoch\",\"--save_total_limit\",\"1\",\"--train_file\",\"/opt/ml/input/data/train/100014.csv\",\"--validation_file\",\"/opt/ml/input/data/test/100014.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=bert-base-chinese\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=1\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/train/100014.csv\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FILE=/opt/ml/input/data/test/100014.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 run_glue.py --learning_rate 2e-05 --max_seq_length 128 --model_name_or_path bert-base-chinese --num_train_epochs 1 --output_dir /opt/ml/model --per_device_train_batch_size 32 --save_strategy epoch --save_total_limit 1 --train_file /opt/ml/input/data/train/100014.csv --validation_file /opt/ml/input/data/test/100014.csv\u001b[0m\n",
      "\u001b[34m05/16/2022 05:34:31 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m05/16/2022 05:34:31 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May16_05-34-31_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.EPOCH, save_steps=500, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=0, mp_parameters=)\u001b[0m\n",
      "\u001b[34m05/16/2022 05:34:31 - INFO - __main__ -   load a local file for train: /opt/ml/input/data/train/100014.csv\u001b[0m\n",
      "\u001b[34m05/16/2022 05:34:31 - INFO - __main__ -   load a local file for validation: /opt/ml/input/data/test/100014.csv\u001b[0m\n",
      "\u001b[34m05/16/2022 05:34:32 - WARNING - datasets.builder -   Using custom data configuration default-81679c45e9f401f7\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-81679c45e9f401f7/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-81679c45e9f401f7/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mtotal labels: ['EPP泡沫', 'T其它', 'T存储', 'T服务器', 'T电脑', 'T网络', 'T网络布线(办公)', '书写工具', '会议桌', '传感器', '低压柜', '保护帽', '保洁', '信号发生器', '光学检测设备 AOI改造和升级', '光谱仪', '其它', '其它 五金件其它', '其它(机械其它）', '其它(气动其它）', '其它(电气其它）', '其它（工具其它）', '其它（非金属材料其它）', '内衬', '冰箱', '冲压设备', '冷却设备', '切片及产品验证', '办公桌', '办公椅', '办公用纸', '功率测试设备', '加热设备', '包材模具相关', '包装设备', '医疗、健康', '印刷品', '压力测量仪', '叉车', '叉车备件', '变压器', '地面', '垫板、垫片', '塑料周转塑料箱', '塑料袋', '外箱', '存储(文件)', '存储(系统)', '实验室工作台', '密封和润滑', '封箱带', '工作台', '工作服装鞋帽', '工作椅', '常用电气', '弹簧', '影像', '性能试验', '恒温恒湿机', '手动', '扎带', '打包带', '打印设备', '投影仪', '排风机', '接插件', '控制器', '支撑板', '数字万用表', '整形设备', '文件储存', '日用杂品', '更衣柜', '服务器(项目)', '机器人', '机床附件和焊接器材', '机械切平设备', '机械脉动试验', '材料', '标签', '气动', '气动执行元件', '气动控制阀', '气动附件', '水槽', '水泵', '洗碗机', '流量测量仪', '测量', '消火栓', '润滑脂', '液压执行元件', '液压控制阀', '液压泵', '液压附件', '清洗类', '清洗设备专用备件', '温度冲击', '温湿度传感器', '灭火器', '灭火用品', '灯具', '炉', '特殊桌面设备', '电动', '电机', '电气仪表', '电气性能测试设备', '电源', '电磁接触器', '电线和电缆', '电话会议设备', '电话系统', '监控探头', '硬度计', '碎纸机', '示波器', '礼品', '空压机', '空调', '空调箱', '立体仓库备件', '管路连接件', '粗糙度仪', '紧固件', '纸护角', '缠绕膜', '耗材', '蒸箱', '螺旋, 带链及齿轮传动件', '衡器', '表面激光成型设备', '计量', '财务用品', '起重, 液压和运输', '车床', '车辆使用的周边设备', '车险', '轴及连接', '轴承', '轴法兰', '运输和起重件', '配电箱', '酸碱溶液', '金属周转器具', '钢材', '钻床', '门窗和家具配件', '阀门', '防冻液', '防护用品', '隔档', '食堂日杂', '马桶', '高位货架备件', '高分子材料分析']\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-05-16 05:34:33,863 >> https://huggingface.co/bert-base-chinese/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9l9kzfll\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-05-16 05:34:34,740 >> storing https://huggingface.co/bert-base-chinese/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-05-16 05:34:34,740 >> creating metadata file for /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:517] 2022-05-16 05:34:34,740 >> loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2022-05-16 05:34:34,742 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\",\n",
      "    \"73\": \"LABEL_73\",\n",
      "    \"74\": \"LABEL_74\",\n",
      "    \"75\": \"LABEL_75\",\n",
      "    \"76\": \"LABEL_76\",\n",
      "    \"77\": \"LABEL_77\",\n",
      "    \"78\": \"LABEL_78\",\n",
      "    \"79\": \"LABEL_79\",\n",
      "    \"80\": \"LABEL_80\",\n",
      "    \"81\": \"LABEL_81\",\n",
      "    \"82\": \"LABEL_82\",\n",
      "    \"83\": \"LABEL_83\",\n",
      "    \"84\": \"LABEL_84\",\n",
      "    \"85\": \"LABEL_85\",\n",
      "    \"86\": \"LABEL_86\",\n",
      "    \"87\": \"LABEL_87\",\n",
      "    \"88\": \"LABEL_88\",\n",
      "    \"89\": \"LABEL_89\",\n",
      "    \"90\": \"LABEL_90\",\n",
      "    \"91\": \"LABEL_91\",\n",
      "    \"92\": \"LABEL_92\",\n",
      "    \"93\": \"LABEL_93\",\n",
      "    \"94\": \"LABEL_94\",\n",
      "    \"95\": \"LABEL_95\",\n",
      "    \"96\": \"LABEL_96\",\n",
      "    \"97\": \"LABEL_97\",\n",
      "    \"98\": \"LABEL_98\",\n",
      "    \"99\": \"LABEL_99\",\n",
      "    \"100\": \"LABEL_100\",\n",
      "    \"101\": \"LABEL_101\",\n",
      "    \"102\": \"LABEL_102\",\n",
      "    \"103\": \"LABEL_103\",\n",
      "    \"104\": \"LABEL_104\",\n",
      "    \"105\": \"LABEL_105\",\n",
      "    \"106\": \"LABEL_106\",\n",
      "    \"107\": \"LABEL_107\",\n",
      "    \"108\": \"LABEL_108\",\n",
      "    \"109\": \"LABEL_109\",\n",
      "    \"110\": \"LABEL_110\",\n",
      "    \"111\": \"LABEL_111\",\n",
      "    \"112\": \"LABEL_112\",\n",
      "    \"113\": \"LABEL_113\",\n",
      "    \"114\": \"LABEL_114\",\n",
      "    \"115\": \"LABEL_115\",\n",
      "    \"116\": \"LABEL_116\",\n",
      "    \"117\": \"LABEL_117\",\n",
      "    \"118\": \"LABEL_118\",\n",
      "    \"119\": \"LABEL_119\",\n",
      "    \"120\": \"LABEL_120\",\n",
      "    \"121\": \"LABEL_121\",\n",
      "    \"122\": \"LABEL_122\",\n",
      "    \"123\": \"LABEL_123\",\n",
      "    \"124\": \"LABEL_124\",\n",
      "    \"125\": \"LABEL_125\",\n",
      "    \"126\": \"LABEL_126\",\n",
      "    \"127\": \"LABEL_127\",\n",
      "    \"128\": \"LABEL_128\",\n",
      "    \"129\": \"LABEL_129\",\n",
      "    \"130\": \"LABEL_130\",\n",
      "    \"131\": \"LABEL_131\",\n",
      "    \"132\": \"LABEL_132\",\n",
      "    \"133\": \"LABEL_133\",\n",
      "    \"134\": \"LABEL_134\",\n",
      "    \"135\": \"LABEL_135\",\n",
      "    \"136\": \"LABEL_136\",\n",
      "    \"137\": \"LABEL_137\",\n",
      "    \"138\": \"LABEL_138\",\n",
      "    \"139\": \"LABEL_139\",\n",
      "    \"140\": \"LABEL_140\",\n",
      "    \"141\": \"LABEL_141\",\n",
      "    \"142\": \"LABEL_142\",\n",
      "    \"143\": \"LABEL_143\",\n",
      "    \"144\": \"LABEL_144\",\n",
      "    \"145\": \"LABEL_145\",\n",
      "    \"146\": \"LABEL_146\",\n",
      "    \"147\": \"LABEL_147\",\n",
      "    \"148\": \"LABEL_148\",\n",
      "    \"149\": \"LABEL_149\",\n",
      "    \"150\": \"LABEL_150\",\n",
      "    \"151\": \"LABEL_151\",\n",
      "    \"152\": \"LABEL_152\",\n",
      "    \"153\": \"LABEL_153\",\n",
      "    \"154\": \"LABEL_154\",\n",
      "    \"155\": \"LABEL_155\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_100\": 100,\n",
      "    \"LABEL_101\": 101,\n",
      "    \"LABEL_102\": 102,\n",
      "    \"LABEL_103\": 103,\n",
      "    \"LABEL_104\": 104,\n",
      "    \"LABEL_105\": 105,\n",
      "    \"LABEL_106\": 106,\n",
      "    \"LABEL_107\": 107,\n",
      "    \"LABEL_108\": 108,\n",
      "    \"LABEL_109\": 109,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_110\": 110,\n",
      "    \"LABEL_111\": 111,\n",
      "    \"LABEL_112\": 112,\n",
      "    \"LABEL_113\": 113,\n",
      "    \"LABEL_114\": 114,\n",
      "    \"LABEL_115\": 115,\n",
      "    \"LABEL_116\": 116,\n",
      "    \"LABEL_117\": 117,\n",
      "    \"LABEL_118\": 118,\n",
      "    \"LABEL_119\": 119,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_120\": 120,\n",
      "    \"LABEL_121\": 121,\n",
      "    \"LABEL_122\": 122,\n",
      "    \"LABEL_123\": 123,\n",
      "    \"LABEL_124\": 124,\n",
      "    \"LABEL_125\": 125,\n",
      "    \"LABEL_126\": 126,\n",
      "    \"LABEL_127\": 127,\n",
      "    \"LABEL_128\": 128,\n",
      "    \"LABEL_129\": 129,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_130\": 130,\n",
      "    \"LABEL_131\": 131,\n",
      "    \"LABEL_132\": 132,\n",
      "    \"LABEL_133\": 133,\n",
      "    \"LABEL_134\": 134,\n",
      "    \"LABEL_135\": 135,\n",
      "    \"LABEL_136\": 136,\n",
      "    \"LABEL_137\": 137,\n",
      "    \"LABEL_138\": 138,\n",
      "    \"LABEL_139\": 139,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_140\": 140,\n",
      "    \"LABEL_141\": 141,\n",
      "    \"LABEL_142\": 142,\n",
      "    \"LABEL_143\": 143,\n",
      "    \"LABEL_144\": 144,\n",
      "    \"LABEL_145\": 145,\n",
      "    \"LABEL_146\": 146,\n",
      "    \"LABEL_147\": 147,\n",
      "    \"LABEL_148\": 148,\n",
      "    \"LABEL_149\": 149,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_150\": 150,\n",
      "    \"LABEL_151\": 151,\n",
      "    \"LABEL_152\": 152,\n",
      "    \"LABEL_153\": 153,\n",
      "    \"LABEL_154\": 154,\n",
      "    \"LABEL_155\": 155,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_73\": 73,\n",
      "    \"LABEL_74\": 74,\n",
      "    \"LABEL_75\": 75,\n",
      "    \"LABEL_76\": 76,\n",
      "    \"LABEL_77\": 77,\n",
      "    \"LABEL_78\": 78,\n",
      "    \"LABEL_79\": 79,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_80\": 80,\n",
      "    \"LABEL_81\": 81,\n",
      "    \"LABEL_82\": 82,\n",
      "    \"LABEL_83\": 83,\n",
      "    \"LABEL_84\": 84,\n",
      "    \"LABEL_85\": 85,\n",
      "    \"LABEL_86\": 86,\n",
      "    \"LABEL_87\": 87,\n",
      "    \"LABEL_88\": 88,\n",
      "    \"LABEL_89\": 89,\n",
      "    \"LABEL_9\": 9,\n",
      "    \"LABEL_90\": 90,\n",
      "    \"LABEL_91\": 91,\n",
      "    \"LABEL_92\": 92,\n",
      "    \"LABEL_93\": 93,\n",
      "    \"LABEL_94\": 94,\n",
      "    \"LABEL_95\": 95,\n",
      "    \"LABEL_96\": 96,\n",
      "    \"LABEL_97\": 97,\n",
      "    \"LABEL_98\": 98,\n",
      "    \"LABEL_99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:517] 2022-05-16 05:34:35,812 >> loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2022-05-16 05:34:35,813 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-05-16 05:34:36,717 >> https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzgz5ll5b\u001b[0m\n",
      "\n",
      "2022-05-16 05:34:44 Training - Training image download completed. Training in progress.\u001b[34m[INFO|file_utils.py:1536] 2022-05-16 05:34:38,316 >> storing https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-05-16 05:34:38,316 >> creating metadata file for /root/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-05-16 05:34:39,188 >> https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpl8bf_h5m\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-05-16 05:34:41,225 >> storing https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-05-16 05:34:41,225 >> creating metadata file for /root/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-05-16 05:34:43,968 >> https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpibljfyfp\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-05-16 05:34:45,024 >> storing https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-05-16 05:34:45,024 >> creating metadata file for /root/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-05-16 05:34:45,025 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-05-16 05:34:45,025 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-05-16 05:34:45,025 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-05-16 05:34:45,025 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-05-16 05:34:45,025 >> loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-05-16 05:34:45,936 >> https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxxhcn9ov\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-05-16 05:35:07,821 >> storing https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-05-16 05:35:07,821 >> creating metadata file for /root/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1155] 2022-05-16 05:35:07,821 >> loading weights file https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1331] 2022-05-16 05:35:09,293 >> Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1342] 2022-05-16 05:35:09,293 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m05/16/2022 05:35:10 - INFO - __main__ -   Sample 5238 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 7392, 5606, 3808, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 93, 'sentence1': '隔膜泵', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\u001b[0m\n",
      "\u001b[34m05/16/2022 05:35:10 - INFO - __main__ -   Sample 912 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1461, 1429, 3322, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 35, 'sentence1': '呼吸机', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\u001b[0m\n",
      "\u001b[34m05/16/2022 05:35:10 - INFO - __main__ -   Sample 204 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 3344, 1072, 1947, 6163, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 51, 'sentence1': '杯具套装', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:516] 2022-05-16 05:35:10,200 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1156] 2022-05-16 05:35:10,302 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1157] 2022-05-16 05:35:10,302 >>   Num examples = 7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1158] 2022-05-16 05:35:10,302 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1159] 2022-05-16 05:35:10,302 >>   Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1160] 2022-05-16 05:35:10,303 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1161] 2022-05-16 05:35:10,303 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1162] 2022-05-16 05:35:10,303 >>   Total optimization steps = 225\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.761 algo-1:33 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.894 algo-1:33 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.895 algo-1:33 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.895 algo-1:33 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.896 algo-1:33 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.896 algo-1:33 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.912 algo-1:33 INFO hook.py:591] name:bert.embeddings.word_embeddings.weight count_params:16226304\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.912 algo-1:33 INFO hook.py:591] name:bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.912 algo-1:33 INFO hook.py:591] name:bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.913 algo-1:33 INFO hook.py:591] name:bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.913 algo-1:33 INFO hook.py:591] name:bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.913 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.913 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.913 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.913 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.913 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.913 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.913 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.913 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.914 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.914 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.914 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.914 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.914 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.914 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.914 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.914 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.914 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.914 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.915 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.915 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.915 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.915 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.915 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.915 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.915 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.915 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.915 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.916 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.916 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.916 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.916 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.916 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.916 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.916 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.916 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.916 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.917 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.917 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.917 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.917 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.917 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.917 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.917 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.917 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.917 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.918 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.918 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.918 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.918 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.918 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.918 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.918 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.918 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.919 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.919 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.919 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.919 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.919 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.919 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.919 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.919 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.919 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.919 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.920 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.920 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.920 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.920 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.920 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.920 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.920 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.920 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.920 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.920 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.920 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.920 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.921 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.921 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.921 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.921 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.921 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.921 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.921 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.921 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.921 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.921 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.922 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.922 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.922 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.922 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.922 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.922 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.922 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.922 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.922 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.923 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.923 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.923 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.923 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.923 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.923 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.923 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.923 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.923 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.923 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.923 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.924 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.924 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.924 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.924 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.924 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.924 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.924 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.924 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.924 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.924 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.925 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.925 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.925 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.925 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.925 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.925 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.925 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.925 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.925 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.925 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.925 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.926 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.926 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.926 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.926 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.926 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.926 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.926 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.926 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.926 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.926 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.926 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.927 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.927 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.927 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.927 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.927 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.927 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.927 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.927 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.928 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.928 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.928 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.928 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.928 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.928 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.928 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.928 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.928 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.929 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.929 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.929 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.929 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.929 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.929 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.929 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.929 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.929 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.929 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.929 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.930 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.930 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.930 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.930 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.930 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.930 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.930 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.930 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.930 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.930 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.930 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.931 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.931 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.931 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.931 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.931 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.931 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.931 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.931 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.931 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.931 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.932 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.932 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.932 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.932 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.932 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.932 algo-1:33 INFO hook.py:591] name:bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.932 algo-1:33 INFO hook.py:591] name:bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.932 algo-1:33 INFO hook.py:591] name:bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.932 algo-1:33 INFO hook.py:591] name:classifier.weight count_params:119808\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.932 algo-1:33 INFO hook.py:591] name:classifier.bias count_params:156\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.933 algo-1:33 INFO hook.py:593] Total Trainable Params: 102387612\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.933 algo-1:33 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-05-16 05:35:10.934 algo-1:33 INFO hook.py:488] Hook is writing from the hook with pid: 33\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(\n",
    "  {'train': data_location+'/100014.csv',\n",
    "   'test': data_location+'/100014.csv'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d412a",
   "metadata": {},
   "source": [
    "# 模型部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "779739ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5522355a",
   "metadata": {},
   "source": [
    "# 模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fcc8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel, HuggingFacePredictor\n",
    "import sagemaker\n",
    "\n",
    "huggingface_predictor=HuggingFacePredictor(\n",
    "    endpoint_name='huggingface-pytorch-training-2022-04-20-07-39-48-503',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56170e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_36', 'score': 0.9989555478096008}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_predictor.predict({'inputs': \"连环画\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d24de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs=['EPP泡沫', 'T其它', 'T存储', 'T服务器', 'T电脑', 'T网络', 'T网络布线(办公)', '书写工具', '会议桌', '传感器', '低压柜', '保护帽', '保洁', '信号发生器', '光学检测设备 AOI改造和升级', '光谱仪', '其它', '其它 五金件其它', '其它(机械其它）', '其它(气动其它）', '其它(电气其它）', '其它（工具其它）', '其它（非金属材料其它）', '内衬', '冰箱', '冲压设备', '冷却设备', '切片及产品验证', '办公桌', '办公椅', '办公用纸', '功率测试设备', '加热设备', '包材模具相关', '包装设备', '医疗、健康', '印刷品', '压力测量仪', '叉车', '叉车备件', '变压器', '地面', '垫板、垫片', '塑料周转塑料箱', '塑料袋', '外箱', '存储(文件)', '存储(系统)', '实验室工作台', '密封和润滑', '封箱带', '工作台', '工作服装鞋帽', '工作椅', '常用电气', '弹簧', '影像', '性能试验', '恒温恒湿机', '手动', '扎带', '打包带', '打印设备', '投影仪', '排风机', '接插件', '控制器', '支撑板', '数字万用表', '整形设备', '文件储存', '日用杂品', '更衣柜', '服务器(项目)', '机器人', '机床附件和焊接器材', '机械切平设备', '机械脉动试验', '材料', '标签', '气动', '气动执行元件', '气动控制阀', '气动附件', '水槽', '水泵', '洗碗机', '流量测量仪', '测量', '消火栓', '润滑脂', '液压执行元件', '液压控制阀', '液压泵', '液压附件', '清洗类', '清洗设备专用备件', '温度冲击', '温湿度传感器', '灭火器', '灭火用品', '灯具', '炉', '特殊桌面设备', '电动', '电机', '电气仪表', '电气性能测试设备', '电源', '电磁接触器', '电线和电缆', '电话会议设备', '电话系统', '监控探头', '硬度计', '碎纸机', '示波器', '礼品', '空压机', '空调', '空调箱', '立体仓库备件', '管路连接件', '粗糙度仪', '紧固件', '纸护角', '缠绕膜', '耗材', '蒸箱', '螺旋, 带链及齿轮传动件', '衡器', '表面激光成型设备', '计量', '财务用品', '起重, 液压和运输', '车床', '车辆使用的周边设备', '车险', '轴及连接', '轴承', '轴法兰', '运输和起重件', '配电箱', '酸碱溶液', '金属周转器具', '钢材', '钻床', '门窗和家具配件', '阀门', '防冻液', '防护用品', '隔档', '食堂日杂', '马桶', '高位货架备件', '高分子材料分析']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c132bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'印刷品'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labs[36]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bee169e",
   "metadata": {},
   "source": [
    "# 直接通过训练任务部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "#    env= {'HF_TASK':'text-generation'},\n",
    "   model_data=\"s3://sagemaker-us-west-2-847380964353/huggingface-pytorch-training-2022-03-30-02-45-50-596/output/model.tar.gz\",  # 根据您的自行替换\n",
    "   role=role,                                            # IAM role with permissions to create an endpoint\n",
    "   transformers_version=\"4.6\",                           # Transformers version used\n",
    "   pytorch_version=\"1.7\",                                # PyTorch version used\n",
    "   py_version='py36',                                    # Python version used\n",
    "    \n",
    ")\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.m5.2xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c42d18b",
   "metadata": {},
   "source": [
    "# 增量训练（前提是类别数不会增加，跟原有的保持一致）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc20cd4",
   "metadata": {},
   "source": [
    "## 下载之前训练好的模型文件并解压"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4515c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘last_model’: File exists\n",
      "/home/ec2-user/SageMaker/zhenyun/last_model\n",
      "Note: AWS CLI version 2, the latest major version of the AWS CLI, is now stable and recommended for general use. For more information, see the AWS CLI version 2 installation instructions at: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html\n",
      "\n",
      "usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters]\n",
      "To see help text, you can run:\n",
      "\n",
      "  aws help\n",
      "  aws <command> help\n",
      "  aws <command> <subcommand> help\n",
      "aws: error: the following arguments are required: paths\n",
      "eval_results.json\n",
      "special_tokens_map.json\n",
      "trainer_state.json\n",
      "all_results.json\n",
      "training_args.bin\n",
      "config.json\n",
      "train_results.json\n",
      "tokenizer_config.json\n",
      "tokenizer.json\n",
      "checkpoint-225/\n",
      "checkpoint-225/special_tokens_map.json\n",
      "checkpoint-225/trainer_state.json\n",
      "checkpoint-225/training_args.bin\n",
      "checkpoint-225/config.json\n",
      "checkpoint-225/tokenizer_config.json\n",
      "checkpoint-225/rng_state.pth\n",
      "checkpoint-225/tokenizer.json\n",
      "checkpoint-225/pytorch_model.bin\n",
      "checkpoint-225/optimizer.pt\n",
      "checkpoint-225/vocab.txt\n",
      "checkpoint-225/scheduler.pt\n",
      "pytorch_model.bin\n",
      "vocab.txt\n",
      "/home/ec2-user/SageMaker/zhenyun\n"
     ]
    }
   ],
   "source": [
    "!rm -rf model.tar.gz\n",
    "!mkdir last_model\n",
    "%cd last_model\n",
    "!aws s3 cp s3://sagemaker-us-west-2-847380964353/huggingface-pytorch-training-2022-04-29-02-24-49-537/output/model.tar.gz\n",
    "!tar -xvf model.tar.gz\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637b79c",
   "metadata": {},
   "source": [
    "## 上传模型文件到S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fba24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = './last_model/'\n",
    "s3_prefix = 'models/zhenyun'\n",
    "data_location2 = sess.upload_data(WORK_DIRECTORY, key_prefix=s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4094ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-847380964353/models/zhenyun'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_location2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b4d91",
   "metadata": {},
   "source": [
    "## 配置训练参数，注意这里的'model_name_or_path'已经改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c1656d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "# hyperparameters which are passed to the training job\n",
    "hyperparameters={'num_train_epochs': 10,\n",
    "                 'train_file':'/opt/ml/input/data/train/100014.csv',\n",
    "                 'validation_file':'/opt/ml/input/data/test/100014.csv',\n",
    "                 'output_dir':'/opt/ml/model',\n",
    "                 'max_seq_length': 128,\n",
    "                 'model_name_or_path': '/opt/ml/input/data/last_model',\n",
    "                 'learning_rate': 2e-5,\n",
    "                 'num_train_epochs': 1,\n",
    "                 'per_device_train_batch_size': 32,\n",
    "                 'save_strategy':'epoch',\n",
    "                 'save_total_limit':1,\n",
    "                 }\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='run_glue.py',\n",
    "        source_dir='./scripts',\n",
    "        instance_type='ml.p3.2xlarge',\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        image_uri='015708445809.dkr.ecr.cn-north-1.amazonaws.com.cn/hf64',\n",
    "#         transformers_version='4.6',\n",
    "#         pytorch_version='1.7',\n",
    "        py_version='py36',\n",
    "        hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af153daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-09 08:36:59 Starting - Starting the training job...\n",
      "2022-05-09 08:37:26 Starting - Preparing the instances for trainingProfilerReport-1652085418: InProgress\n",
      ".........\n",
      "2022-05-09 08:38:43 Downloading - Downloading input data...\n",
      "2022-05-09 08:39:23 Training - Downloading the training image..................\n",
      "2022-05-09 08:42:27 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-05-09 08:42:29,993 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-05-09 08:42:30,016 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-05-09 08:42:30,024 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-05-09 08:42:30,374 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting accelerate\n",
      "  Downloading accelerate-0.7.1-py3-none-any.whl (79 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.1.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (3.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->-r requirements.txt (line 5)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r requirements.txt (line 2)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from accelerate->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.1.3->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.1.3->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: accelerate\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.7.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-05-09 08:42:33,415 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"last_model\": \"/opt/ml/input/data/last_model\",\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"learning_rate\": 2e-05,\n",
      "        \"max_seq_length\": 128,\n",
      "        \"model_name_or_path\": \"/opt/ml/input/data/last_model\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_train_batch_size\": 32,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"save_total_limit\": 1,\n",
      "        \"train_file\": \"/opt/ml/input/data/train/100014.csv\",\n",
      "        \"validation_file\": \"/opt/ml/input/data/test/100014.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"last_model\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-05-09-08-36-58-530\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-847380964353/huggingface-pytorch-training-2022-05-09-08-36-58-530/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_glue\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_glue.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"learning_rate\":2e-05,\"max_seq_length\":128,\"model_name_or_path\":\"/opt/ml/input/data/last_model\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"per_device_train_batch_size\":32,\"save_strategy\":\"epoch\",\"save_total_limit\":1,\"train_file\":\"/opt/ml/input/data/train/100014.csv\",\"validation_file\":\"/opt/ml/input/data/test/100014.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_glue.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"last_model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"last_model\",\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_glue\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-847380964353/huggingface-pytorch-training-2022-05-09-08-36-58-530/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"last_model\":\"/opt/ml/input/data/last_model\",\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"learning_rate\":2e-05,\"max_seq_length\":128,\"model_name_or_path\":\"/opt/ml/input/data/last_model\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"per_device_train_batch_size\":32,\"save_strategy\":\"epoch\",\"save_total_limit\":1,\"train_file\":\"/opt/ml/input/data/train/100014.csv\",\"validation_file\":\"/opt/ml/input/data/test/100014.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"last_model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-05-09-08-36-58-530\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-847380964353/huggingface-pytorch-training-2022-05-09-08-36-58-530/source/sourcedir.tar.gz\",\"module_name\":\"run_glue\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_glue.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--learning_rate\",\"2e-05\",\"--max_seq_length\",\"128\",\"--model_name_or_path\",\"/opt/ml/input/data/last_model\",\"--num_train_epochs\",\"1\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_train_batch_size\",\"32\",\"--save_strategy\",\"epoch\",\"--save_total_limit\",\"1\",\"--train_file\",\"/opt/ml/input/data/train/100014.csv\",\"--validation_file\",\"/opt/ml/input/data/test/100014.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_LAST_MODEL=/opt/ml/input/data/last_model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=/opt/ml/input/data/last_model\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=1\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/train/100014.csv\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FILE=/opt/ml/input/data/test/100014.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 run_glue.py --learning_rate 2e-05 --max_seq_length 128 --model_name_or_path /opt/ml/input/data/last_model --num_train_epochs 1 --output_dir /opt/ml/model --per_device_train_batch_size 32 --save_strategy epoch --save_total_limit 1 --train_file /opt/ml/input/data/train/100014.csv --validation_file /opt/ml/input/data/test/100014.csv\u001b[0m\n",
      "\u001b[34m05/09/2022 08:42:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m05/09/2022 08:42:38 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May09_08-42-38_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.EPOCH, save_steps=500, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\u001b[0m\n",
      "\u001b[34m05/09/2022 08:42:38 - INFO - __main__ -   load a local file for train: /opt/ml/input/data/train/100014.csv\u001b[0m\n",
      "\u001b[34m05/09/2022 08:42:38 - INFO - __main__ -   load a local file for validation: /opt/ml/input/data/test/100014.csv\u001b[0m\n",
      "\u001b[34m05/09/2022 08:42:38 - WARNING - datasets.builder -   Using custom data configuration default-2c57351827ad5222\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-2c57351827ad5222/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-2c57351827ad5222/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mtotal labels: ['EPP泡沫', 'T其它', 'T存储', 'T服务器', 'T电脑', 'T网络', 'T网络布线(办公)', '书写工具', '会议桌', '传感器', '低压柜', '保护帽', '保洁', '信号发生器', '光学检测设备 AOI改造和升级', '光谱仪', '其它', '其它 五金件其它', '其它(机械其它）', '其它(气动其它）', '其它(电气其它）', '其它（工具其它）', '其它（非金属材料其它）', '内衬', '冰箱', '冲压设备', '冷却设备', '切片及产品验证', '办公桌', '办公椅', '办公用纸', '功率测试设备', '加热设备', '包材模具相关', '包装设备', '医疗、健康', '印刷品', '压力测量仪', '叉车', '叉车备件', '变压器', '地面', '垫板、垫片', '塑料周转塑料箱', '塑料袋', '外箱', '存储(文件)', '存储(系统)', '实验室工作台', '密封和润滑', '封箱带', '工作台', '工作服装鞋帽', '工作椅', '常用电气', '弹簧', '影像', '性能试验', '恒温恒湿机', '手动', '扎带', '打包带', '打印设备', '投影仪', '排风机', '接插件', '控制器', '支撑板', '数字万用表', '整形设备', '文件储存', '日用杂品', '更衣柜', '服务器(项目)', '机器人', '机床附件和焊接器材', '机械切平设备', '机械脉动试验', '材料', '标签', '气动', '气动执行元件', '气动控制阀', '气动附件', '水槽', '水泵', '洗碗机', '流量测量仪', '测量', '消火栓', '润滑脂', '液压执行元件', '液压控制阀', '液压泵', '液压附件', '清洗类', '清洗设备专用备件', '温度冲击', '温湿度传感器', '灭火器', '灭火用品', '灯具', '炉', '特殊桌面设备', '电动', '电机', '电气仪表', '电气性能测试设备', '电源', '电磁接触器', '电线和电缆', '电话会议设备', '电话系统', '监控探头', '硬度计', '碎纸机', '示波器', '礼品', '空压机', '空调', '空调箱', '立体仓库备件', '管路连接件', '粗糙度仪', '紧固件', '纸护角', '缠绕膜', '耗材', '蒸箱', '螺旋, 带链及齿轮传动件', '衡器', '表面激光成型设备', '计量', '财务用品', '起重, 液压和运输', '车床', '车辆使用的周边设备', '车险', '轴及连接', '轴承', '轴法兰', '运输和起重件', '配电箱', '酸碱溶液', '金属周转器具', '钢材', '钻床', '门窗和家具配件', '阀门', '防冻液', '防护用品', '隔档', '食堂日杂', '马桶', '高位货架备件', '高分子材料分析']\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:515] 2022-05-09 08:42:38,954 >> loading configuration file /opt/ml/input/data/last_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2022-05-09 08:42:38,956 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\",\n",
      "    \"73\": \"LABEL_73\",\n",
      "    \"74\": \"LABEL_74\",\n",
      "    \"75\": \"LABEL_75\",\n",
      "    \"76\": \"LABEL_76\",\n",
      "    \"77\": \"LABEL_77\",\n",
      "    \"78\": \"LABEL_78\",\n",
      "    \"79\": \"LABEL_79\",\n",
      "    \"80\": \"LABEL_80\",\n",
      "    \"81\": \"LABEL_81\",\n",
      "    \"82\": \"LABEL_82\",\n",
      "    \"83\": \"LABEL_83\",\n",
      "    \"84\": \"LABEL_84\",\n",
      "    \"85\": \"LABEL_85\",\n",
      "    \"86\": \"LABEL_86\",\n",
      "    \"87\": \"LABEL_87\",\n",
      "    \"88\": \"LABEL_88\",\n",
      "    \"89\": \"LABEL_89\",\n",
      "    \"90\": \"LABEL_90\",\n",
      "    \"91\": \"LABEL_91\",\n",
      "    \"92\": \"LABEL_92\",\n",
      "    \"93\": \"LABEL_93\",\n",
      "    \"94\": \"LABEL_94\",\n",
      "    \"95\": \"LABEL_95\",\n",
      "    \"96\": \"LABEL_96\",\n",
      "    \"97\": \"LABEL_97\",\n",
      "    \"98\": \"LABEL_98\",\n",
      "    \"99\": \"LABEL_99\",\n",
      "    \"100\": \"LABEL_100\",\n",
      "    \"101\": \"LABEL_101\",\n",
      "    \"102\": \"LABEL_102\",\n",
      "    \"103\": \"LABEL_103\",\n",
      "    \"104\": \"LABEL_104\",\n",
      "    \"105\": \"LABEL_105\",\n",
      "    \"106\": \"LABEL_106\",\n",
      "    \"107\": \"LABEL_107\",\n",
      "    \"108\": \"LABEL_108\",\n",
      "    \"109\": \"LABEL_109\",\n",
      "    \"110\": \"LABEL_110\",\n",
      "    \"111\": \"LABEL_111\",\n",
      "    \"112\": \"LABEL_112\",\n",
      "    \"113\": \"LABEL_113\",\n",
      "    \"114\": \"LABEL_114\",\n",
      "    \"115\": \"LABEL_115\",\n",
      "    \"116\": \"LABEL_116\",\n",
      "    \"117\": \"LABEL_117\",\n",
      "    \"118\": \"LABEL_118\",\n",
      "    \"119\": \"LABEL_119\",\n",
      "    \"120\": \"LABEL_120\",\n",
      "    \"121\": \"LABEL_121\",\n",
      "    \"122\": \"LABEL_122\",\n",
      "    \"123\": \"LABEL_123\",\n",
      "    \"124\": \"LABEL_124\",\n",
      "    \"125\": \"LABEL_125\",\n",
      "    \"126\": \"LABEL_126\",\n",
      "    \"127\": \"LABEL_127\",\n",
      "    \"128\": \"LABEL_128\",\n",
      "    \"129\": \"LABEL_129\",\n",
      "    \"130\": \"LABEL_130\",\n",
      "    \"131\": \"LABEL_131\",\n",
      "    \"132\": \"LABEL_132\",\n",
      "    \"133\": \"LABEL_133\",\n",
      "    \"134\": \"LABEL_134\",\n",
      "    \"135\": \"LABEL_135\",\n",
      "    \"136\": \"LABEL_136\",\n",
      "    \"137\": \"LABEL_137\",\n",
      "    \"138\": \"LABEL_138\",\n",
      "    \"139\": \"LABEL_139\",\n",
      "    \"140\": \"LABEL_140\",\n",
      "    \"141\": \"LABEL_141\",\n",
      "    \"142\": \"LABEL_142\",\n",
      "    \"143\": \"LABEL_143\",\n",
      "    \"144\": \"LABEL_144\",\n",
      "    \"145\": \"LABEL_145\",\n",
      "    \"146\": \"LABEL_146\",\n",
      "    \"147\": \"LABEL_147\",\n",
      "    \"148\": \"LABEL_148\",\n",
      "    \"149\": \"LABEL_149\",\n",
      "    \"150\": \"LABEL_150\",\n",
      "    \"151\": \"LABEL_151\",\n",
      "    \"152\": \"LABEL_152\",\n",
      "    \"153\": \"LABEL_153\",\n",
      "    \"154\": \"LABEL_154\",\n",
      "    \"155\": \"LABEL_155\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_100\": 100,\n",
      "    \"LABEL_101\": 101,\n",
      "    \"LABEL_102\": 102,\n",
      "    \"LABEL_103\": 103,\n",
      "    \"LABEL_104\": 104,\n",
      "    \"LABEL_105\": 105,\n",
      "    \"LABEL_106\": 106,\n",
      "    \"LABEL_107\": 107,\n",
      "    \"LABEL_108\": 108,\n",
      "    \"LABEL_109\": 109,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_110\": 110,\n",
      "    \"LABEL_111\": 111,\n",
      "    \"LABEL_112\": 112,\n",
      "    \"LABEL_113\": 113,\n",
      "    \"LABEL_114\": 114,\n",
      "    \"LABEL_115\": 115,\n",
      "    \"LABEL_116\": 116,\n",
      "    \"LABEL_117\": 117,\n",
      "    \"LABEL_118\": 118,\n",
      "    \"LABEL_119\": 119,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_120\": 120,\n",
      "    \"LABEL_121\": 121,\n",
      "    \"LABEL_122\": 122,\n",
      "    \"LABEL_123\": 123,\n",
      "    \"LABEL_124\": 124,\n",
      "    \"LABEL_125\": 125,\n",
      "    \"LABEL_126\": 126,\n",
      "    \"LABEL_127\": 127,\n",
      "    \"LABEL_128\": 128,\n",
      "    \"LABEL_129\": 129,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_130\": 130,\n",
      "    \"LABEL_131\": 131,\n",
      "    \"LABEL_132\": 132,\n",
      "    \"LABEL_133\": 133,\n",
      "    \"LABEL_134\": 134,\n",
      "    \"LABEL_135\": 135,\n",
      "    \"LABEL_136\": 136,\n",
      "    \"LABEL_137\": 137,\n",
      "    \"LABEL_138\": 138,\n",
      "    \"LABEL_139\": 139,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_140\": 140,\n",
      "    \"LABEL_141\": 141,\n",
      "    \"LABEL_142\": 142,\n",
      "    \"LABEL_143\": 143,\n",
      "    \"LABEL_144\": 144,\n",
      "    \"LABEL_145\": 145,\n",
      "    \"LABEL_146\": 146,\n",
      "    \"LABEL_147\": 147,\n",
      "    \"LABEL_148\": 148,\n",
      "    \"LABEL_149\": 149,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_150\": 150,\n",
      "    \"LABEL_151\": 151,\n",
      "    \"LABEL_152\": 152,\n",
      "    \"LABEL_153\": 153,\n",
      "    \"LABEL_154\": 154,\n",
      "    \"LABEL_155\": 155,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_73\": 73,\n",
      "    \"LABEL_74\": 74,\n",
      "    \"LABEL_75\": 75,\n",
      "    \"LABEL_76\": 76,\n",
      "    \"LABEL_77\": 77,\n",
      "    \"LABEL_78\": 78,\n",
      "    \"LABEL_79\": 79,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_80\": 80,\n",
      "    \"LABEL_81\": 81,\n",
      "    \"LABEL_82\": 82,\n",
      "    \"LABEL_83\": 83,\n",
      "    \"LABEL_84\": 84,\n",
      "    \"LABEL_85\": 85,\n",
      "    \"LABEL_86\": 86,\n",
      "    \"LABEL_87\": 87,\n",
      "    \"LABEL_88\": 88,\n",
      "    \"LABEL_89\": 89,\n",
      "    \"LABEL_9\": 9,\n",
      "    \"LABEL_90\": 90,\n",
      "    \"LABEL_91\": 91,\n",
      "    \"LABEL_92\": 92,\n",
      "    \"LABEL_93\": 93,\n",
      "    \"LABEL_94\": 94,\n",
      "    \"LABEL_95\": 95,\n",
      "    \"LABEL_96\": 96,\n",
      "    \"LABEL_97\": 97,\n",
      "    \"LABEL_98\": 98,\n",
      "    \"LABEL_99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:515] 2022-05-09 08:42:38,956 >> loading configuration file /opt/ml/input/data/last_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2022-05-09 08:42:38,959 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\",\n",
      "    \"73\": \"LABEL_73\",\n",
      "    \"74\": \"LABEL_74\",\n",
      "    \"75\": \"LABEL_75\",\n",
      "    \"76\": \"LABEL_76\",\n",
      "    \"77\": \"LABEL_77\",\n",
      "    \"78\": \"LABEL_78\",\n",
      "    \"79\": \"LABEL_79\",\n",
      "    \"80\": \"LABEL_80\",\n",
      "    \"81\": \"LABEL_81\",\n",
      "    \"82\": \"LABEL_82\",\n",
      "    \"83\": \"LABEL_83\",\n",
      "    \"84\": \"LABEL_84\",\n",
      "    \"85\": \"LABEL_85\",\n",
      "    \"86\": \"LABEL_86\",\n",
      "    \"87\": \"LABEL_87\",\n",
      "    \"88\": \"LABEL_88\",\n",
      "    \"89\": \"LABEL_89\",\n",
      "    \"90\": \"LABEL_90\",\n",
      "    \"91\": \"LABEL_91\",\n",
      "    \"92\": \"LABEL_92\",\n",
      "    \"93\": \"LABEL_93\",\n",
      "    \"94\": \"LABEL_94\",\n",
      "    \"95\": \"LABEL_95\",\n",
      "    \"96\": \"LABEL_96\",\n",
      "    \"97\": \"LABEL_97\",\n",
      "    \"98\": \"LABEL_98\",\n",
      "    \"99\": \"LABEL_99\",\n",
      "    \"100\": \"LABEL_100\",\n",
      "    \"101\": \"LABEL_101\",\n",
      "    \"102\": \"LABEL_102\",\n",
      "    \"103\": \"LABEL_103\",\n",
      "    \"104\": \"LABEL_104\",\n",
      "    \"105\": \"LABEL_105\",\n",
      "    \"106\": \"LABEL_106\",\n",
      "    \"107\": \"LABEL_107\",\n",
      "    \"108\": \"LABEL_108\",\n",
      "    \"109\": \"LABEL_109\",\n",
      "    \"110\": \"LABEL_110\",\n",
      "    \"111\": \"LABEL_111\",\n",
      "    \"112\": \"LABEL_112\",\n",
      "    \"113\": \"LABEL_113\",\n",
      "    \"114\": \"LABEL_114\",\n",
      "    \"115\": \"LABEL_115\",\n",
      "    \"116\": \"LABEL_116\",\n",
      "    \"117\": \"LABEL_117\",\n",
      "    \"118\": \"LABEL_118\",\n",
      "    \"119\": \"LABEL_119\",\n",
      "    \"120\": \"LABEL_120\",\n",
      "    \"121\": \"LABEL_121\",\n",
      "    \"122\": \"LABEL_122\",\n",
      "    \"123\": \"LABEL_123\",\n",
      "    \"124\": \"LABEL_124\",\n",
      "    \"125\": \"LABEL_125\",\n",
      "    \"126\": \"LABEL_126\",\n",
      "    \"127\": \"LABEL_127\",\n",
      "    \"128\": \"LABEL_128\",\n",
      "    \"129\": \"LABEL_129\",\n",
      "    \"130\": \"LABEL_130\",\n",
      "    \"131\": \"LABEL_131\",\n",
      "    \"132\": \"LABEL_132\",\n",
      "    \"133\": \"LABEL_133\",\n",
      "    \"134\": \"LABEL_134\",\n",
      "    \"135\": \"LABEL_135\",\n",
      "    \"136\": \"LABEL_136\",\n",
      "    \"137\": \"LABEL_137\",\n",
      "    \"138\": \"LABEL_138\",\n",
      "    \"139\": \"LABEL_139\",\n",
      "    \"140\": \"LABEL_140\",\n",
      "    \"141\": \"LABEL_141\",\n",
      "    \"142\": \"LABEL_142\",\n",
      "    \"143\": \"LABEL_143\",\n",
      "    \"144\": \"LABEL_144\",\n",
      "    \"145\": \"LABEL_145\",\n",
      "    \"146\": \"LABEL_146\",\n",
      "    \"147\": \"LABEL_147\",\n",
      "    \"148\": \"LABEL_148\",\n",
      "    \"149\": \"LABEL_149\",\n",
      "    \"150\": \"LABEL_150\",\n",
      "    \"151\": \"LABEL_151\",\n",
      "    \"152\": \"LABEL_152\",\n",
      "    \"153\": \"LABEL_153\",\n",
      "    \"154\": \"LABEL_154\",\n",
      "    \"155\": \"LABEL_155\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_100\": 100,\n",
      "    \"LABEL_101\": 101,\n",
      "    \"LABEL_102\": 102,\n",
      "    \"LABEL_103\": 103,\n",
      "    \"LABEL_104\": 104,\n",
      "    \"LABEL_105\": 105,\n",
      "    \"LABEL_106\": 106,\n",
      "    \"LABEL_107\": 107,\n",
      "    \"LABEL_108\": 108,\n",
      "    \"LABEL_109\": 109,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_110\": 110,\n",
      "    \"LABEL_111\": 111,\n",
      "    \"LABEL_112\": 112,\n",
      "    \"LABEL_113\": 113,\n",
      "    \"LABEL_114\": 114,\n",
      "    \"LABEL_115\": 115,\n",
      "    \"LABEL_116\": 116,\n",
      "    \"LABEL_117\": 117,\n",
      "    \"LABEL_118\": 118,\n",
      "    \"LABEL_119\": 119,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_120\": 120,\n",
      "    \"LABEL_121\": 121,\n",
      "    \"LABEL_122\": 122,\n",
      "    \"LABEL_123\": 123,\n",
      "    \"LABEL_124\": 124,\n",
      "    \"LABEL_125\": 125,\n",
      "    \"LABEL_126\": 126,\n",
      "    \"LABEL_127\": 127,\n",
      "    \"LABEL_128\": 128,\n",
      "    \"LABEL_129\": 129,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_130\": 130,\n",
      "    \"LABEL_131\": 131,\n",
      "    \"LABEL_132\": 132,\n",
      "    \"LABEL_133\": 133,\n",
      "    \"LABEL_134\": 134,\n",
      "    \"LABEL_135\": 135,\n",
      "    \"LABEL_136\": 136,\n",
      "    \"LABEL_137\": 137,\n",
      "    \"LABEL_138\": 138,\n",
      "    \"LABEL_139\": 139,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_140\": 140,\n",
      "    \"LABEL_141\": 141,\n",
      "    \"LABEL_142\": 142,\n",
      "    \"LABEL_143\": 143,\n",
      "    \"LABEL_144\": 144,\n",
      "    \"LABEL_145\": 145,\n",
      "    \"LABEL_146\": 146,\n",
      "    \"LABEL_147\": 147,\n",
      "    \"LABEL_148\": 148,\n",
      "    \"LABEL_149\": 149,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_150\": 150,\n",
      "    \"LABEL_151\": 151,\n",
      "    \"LABEL_152\": 152,\n",
      "    \"LABEL_153\": 153,\n",
      "    \"LABEL_154\": 154,\n",
      "    \"LABEL_155\": 155,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_73\": 73,\n",
      "    \"LABEL_74\": 74,\n",
      "    \"LABEL_75\": 75,\n",
      "    \"LABEL_76\": 76,\n",
      "    \"LABEL_77\": 77,\n",
      "    \"LABEL_78\": 78,\n",
      "    \"LABEL_79\": 79,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_80\": 80,\n",
      "    \"LABEL_81\": 81,\n",
      "    \"LABEL_82\": 82,\n",
      "    \"LABEL_83\": 83,\n",
      "    \"LABEL_84\": 84,\n",
      "    \"LABEL_85\": 85,\n",
      "    \"LABEL_86\": 86,\n",
      "    \"LABEL_87\": 87,\n",
      "    \"LABEL_88\": 88,\n",
      "    \"LABEL_89\": 89,\n",
      "    \"LABEL_9\": 9,\n",
      "    \"LABEL_90\": 90,\n",
      "    \"LABEL_91\": 91,\n",
      "    \"LABEL_92\": 92,\n",
      "    \"LABEL_93\": 93,\n",
      "    \"LABEL_94\": 94,\n",
      "    \"LABEL_95\": 95,\n",
      "    \"LABEL_96\": 96,\n",
      "    \"LABEL_97\": 97,\n",
      "    \"LABEL_98\": 98,\n",
      "    \"LABEL_99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1651] 2022-05-09 08:42:38,960 >> Didn't find file /opt/ml/input/data/last_model/added_tokens.json. We won't load it.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1715] 2022-05-09 08:42:38,960 >> loading file /opt/ml/input/data/last_model/vocab.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1715] 2022-05-09 08:42:38,960 >> loading file /opt/ml/input/data/last_model/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1715] 2022-05-09 08:42:38,960 >> loading file None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1715] 2022-05-09 08:42:38,961 >> loading file /opt/ml/input/data/last_model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1715] 2022-05-09 08:42:38,961 >> loading file /opt/ml/input/data/last_model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1153] 2022-05-09 08:42:38,990 >> loading weights file /opt/ml/input/data/last_model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1339] 2022-05-09 08:42:43,440 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1348] 2022-05-09 08:42:43,440 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /opt/ml/input/data/last_model.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\u001b[0m\n",
      "\u001b[34m05/09/2022 08:42:44 - INFO - __main__ -   Sample 5238 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 7392, 5606, 3808, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 93, 'sentence1': '隔膜泵', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\u001b[0m\n",
      "\u001b[34m05/09/2022 08:42:44 - INFO - __main__ -   Sample 912 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1461, 1429, 3322, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 35, 'sentence1': '呼吸机', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\u001b[0m\n",
      "\u001b[34m05/09/2022 08:42:44 - INFO - __main__ -   Sample 204 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 3344, 1072, 1947, 6163, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 51, 'sentence1': '杯具套装', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:516] 2022-05-09 08:42:48,456 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1156] 2022-05-09 08:42:48,476 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1157] 2022-05-09 08:42:48,476 >>   Num examples = 7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1158] 2022-05-09 08:42:48,476 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1159] 2022-05-09 08:42:48,477 >>   Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1160] 2022-05-09 08:42:48,477 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1161] 2022-05-09 08:42:48,477 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1162] 2022-05-09 08:42:48,477 >>   Total optimization steps = 225\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:48.720 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:48.893 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:48.894 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:48.895 algo-1:32 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:48.896 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:48.896 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.101 algo-1:32 INFO hook.py:591] name:bert.embeddings.word_embeddings.weight count_params:16226304\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.101 algo-1:32 INFO hook.py:591] name:bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.101 algo-1:32 INFO hook.py:591] name:bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.101 algo-1:32 INFO hook.py:591] name:bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.102 algo-1:32 INFO hook.py:591] name:bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.102 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.102 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.102 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.102 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.102 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.102 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.103 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.103 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.103 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.103 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.103 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.104 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.104 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.104 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.104 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.104 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.104 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.104 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.104 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.105 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.105 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.105 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.105 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.105 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.105 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.105 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.105 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.105 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.105 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.106 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.106 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.106 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.106 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.106 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.106 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.106 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.106 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.107 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.107 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.107 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.107 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.107 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.107 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.107 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.108 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.108 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.108 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.108 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.108 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.108 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.108 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.109 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.109 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.109 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.109 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.109 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.109 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.109 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.109 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.110 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.110 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.110 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.110 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.110 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.110 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.110 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.111 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.111 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.111 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.111 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.111 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.111 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.111 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.111 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.112 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.112 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.112 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.112 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.112 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.112 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.112 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.112 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.112 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.113 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.113 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.113 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.114 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.114 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.114 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.114 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.114 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.114 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.115 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.115 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.115 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.115 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.115 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.115 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.115 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.115 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.116 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.116 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.116 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.116 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.116 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.116 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.116 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.116 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.116 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.117 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.117 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.117 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.117 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.117 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.117 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.117 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.117 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.118 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.118 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.118 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.118 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.118 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.118 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.119 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.119 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.119 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.119 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.119 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.119 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.119 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.119 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.119 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.119 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.120 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.120 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.120 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.120 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.120 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.120 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.120 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.120 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.120 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.120 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.121 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.121 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.121 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.121 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.121 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.121 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.121 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.121 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.121 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.122 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.122 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.122 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.122 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.122 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.123 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.123 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.123 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.123 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.123 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.123 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.124 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.124 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.124 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.124 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.124 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.124 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.124 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.124 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.125 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.125 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.125 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.125 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.125 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.125 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.125 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.125 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.126 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.126 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.126 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.126 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.126 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.127 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.127 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.127 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.127 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.127 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.127 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.127 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.127 algo-1:32 INFO hook.py:591] name:bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.127 algo-1:32 INFO hook.py:591] name:bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.128 algo-1:32 INFO hook.py:591] name:bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.128 algo-1:32 INFO hook.py:591] name:classifier.weight count_params:119808\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.128 algo-1:32 INFO hook.py:591] name:classifier.bias count_params:156\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.128 algo-1:32 INFO hook.py:593] Total Trainable Params: 102387612\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.128 algo-1:32 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-05-09 08:42:49.131 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-05-09 08:44:24,943 >> Saving model checkpoint to /opt/ml/model/checkpoint-225\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-05-09 08:44:24,945 >> Configuration saved in /opt/ml/model/checkpoint-225/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-05-09 08:44:25,889 >> Model weights saved in /opt/ml/model/checkpoint-225/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-05-09 08:44:25,890 >> tokenizer config file saved in /opt/ml/model/checkpoint-225/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-05-09 08:44:25,890 >> Special tokens file saved in /opt/ml/model/checkpoint-225/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1352] 2022-05-09 08:44:27,668 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 99.1911, 'train_samples_per_second': 2.268, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-05-09 08:44:27,832 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-05-09 08:44:27,833 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-05-09 08:44:28,650 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-05-09 08:44:28,651 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-05-09 08:44:28,651 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:907] 2022-05-09 08:44:28,681 >> ***** train metrics *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,681 >>   epoch                      =        1.0\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,681 >>   init_mem_cpu_alloc_delta   =      822MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   init_mem_cpu_peaked_delta  =      387MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   init_mem_gpu_alloc_delta   =      391MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   init_mem_gpu_peaked_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   train_mem_cpu_alloc_delta  =      437MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   train_mem_cpu_peaked_delta =      185MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   train_mem_gpu_alloc_delta  =     1231MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   train_mem_gpu_peaked_delta =     3393MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   train_runtime              = 0:01:39.19\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   train_samples              =       7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   train_samples_per_second   =      2.268\u001b[0m\n",
      "\u001b[34m05/09/2022 08:44:28 - INFO - __main__ -   *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:516] 2022-05-09 08:44:28,746 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2115] 2022-05-09 08:44:28,750 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2117] 2022-05-09 08:44:28,750 >>   Num examples = 7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2120] 2022-05-09 08:44:28,750 >>   Batch size = 8\u001b[0m\n",
      "\n",
      "2022-05-09 08:45:38 Uploading - Uploading generated training model\u001b[34m[INFO|trainer_pt_utils.py:907] 2022-05-09 08:45:28,075 >> ***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,075 >>   epoch                     =        1.0\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,075 >>   eval_accuracy             =     0.5469\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_loss                 =     2.1974\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_mem_cpu_alloc_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_mem_cpu_peaked_delta =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_mem_gpu_alloc_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_mem_gpu_peaked_delta =       38MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_runtime              = 0:00:59.26\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_samples              =       7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_samples_per_second   =    120.968\u001b[0m\n",
      "\u001b[34m#0150 tables [00:00, ? tables/s]#015                            #015#0150 tables [00:00, ? tables/s]#015                            #015[INFO|configuration_utils.py:515] 2022-05-09 08:42:38,954 >> loading configuration file /opt/ml/input/data/last_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2022-05-09 08:42:38,956 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\u001b[0m\n",
      "\u001b[34m2022-05-09 08:45:28,862 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\",\n",
      "    \"73\": \"LABEL_73\",\n",
      "    \"74\": \"LABEL_74\",\n",
      "    \"75\": \"LABEL_75\",\n",
      "    \"76\": \"LABEL_76\",\n",
      "    \"77\": \"LABEL_77\",\n",
      "    \"78\": \"LABEL_78\",\n",
      "    \"79\": \"LABEL_79\",\n",
      "    \"80\": \"LABEL_80\",\n",
      "    \"81\": \"LABEL_81\",\n",
      "    \"82\": \"LABEL_82\",\n",
      "    \"83\": \"LABEL_83\",\n",
      "    \"84\": \"LABEL_84\",\n",
      "    \"85\": \"LABEL_85\",\n",
      "    \"86\": \"LABEL_86\",\n",
      "    \"87\": \"LABEL_87\",\n",
      "    \"88\": \"LABEL_88\",\n",
      "    \"89\": \"LABEL_89\",\n",
      "    \"90\": \"LABEL_90\",\n",
      "    \"91\": \"LABEL_91\",\n",
      "    \"92\": \"LABEL_92\",\n",
      "    \"93\": \"LABEL_93\",\n",
      "    \"94\": \"LABEL_94\",\n",
      "    \"95\": \"LABEL_95\",\n",
      "    \"96\": \"LABEL_96\",\n",
      "    \"97\": \"LABEL_97\",\n",
      "    \"98\": \"LABEL_98\",\n",
      "    \"99\": \"LABEL_99\",\n",
      "    \"100\": \"LABEL_100\",\n",
      "    \"101\": \"LABEL_101\",\n",
      "    \"102\": \"LABEL_102\",\n",
      "    \"103\": \"LABEL_103\",\n",
      "    \"104\": \"LABEL_104\",\n",
      "    \"105\": \"LABEL_105\",\n",
      "    \"106\": \"LABEL_106\",\n",
      "    \"107\": \"LABEL_107\",\n",
      "    \"108\": \"LABEL_108\",\n",
      "    \"109\": \"LABEL_109\",\n",
      "    \"110\": \"LABEL_110\",\n",
      "    \"111\": \"LABEL_111\",\n",
      "    \"112\": \"LABEL_112\",\n",
      "    \"113\": \"LABEL_113\",\n",
      "    \"114\": \"LABEL_114\",\n",
      "    \"115\": \"LABEL_115\",\n",
      "    \"116\": \"LABEL_116\",\n",
      "    \"117\": \"LABEL_117\",\n",
      "    \"118\": \"LABEL_118\",\n",
      "    \"119\": \"LABEL_119\",\n",
      "    \"120\": \"LABEL_120\",\n",
      "    \"121\": \"LABEL_121\",\n",
      "    \"122\": \"LABEL_122\",\n",
      "    \"123\": \"LABEL_123\",\n",
      "    \"124\": \"LABEL_124\",\n",
      "    \"125\": \"LABEL_125\",\n",
      "    \"126\": \"LABEL_126\",\n",
      "    \"127\": \"LABEL_127\",\n",
      "    \"128\": \"LABEL_128\",\n",
      "    \"129\": \"LABEL_129\",\n",
      "    \"130\": \"LABEL_130\",\n",
      "    \"131\": \"LABEL_131\",\n",
      "    \"132\": \"LABEL_132\",\n",
      "    \"133\": \"LABEL_133\",\n",
      "    \"134\": \"LABEL_134\",\n",
      "    \"135\": \"LABEL_135\",\n",
      "    \"136\": \"LABEL_136\",\n",
      "    \"137\": \"LABEL_137\",\n",
      "    \"138\": \"LABEL_138\",\n",
      "    \"139\": \"LABEL_139\",\n",
      "    \"140\": \"LABEL_140\",\n",
      "    \"141\": \"LABEL_141\",\n",
      "    \"142\": \"LABEL_142\",\n",
      "    \"143\": \"LABEL_143\",\n",
      "    \"144\": \"LABEL_144\",\n",
      "    \"145\": \"LABEL_145\",\n",
      "    \"146\": \"LABEL_146\",\n",
      "    \"147\": \"LABEL_147\",\n",
      "    \"148\": \"LABEL_148\",\n",
      "    \"149\": \"LABEL_149\",\n",
      "    \"150\": \"LABEL_150\",\n",
      "    \"151\": \"LABEL_151\",\n",
      "    \"152\": \"LABEL_152\",\n",
      "    \"153\": \"LABEL_153\",\n",
      "    \"154\": \"LABEL_154\",\n",
      "    \"155\": \"LABEL_155\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_100\": 100,\n",
      "    \"LABEL_101\": 101,\n",
      "    \"LABEL_102\": 102,\n",
      "    \"LABEL_103\": 103,\n",
      "    \"LABEL_104\": 104,\n",
      "    \"LABEL_105\": 105,\n",
      "    \"LABEL_106\": 106,\n",
      "    \"LABEL_107\": 107,\n",
      "    \"LABEL_108\": 108,\n",
      "    \"LABEL_109\": 109,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_110\": 110,\n",
      "    \"LABEL_111\": 111,\n",
      "    \"LABEL_112\": 112,\n",
      "    \"LABEL_113\": 113,\n",
      "    \"LABEL_114\": 114,\n",
      "    \"LABEL_115\": 115,\n",
      "    \"LABEL_116\": 116,\n",
      "    \"LABEL_117\": 117,\n",
      "    \"LABEL_118\": 118,\n",
      "    \"LABEL_119\": 119,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_120\": 120,\n",
      "    \"LABEL_121\": 121,\n",
      "    \"LABEL_122\": 122,\n",
      "    \"LABEL_123\": 123,\n",
      "    \"LABEL_124\": 124,\n",
      "    \"LABEL_125\": 125,\n",
      "    \"LABEL_126\": 126,\n",
      "    \"LABEL_127\": 127,\n",
      "    \"LABEL_128\": 128,\n",
      "    \"LABEL_129\": 129,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_130\": 130,\n",
      "    \"LABEL_131\": 131,\n",
      "    \"LABEL_132\": 132,\n",
      "    \"LABEL_133\": 133,\n",
      "    \"LABEL_134\": 134,\n",
      "    \"LABEL_135\": 135,\n",
      "    \"LABEL_136\": 136,\n",
      "    \"LABEL_137\": 137,\n",
      "    \"LABEL_138\": 138,\n",
      "    \"LABEL_139\": 139,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_140\": 140,\n",
      "    \"LABEL_141\": 141,\n",
      "    \"LABEL_142\": 142,\n",
      "    \"LABEL_143\": 143,\n",
      "    \"LABEL_144\": 144,\n",
      "    \"LABEL_145\": 145,\n",
      "    \"LABEL_146\": 146,\n",
      "    \"LABEL_147\": 147,\n",
      "    \"LABEL_148\": 148,\n",
      "    \"LABEL_149\": 149,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_150\": 150,\n",
      "    \"LABEL_151\": 151,\n",
      "    \"LABEL_152\": 152,\n",
      "    \"LABEL_153\": 153,\n",
      "    \"LABEL_154\": 154,\n",
      "    \"LABEL_155\": 155,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_73\": 73,\n",
      "    \"LABEL_74\": 74,\n",
      "    \"LABEL_75\": 75,\n",
      "    \"LABEL_76\": 76,\n",
      "    \"LABEL_77\": 77,\n",
      "    \"LABEL_78\": 78,\n",
      "    \"LABEL_79\": 79,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_80\": 80,\n",
      "    \"LABEL_81\": 81,\n",
      "    \"LABEL_82\": 82,\n",
      "    \"LABEL_83\": 83,\n",
      "    \"LABEL_84\": 84,\n",
      "    \"LABEL_85\": 85,\n",
      "    \"LABEL_86\": 86,\n",
      "    \"LABEL_87\": 87,\n",
      "    \"LABEL_88\": 88,\n",
      "    \"LABEL_89\": 89,\n",
      "    \"LABEL_9\": 9,\n",
      "    \"LABEL_90\": 90,\n",
      "    \"LABEL_91\": 91,\n",
      "    \"LABEL_92\": 92,\n",
      "    \"LABEL_93\": 93,\n",
      "    \"LABEL_94\": 94,\n",
      "    \"LABEL_95\": 95,\n",
      "    \"LABEL_96\": 96,\n",
      "    \"LABEL_97\": 97,\n",
      "    \"LABEL_98\": 98,\n",
      "    \"LABEL_99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:515] 2022-05-09 08:42:38,956 >> loading configuration file /opt/ml/input/data/last_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2022-05-09 08:42:38,959 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\",\n",
      "    \"73\": \"LABEL_73\",\n",
      "    \"74\": \"LABEL_74\",\n",
      "    \"75\": \"LABEL_75\",\n",
      "    \"76\": \"LABEL_76\",\n",
      "    \"77\": \"LABEL_77\",\n",
      "    \"78\": \"LABEL_78\",\n",
      "    \"79\": \"LABEL_79\",\n",
      "    \"80\": \"LABEL_80\",\n",
      "    \"81\": \"LABEL_81\",\n",
      "    \"82\": \"LABEL_82\",\n",
      "    \"83\": \"LABEL_83\",\n",
      "    \"84\": \"LABEL_84\",\n",
      "    \"85\": \"LABEL_85\",\n",
      "    \"86\": \"LABEL_86\",\n",
      "    \"87\": \"LABEL_87\",\n",
      "    \"88\": \"LABEL_88\",\n",
      "    \"89\": \"LABEL_89\",\n",
      "    \"90\": \"LABEL_90\",\n",
      "    \"91\": \"LABEL_91\",\n",
      "    \"92\": \"LABEL_92\",\n",
      "    \"93\": \"LABEL_93\",\n",
      "    \"94\": \"LABEL_94\",\n",
      "    \"95\": \"LABEL_95\",\n",
      "    \"96\": \"LABEL_96\",\n",
      "    \"97\": \"LABEL_97\",\n",
      "    \"98\": \"LABEL_98\",\n",
      "    \"99\": \"LABEL_99\",\n",
      "    \"100\": \"LABEL_100\",\n",
      "    \"101\": \"LABEL_101\",\n",
      "    \"102\": \"LABEL_102\",\n",
      "    \"103\": \"LABEL_103\",\n",
      "    \"104\": \"LABEL_104\",\n",
      "    \"105\": \"LABEL_105\",\n",
      "    \"106\": \"LABEL_106\",\n",
      "    \"107\": \"LABEL_107\",\n",
      "    \"108\": \"LABEL_108\",\n",
      "    \"109\": \"LABEL_109\",\n",
      "    \"110\": \"LABEL_110\",\n",
      "    \"111\": \"LABEL_111\",\n",
      "    \"112\": \"LABEL_112\",\n",
      "    \"113\": \"LABEL_113\",\n",
      "    \"114\": \"LABEL_114\",\n",
      "    \"115\": \"LABEL_115\",\n",
      "    \"116\": \"LABEL_116\",\n",
      "    \"117\": \"LABEL_117\",\n",
      "    \"118\": \"LABEL_118\",\n",
      "    \"119\": \"LABEL_119\",\n",
      "    \"120\": \"LABEL_120\",\n",
      "    \"121\": \"LABEL_121\",\n",
      "    \"122\": \"LABEL_122\",\n",
      "    \"123\": \"LABEL_123\",\n",
      "    \"124\": \"LABEL_124\",\n",
      "    \"125\": \"LABEL_125\",\n",
      "    \"126\": \"LABEL_126\",\n",
      "    \"127\": \"LABEL_127\",\n",
      "    \"128\": \"LABEL_128\",\n",
      "    \"129\": \"LABEL_129\",\n",
      "    \"130\": \"LABEL_130\",\n",
      "    \"131\": \"LABEL_131\",\n",
      "    \"132\": \"LABEL_132\",\n",
      "    \"133\": \"LABEL_133\",\n",
      "    \"134\": \"LABEL_134\",\n",
      "    \"135\": \"LABEL_135\",\n",
      "    \"136\": \"LABEL_136\",\n",
      "    \"137\": \"LABEL_137\",\n",
      "    \"138\": \"LABEL_138\",\n",
      "    \"139\": \"LABEL_139\",\n",
      "    \"140\": \"LABEL_140\",\n",
      "    \"141\": \"LABEL_141\",\n",
      "    \"142\": \"LABEL_142\",\n",
      "    \"143\": \"LABEL_143\",\n",
      "    \"144\": \"LABEL_144\",\n",
      "    \"145\": \"LABEL_145\",\n",
      "    \"146\": \"LABEL_146\",\n",
      "    \"147\": \"LABEL_147\",\n",
      "    \"148\": \"LABEL_148\",\n",
      "    \"149\": \"LABEL_149\",\n",
      "    \"150\": \"LABEL_150\",\n",
      "    \"151\": \"LABEL_151\",\n",
      "    \"152\": \"LABEL_152\",\n",
      "    \"153\": \"LABEL_153\",\n",
      "    \"154\": \"LABEL_154\",\n",
      "    \"155\": \"LABEL_155\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_100\": 100,\n",
      "    \"LABEL_101\": 101,\n",
      "    \"LABEL_102\": 102,\n",
      "    \"LABEL_103\": 103,\n",
      "    \"LABEL_104\": 104,\n",
      "    \"LABEL_105\": 105,\n",
      "    \"LABEL_106\": 106,\n",
      "    \"LABEL_107\": 107,\n",
      "    \"LABEL_108\": 108,\n",
      "    \"LABEL_109\": 109,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_110\": 110,\n",
      "    \"LABEL_111\": 111,\n",
      "    \"LABEL_112\": 112,\n",
      "    \"LABEL_113\": 113,\n",
      "    \"LABEL_114\": 114,\n",
      "    \"LABEL_115\": 115,\n",
      "    \"LABEL_116\": 116,\n",
      "    \"LABEL_117\": 117,\n",
      "    \"LABEL_118\": 118,\n",
      "    \"LABEL_119\": 119,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_120\": 120,\n",
      "    \"LABEL_121\": 121,\n",
      "    \"LABEL_122\": 122,\n",
      "    \"LABEL_123\": 123,\n",
      "    \"LABEL_124\": 124,\n",
      "    \"LABEL_125\": 125,\n",
      "    \"LABEL_126\": 126,\n",
      "    \"LABEL_127\": 127,\n",
      "    \"LABEL_128\": 128,\n",
      "    \"LABEL_129\": 129,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_130\": 130,\n",
      "    \"LABEL_131\": 131,\n",
      "    \"LABEL_132\": 132,\n",
      "    \"LABEL_133\": 133,\n",
      "    \"LABEL_134\": 134,\n",
      "    \"LABEL_135\": 135,\n",
      "    \"LABEL_136\": 136,\n",
      "    \"LABEL_137\": 137,\n",
      "    \"LABEL_138\": 138,\n",
      "    \"LABEL_139\": 139,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_140\": 140,\n",
      "    \"LABEL_141\": 141,\n",
      "    \"LABEL_142\": 142,\n",
      "    \"LABEL_143\": 143,\n",
      "    \"LABEL_144\": 144,\n",
      "    \"LABEL_145\": 145,\n",
      "    \"LABEL_146\": 146,\n",
      "    \"LABEL_147\": 147,\n",
      "    \"LABEL_148\": 148,\n",
      "    \"LABEL_149\": 149,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_150\": 150,\n",
      "    \"LABEL_151\": 151,\n",
      "    \"LABEL_152\": 152,\n",
      "    \"LABEL_153\": 153,\n",
      "    \"LABEL_154\": 154,\n",
      "    \"LABEL_155\": 155,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_73\": 73,\n",
      "    \"LABEL_74\": 74,\n",
      "    \"LABEL_75\": 75,\n",
      "    \"LABEL_76\": 76,\n",
      "    \"LABEL_77\": 77,\n",
      "    \"LABEL_78\": 78,\n",
      "    \"LABEL_79\": 79,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_80\": 80,\n",
      "    \"LABEL_81\": 81,\n",
      "    \"LABEL_82\": 82,\n",
      "    \"LABEL_83\": 83,\n",
      "    \"LABEL_84\": 84,\n",
      "    \"LABEL_85\": 85,\n",
      "    \"LABEL_86\": 86,\n",
      "    \"LABEL_87\": 87,\n",
      "    \"LABEL_88\": 88,\n",
      "    \"LABEL_89\": 89,\n",
      "    \"LABEL_9\": 9,\n",
      "    \"LABEL_90\": 90,\n",
      "    \"LABEL_91\": 91,\n",
      "    \"LABEL_92\": 92,\n",
      "    \"LABEL_93\": 93,\n",
      "    \"LABEL_94\": 94,\n",
      "    \"LABEL_95\": 95,\n",
      "    \"LABEL_96\": 96,\n",
      "    \"LABEL_97\": 97,\n",
      "    \"LABEL_98\": 98,\n",
      "    \"LABEL_99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1651] 2022-05-09 08:42:38,960 >> Didn't find file /opt/ml/input/data/last_model/added_tokens.json. We won't load it.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1715] 2022-05-09 08:42:38,960 >> loading file /opt/ml/input/data/last_model/vocab.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1715] 2022-05-09 08:42:38,960 >> loading file /opt/ml/input/data/last_model/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1715] 2022-05-09 08:42:38,960 >> loading file None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1715] 2022-05-09 08:42:38,961 >> loading file /opt/ml/input/data/last_model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1715] 2022-05-09 08:42:38,961 >> loading file /opt/ml/input/data/last_model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1153] 2022-05-09 08:42:38,990 >> loading weights file /opt/ml/input/data/last_model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1339] 2022-05-09 08:42:43,440 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1348] 2022-05-09 08:42:43,440 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /opt/ml/input/data/last_model.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/8 [00:00<?, ?ba/s]#015 12%|█▎        | 1/8 [00:00<00:01,  6.86ba/s]#015 50%|█████     | 4/8 [00:00<00:00,  8.60ba/s]#015 88%|████████▊ | 7/8 [00:00<00:00, 10.43ba/s]#015100%|██████████| 8/8 [00:00<00:00, 18.07ba/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/8 [00:00<?, ?ba/s]#015 25%|██▌       | 2/8 [00:00<00:00, 18.74ba/s]#015 62%|██████▎   | 5/8 [00:00<00:00, 17.26ba/s]#015100%|██████████| 8/8 [00:00<00:00, 19.69ba/s]#015100%|██████████| 8/8 [00:00<00:00, 19.27ba/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:516] 2022-05-09 08:42:48,456 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1156] 2022-05-09 08:42:48,476 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1157] 2022-05-09 08:42:48,476 >>   Num examples = 7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1158] 2022-05-09 08:42:48,476 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1159] 2022-05-09 08:42:48,477 >>   Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1160] 2022-05-09 08:42:48,477 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1161] 2022-05-09 08:42:48,477 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1162] 2022-05-09 08:42:48,477 >>   Total optimization steps = 225\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/225 [00:00<?, ?it/s]#015  0%|          | 1/225 [00:02<08:47,  2.36s/it]#015  1%|          | 2/225 [00:02<06:37,  1.78s/it]#015  1%|▏         | 3/225 [00:03<05:08,  1.39s/it]#015  2%|▏         | 4/225 [00:03<04:03,  1.10s/it]#015  2%|▏         | 5/225 [00:04<03:16,  1.12it/s]#015  3%|▎         | 6/225 [00:04<02:44,  1.33it/s]#015  3%|▎         | 7/225 [00:04<02:23,  1.52it/s]#015  4%|▎         | 8/225 [00:05<02:12,  1.64it/s]#015  4%|▍         | 9/225 [00:05<02:02,  1.76it/s]#015  4%|▍         | 10/225 [00:06<01:53,  1.89it/s]#015  5%|▍         | 11/225 [00:06<01:45,  2.03it/s]#015  5%|▌         | 12/225 [00:07<01:39,  2.14it/s]#015  6%|▌         | 13/225 [00:07<01:35,  2.23it/s]#015  6%|▌         | 14/225 [00:07<01:31,  2.31it/s]#015  7%|▋         | 15/225 [00:08<01:29,  2.34it/s]#015  7%|▋         | 16/225 [00:08<01:27,  2.38it/s]#015  8%|▊         | 17/225 [00:09<01:26,  2.41it/s]#015  8%|▊         | 18/225 [00:09<01:26,  2.39it/s]#015  8%|▊         | 19/225 [00:10<01:26,  2.38it/s]#015  9%|▉         | 20/225 [00:10<01:26,  2.37it/s]#015  9%|▉         | 21/225 [00:10<01:26,  2.36it/s]#015 10%|▉         | 22/225 [00:11<01:28,  2.30it/s]#015 10%|█         | 23/225 [00:11<01:27,  2.31it/s]#015 11%|█         | 24/225 [00:12<01:26,  2.34it/s]#015 11%|█         | 25/225 [00:12<01:23,  2.39it/s]#015 12%|█▏        | 26/225 [00:13<01:24,  2.37it/s]#015 12%|█▏        | 27/225 [00:13<01:23,  2.37it/s]#015 12%|█▏        | 28/225 [00:13<01:23,  2.35it/s]#015 13%|█▎        | 29/225 [00:14<01:23,  2.36it/s]#015 13%|█▎        | 30/225 [00:14<01:23,  2.34it/s]#015 14%|█▍        | 31/225 [00:15<01:21,  2.37it/s]#015 14%|█▍        | 32/225 [00:15<01:22,  2.33it/s]#015 15%|█▍        | 33/225 [00:16<01:22,  2.31it/s]#015 15%|█▌        | 34/225 [00:16<01:20,  2.37it/s]#015 16%|█▌        | 35/225 [00:16<01:20,  2.36it/s]#015 16%|█▌        | 36/225 [00:17<01:19,  2.39it/s]#015 16%|█▋        | 37/225 [00:17<01:21,  2.31it/s]#015 17%|█▋        | 38/225 [00:18<01:18,  2.37it/s]#015 17%|█▋        | 39/225 [00:18<01:18,  2.37it/s]#015 18%|█▊        | 40/225 [00:18<01:16,  2.41it/s]#015 18%|█▊        | 41/225 [00:19<01:18,  2.34it/s]#015 19%|█▊        | 42/225 [00:19<01:17,  2.36it/s]#015 19%|█▉        | 43/225 [00:20<01:16,  2.38it/s]#015 20%|█▉        | 44/225 [00:20<01:15,  2.40it/s]#015 20%|██        | 45/225 [00:21<01:14,  2.41it/s]#015 20%|██        | 46/225 [00:21<01:13,  2.43it/s]#015 21%|██        | 47/225 [00:21<01:13,  2.42it/s]#015 21%|██▏       | 48/225 [00:22<01:13,  2.41it/s]#015 22%|██▏       | 49/225 [00:22<01:12,  2.43it/s]#015 22%|██▏       | 50/225 [00:23<01:12,  2.41it/s]#015 23%|██▎       | 51/225 [00:23<01:14,  2.35it/s]#015 23%|██▎       | 52/225 [00:23<01:12,  2.40it/s]#015 24%|██▎       | 53/225 [00:24<01:11,  2.42it/s]#015 24%|██▍       | 54/225 [00:24<01:10,  2.44it/s]#015 24%|██▍       | 55/225 [00:25<01:09,  2.43it/s]#015 25%|██▍       | 56/225 [00:25<01:08,  2.47it/s]#015 25%|██▌       | 57/225 [00:26<01:08,  2.45it/s]#015 26%|██▌       | 58/225 [00:26<01:07,  2.46it/s]#015 26%|██▌       | 59/225 [00:26<01:07,  2.45it/s]#015 27%|██▋       | 60/225 [00:27<01:06,  2.47it/s]#015 27%|██▋       | 61/225 [00:27<01:06,  2.48it/s]#015 28%|██▊       | 62/225 [00:28<01:06,  2.46it/s]#015 28%|██▊       | 63/225 [00:28<01:06,  2.43it/s]#015 28%|██▊       | 64/225 [00:28<01:06,  2.43it/s]#015 29%|██▉       | 65/225 [00:29<01:05,  2.44it/s]#015 29%|██▉       | 66/225 [00:29<01:09,  2.30it/s]#015 30%|██▉       | 67/225 [00:30<01:07,  2.33it/s]#015 30%|███       | 68/225 [00:30<01:05,  2.40it/s]#015 31%|███       | 69/225 [00:30<01:05,  2.40it/s]#015 31%|███       | 70/225 [00:31<01:04,  2.41it/s]#015 32%|███▏      | 71/225 [00:31<01:04,  2.38it/s]#015 32%|███▏      | 72/225 [00:32<01:03,  2.40it/s]#015 32%|███▏      | 73/225 [00:32<01:02,  2.44it/s]#015 33%|███▎      | 74/225 [00:33<01:03,  2.37it/s]#015 33%|███▎      | 75/225 [00:33<01:02,  2.39it/s]#015 34%|███▍      | 76/225 [00:33<01:02,  2.40it/s]#015 34%|███▍      | 77/225 [00:34<01:01,  2.42it/s]#015 35%|███▍      | 78/225 [00:34<01:01,  2.39it/s]#015 35%|███▌      | 79/225 [00:35<01:00,  2.42it/s]#015 36%|███▌      | 80/225 [00:35<01:00,  2.41it/s]#015 36%|███▌      | 81/225 [00:35<00:59,  2.41it/s]#015 36%|███▋      | 82/225 [00:36<00:59,  2.42it/s]#015 37%|███▋      | 83/225 [00:36<01:01,  2.30it/s]#015 37%|███▋      | 84/225 [00:37<01:00,  2.33it/s]#015 38%|███▊      | 85/225 [00:37<00:58,  2.37it/s]#015 38%|███▊      | 86/225 [00:38<00:58,  2.36it/s]#015 39%|███▊      | 87/225 [00:38<00:57,  2.42it/s]#015 39%|███▉      | 88/225 [00:38<00:56,  2.42it/s]#015 40%|███▉      | 89/225 [00:39<00:56,  2.43it/s]#015 40%|████      | 90/225 [00:39<00:55,  2.41it/s]#015 40%|████      | 91/225 [00:40<00:56,  2.38it/s]#015 41%|████      | 92/225 [00:40<00:55,  2.39it/s]#015 41%|████▏     | 93/225 [00:41<00:54,  2.41it/s]#015 42%|████▏     | 94/225 [00:41<00:53,  2.44it/s]#015 42%|████▏     | 95/225 [00:41<00:53,  2.42it/s]#015 43%|████▎     | 96/225 [00:42<00:53,  2.40it/s]#015 43%|████▎     | 97/225 [00:42<00:52,  2.42it/s]#015 44%|████▎     | 98/225 [00:43<00:52,  2.44it/s]#015 44%|████▍     | 99/225 [00:43<00:52,  2.41it/s]#015 44%|████▍     | 100/225 [00:43<00:51,  2.43it/s]#015 45%|████▍     | 101/225 [00:44<00:51,  2.40it/s]#015 45%|████▌     | 102/225 [00:44<00:50,  2.42it/s]#015 46%|████▌     | 103/225 [00:45<00:51,  2.38it/s]#015 46%|████▌     | 104/225 [00:45<00:50,  2.40it/s]#015 47%|████▋     | 105/225 [00:45<00:49,  2.40it/s]#015 47%|████▋     | 106/225 [00:46<00:49,  2.40it/s]#015 48%|████▊     | 107/225 [00:46<00:49,  2.40it/s]#015 48%|████▊     | 108/225 [00:47<00:48,  2.42it/s]#015 48%|████▊     | 109/225 [00:47<00:48,  2.41it/s]#015 49%|████▉     | 110/225 [00:48<00:49,  2.34it/s]#015 49%|████▉     | 111/225 [00:48<00:48,  2.37it/s]#015 50%|████▉     | 112/225 [00:48<00:47,  2.40it/s]#015 50%|█████     | 113/225 [00:49<00:46,  2.39it/s]#015 51%|█████     | 114/225 [00:49<00:46,  2.39it/s]#015 51%|█████     | 115/225 [00:50<00:45,  2.42it/s]#015 52%|█████▏    | 116/225 [00:50<00:44,  2.43it/s]#015 52%|█████▏    | 117/225 [00:50<00:44,  2.45it/s]#015 52%|█████▏    | 118/225 [00:51<00:44,  2.42it/s]#015 53%|█████▎    | 119/225 [00:51<00:43,  2.42it/s]#015 53%|█████▎    | 120/225 [00:52<00:45,  2.33it/s]#015 54%|█████▍    | 121/225 [00:52<00:43,  2.37it/s]#015 54%|█████▍    | 122/225 [00:53<00:44,  2.33it/s]#015 55%|█████▍    | 123/225 [00:53<00:43,  2.34it/s]#015 55%|█████▌    | 124/225 [00:53<00:42,  2.39it/s]#015 56%|█████▌    | 125/225 [00:54<00:41,  2.39it/s]#015 56%|█████▌    | 126/225 [00:54<00:40,  2.42it/s]#015 56%|█████▋    | 127/225 [00:55<00:40,  2.42it/s]#015 57%|█████▋    | 128/225 [00:55<00:39,  2.44it/s]#015 57%|█████▋    | 129/225 [00:56<00:40,  2.36it/s]#015 58%|█████▊    | 130/225 [00:56<00:40,  2.37it/s]#015 58%|█████▊    | 131/225 [00:56<00:39,  2.38it/s]#015 59%|█████▊    | 132/225 [00:57<00:39,  2.37it/s]#015 59%|█████▉    | 133/225 [00:57<00:40,  2.28it/s]#015 60%|█████▉    | 134/225 [00:58<00:38,  2.34it/s]#015 60%|██████    | 135/225 [00:58<00:39,  2.30it/s]#015 60%|██████    | 136/225 [00:59<00:38,  2.31it/s]#015 61%|██████    | 137/225 [00:59<00:38,  2.31it/s]#015 61%|██████▏   | 138/225 [00:59<00:37,  2.35it/s]#015 62%|██████▏   | 139/225 [01:00<00:35,  2.39it/s]#015 62%|██████▏   | 140/225 [01:00<00:35,  2.39it/s]#015 63%|██████▎   | 141/225 [01:01<00:34,  2.43it/s]#015 63%|██████▎   | 142/225 [01:01<00:34,  2.43it/s]#015 64%|██████▎   | 143/225 [01:01<00:33,  2.46it/s]#015 64%|██████▍   | 144/225 [01:02<00:33,  2.40it/s]#015 64%|██████▍   | 145/225 [01:02<00:34,  2.33it/s]#015 65%|██████▍   | 146/225 [01:03<00:33,  2.39it/s]#015 65%|██████▌   | 147/225 [01:03<00:32,  2.41it/s]#015 66%|██████▌   | 148/225 [01:04<00:32,  2.34it/s]#015 66%|██████▌   | 149/225 [01:04<00:32,  2.37it/s]#015 67%|██████▋   | 150/225 [01:04<00:31,  2.39it/s]#015 67%|██████▋   | 151/225 [01:05<00:30,  2.40it/s]#015 68%|██████▊   | 152/225 [01:05<00:30,  2.38it/s]#015 68%|██████▊   | 153/225 [01:06<00:29,  2.40it/s]#015 68%|██████▊   | 154/225 [01:06<00:29,  2.41it/s]#015 69%|██████▉   | 155/225 [01:06<00:28,  2.44it/s]#015 69%|██████▉   | 156/225 [01:07<00:28,  2.44it/s]#015 70%|██████▉   | 157/225 [01:07<00:28,  2.40it/s]#015 70%|███████   | 158/225 [01:08<00:27,  2.41it/s]#015 71%|███████   | 159/225 [01:08<00:27,  2.40it/s]#015 71%|███████   | 160/225 [01:09<00:27,  2.40it/s]#015 72%|███████▏  | 161/225 [01:09<00:26,  2.43it/s]#015 72%|███████▏  | 162/225 [01:09<00:25,  2.43it/s]#015 72%|███████▏  | 163/225 [01:10<00:25,  2.43it/s]#015 73%|███████▎  | 164/225 [01:10<00:25,  2.40it/s]#015 73%|███████▎  | 165/225 [01:11<00:24,  2.40it/s]#015 74%|███████▍  | 166/225 [01:11<00:24,  2.40it/s]#015 74%|███████▍  | 167/225 [01:11<00:24,  2.36it/s]#015 75%|███████▍  | 168/225 [01:12<00:23,  2.39it/s]#015 75%|███████▌  | 169/225 [01:12<00:23,  2.43it/s]#015 76%|███████▌  | 170/225 [01:13<00:24,  2.28it/s]#015 76%|███████▌  | 171/225 [01:13<00:23,  2.31it/s]#015 76%|███████▋  | 172/225 [01:14<00:22,  2.36it/s]#015 77%|███████▋  | 173/225 [01:14<00:23,  2.24it/s]#015 77%|███████▋  | 174/225 [01:14<00:22,  2.28it/s]#015 78%|███████▊  | 175/225 [01:15<00:21,  2.31it/s]#015 78%|███████▊  | 176/225 [01:15<00:21,  2.28it/s]#015 79%|███████▊  | 177/225 [01:16<00:21,  2.28it/s]#015 79%|███████▉  | 178/225 [01:16<00:20,  2.29it/s]#015 80%|███████▉  | 179/225 [01:17<00:20,  2.27it/s]#015 80%|████████  | 180/225 [01:17<00:20,  2.23it/s]#015 80%|████████  | 181/225 [01:18<00:19,  2.28it/s]#015 81%|████████  | 182/225 [01:18<00:18,  2.32it/s]#015 81%|████████▏ | 183/225 [01:18<00:17,  2.35it/s]#015 82%|████████▏ | 184/225 [01:19<00:17,  2.38it/s]#015 82%|████████▏ | 185/225 [01:19<00:16,  2.40it/s]#015 83%|████████▎ | 186/225 [01:20<00:16,  2.40it/s]#015 83%|████████▎ | 187/225 [01:20<00:16,  2.35it/s]#015 84%|████████▎ | 188/225 [01:21<00:15,  2.34it/s]#015 84%|████████▍ | 189/225 [01:21<00:15,  2.36it/s]#015 84%|████████▍ | 190/225 [01:21<00:14,  2.35it/s]#015 85%|████████▍ | 191/225 [01:22<00:14,  2.36it/s]#015 85%|████████▌ | 192/225 [01:22<00:13,  2.39it/s]#015 86%|████████▌ | 193/225 [01:23<00:13,  2.38it/s]#015 86%|████████▌ | 194/225 [01:23<00:13,  2.34it/s]#015 87%|████████▋ | 195/225 [01:24<00:14,  2.14it/s]#015 87%|████████▋ | 196/225 [01:24<00:13,  2.22it/s]#015 88%|████████▊ | 197/225 [01:24<00:12,  2.27it/s]#015 88%|████████▊ | 198/225 [01:25<00:11,  2.29it/s]#015 88%|████████▊ | 199/225 [01:25<00:11,  2.30it/s]#015 89%|████████▉ | 200/225 [01:26<00:10,  2.35it/s]#015 89%|████████▉ | 201/225 [01:26<00:10,  2.39it/s]#015 90%|████████▉ | 202/225 [01:27<00:09,  2.41it/s]#015 90%|█████████ | 203/225 [01:27<00:09,  2.35it/s]#015 91%|█████████ | 204/225 [01:27<00:08,  2.36it/s]#015 91%|█████████ | 205/225 [01:28<00:08,  2.39it/s]#015 92%|█████████▏| 206/225 [01:28<00:07,  2.39it/s]#015 92%|█████████▏| 207/225 [01:29<00:07,  2.42it/s]#015 92%|█████████▏| 208/225 [01:29<00:07,  2.41it/s]#015 93%|█████████▎| 209/225 [01:29<00:06,  2.39it/s]#015 93%|█████████▎| 210/225 [01:30<00:06,  2.38it/s]#015 94%|█████████▍| 211/225 [01:30<00:05,  2.40it/s]#015 94%|█████████▍| 212/225 [01:31<00:05,  2.42it/s]#015 95%|█████████▍| 213/225 [01:31<00:04,  2.43it/s]#015 95%|█████████▌| 214/225 [01:32<00:04,  2.41it/s]#015 96%|█████████▌| 215/225 [01:32<00:04,  2.41it/s]#015 96%|█████████▌| 216/225 [01:32<00:03,  2.36it/s]#015 96%|█████████▋| 217/225 [01:33<00:03,  2.40it/s]#015 97%|█████████▋| 218/225 [01:33<00:02,  2.41it/s]#015 97%|█████████▋| 219/225 [01:34<00:02,  2.45it/s]#015 98%|█████████▊| 220/225 [01:34<00:02,  2.42it/s]#015 98%|█████████▊| 221/225 [01:34<00:01,  2.45it/s]#015 99%|█████████▊| 222/225 [01:35<00:01,  2.45it/s]#015 99%|█████████▉| 223/225 [01:35<00:00,  2.46it/s]#015100%|█████████▉| 224/225 [01:36<00:00,  2.46it/s]#015100%|██████████| 225/225 [01:36<00:00,  2.58it/s][INFO|trainer.py:1885] 2022-05-09 08:44:24,943 >> Saving model checkpoint to /opt/ml/model/checkpoint-225\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-05-09 08:44:24,945 >> Configuration saved in /opt/ml/model/checkpoint-225/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-05-09 08:44:25,889 >> Model weights saved in /opt/ml/model/checkpoint-225/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-05-09 08:44:25,890 >> tokenizer config file saved in /opt/ml/model/checkpoint-225/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-05-09 08:44:25,890 >> Special tokens file saved in /opt/ml/model/checkpoint-225/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1352] 2022-05-09 08:44:27,668 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m#015                                                 #015#015100%|██████████| 225/225 [01:39<00:00,  2.58it/s]#015100%|██████████| 225/225 [01:39<00:00,  2.27it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-05-09 08:44:27,832 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-05-09 08:44:27,833 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-05-09 08:44:28,650 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-05-09 08:44:28,651 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-05-09 08:44:28,651 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:907] 2022-05-09 08:44:28,681 >> ***** train metrics *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,681 >>   epoch                      =        1.0\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,681 >>   init_mem_cpu_alloc_delta   =      822MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   init_mem_cpu_peaked_delta  =      387MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   init_mem_gpu_alloc_delta   =      391MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   init_mem_gpu_peaked_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   train_mem_cpu_alloc_delta  =      437MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   train_mem_cpu_peaked_delta =      185MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   train_mem_gpu_alloc_delta  =     1231MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   train_mem_gpu_peaked_delta =     3393MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   train_runtime              = 0:01:39.19\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   train_samples              =       7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:44:28,682 >>   train_samples_per_second   =      2.268\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:516] 2022-05-09 08:44:28,746 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2115] 2022-05-09 08:44:28,750 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2117] 2022-05-09 08:44:28,750 >>   Num examples = 7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2120] 2022-05-09 08:44:28,750 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/897 [00:00<?, ?it/s]#015  0%|          | 3/897 [00:00<00:35, 25.06it/s]#015  1%|          | 5/897 [00:00<00:41, 21.43it/s]#015  1%|          | 7/897 [00:00<00:47, 18.86it/s]#015  1%|          | 9/897 [00:00<00:49, 17.96it/s]#015  1%|          | 11/897 [00:00<00:51, 17.18it/s]#015  1%|▏         | 13/897 [00:00<00:54, 16.28it/s]#015  2%|▏         | 15/897 [00:00<00:54, 16.22it/s]#015  2%|▏         | 17/897 [00:01<00:54, 16.14it/s]#015  2%|▏         | 19/897 [00:01<00:56, 15.58it/s]#015  2%|▏         | 21/897 [00:01<00:55, 15.72it/s]#015  3%|▎         | 23/897 [00:01<00:55, 15.69it/s]#015  3%|▎         | 25/897 [00:01<00:54, 15.98it/s]#015  3%|▎         | 27/897 [00:01<00:56, 15.47it/s]#015  3%|▎         | 29/897 [00:01<00:55, 15.53it/s]#015  3%|▎         | 31/897 [00:01<00:58, 14.86it/s]#015  4%|▎         | 33/897 [00:02<00:57, 15.05it/s]#015  4%|▍         | 35/897 [00:02<00:55, 15.46it/s]#015  4%|▍         | 37/897 [00:02<00:54, 15.91it/s]#015  4%|▍         | 39/897 [00:02<00:53, 16.06it/s]#015  5%|▍         | 41/897 [00:02<00:53, 16.13it/s]#015  5%|▍         | 43/897 [00:02<00:53, 15.85it/s]#015  5%|▌         | 45/897 [00:02<00:53, 16.03it/s]#015  5%|▌         | 47/897 [00:02<00:53, 15.95it/s]#015  5%|▌         | 49/897 [00:03<00:55, 15.17it/s]#015  6%|▌         | 51/897 [00:03<01:00, 14.04it/s]#015  6%|▌         | 53/897 [00:03<01:01, 13.69it/s]#015  6%|▌         | 55/897 [00:03<00:59, 14.18it/s]#015  6%|▋         | 57/897 [00:03<00:57, 14.69it/s]#015  7%|▋         | 59/897 [00:03<00:55, 15.14it/s]#015  7%|▋         | 61/897 [00:03<00:53, 15.61it/s]#015  7%|▋         | 63/897 [00:04<00:53, 15.57it/s]#015  7%|▋         | 65/897 [00:04<00:52, 15.93it/s]#015  7%|▋         | 67/897 [00:04<00:52, 15.95it/s]#015  8%|▊         | 69/897 [00:04<00:51, 16.13it/s]#015  8%|▊         | 71/897 [00:04<00:51, 15.97it/s]#015  8%|▊         | 73/897 [00:04<00:50, 16.37it/s]#015  8%|▊         | 75/897 [00:04<00:51, 15.90it/s]#015  9%|▊         | 77/897 [00:04<00:51, 16.02it/s]#015  9%|▉         | 79/897 [00:05<00:52, 15.45it/s]#015  9%|▉         | 81/897 [00:05<00:56, 14.52it/s]#015  9%|▉         | 83/897 [00:05<00:58, 13.88it/s]#015  9%|▉         | 85/897 [00:05<01:00, 13.33it/s]#015 10%|▉         | 87/897 [00:05<01:00, 13.40it/s]#015 10%|▉         | 89/897 [00:05<00:57, 14.06it/s]#015 10%|█         | 91/897 [00:05<00:57, 14.03it/s]#015 10%|█         | 93/897 [00:06<00:57, 14.10it/s]#015 11%|█         | 95/897 [00:06<00:58, 13.66it/s]#015 11%|█         | 97/897 [00:06<00:59, 13.36it/s]#015 11%|█         | 99/897 [00:06<01:01, 13.02it/s]#015 11%|█▏        | 101/897 [00:06<00:57, 13.90it/s]#015 11%|█▏        | 103/897 [00:06<00:54, 14.45it/s]#015 12%|█▏        | 105/897 [00:06<00:52, 15.12it/s]#015 12%|█▏        | 107/897 [00:07<00:55, 14.32it/s]#015 12%|█▏        | 109/897 [00:07<00:53, 14.82it/s]#015 12%|█▏        | 111/897 [00:07<00:52, 14.98it/s]#015 13%|█▎        | 113/897 [00:07<00:50, 15.46it/s]#015 13%|█▎        | 115/897 [00:07<00:51, 15.13it/s]#015 13%|█▎        | 117/897 [00:07<00:50, 15.30it/s]#015 13%|█▎        | 119/897 [00:07<00:52, 14.94it/s]#015 13%|█▎        | 121/897 [00:07<00:51, 15.09it/s]#015 14%|█▎        | 123/897 [00:08<00:49, 15.62it/s]#015 14%|█▍        | 125/897 [00:08<00:48, 16.06it/s]#015 14%|█▍        | 127/897 [00:08<00:48, 15.78it/s]#015 14%|█▍        | 129/897 [00:08<00:48, 15.84it/s]#015 15%|█▍        | 131/897 [00:08<00:48, 15.82it/s]#015 15%|█▍        | 133/897 [00:08<00:48, 15.67it/s]#015 15%|█▌        | 135/897 [00:08<00:48, 15.72it/s]#015 15%|█▌        | 137/897 [00:08<00:47, 16.06it/s]#015 15%|█▌        | 139/897 [00:09<00:46, 16.27it/s]#015 16%|█▌        | 141/897 [00:09<00:50, 14.97it/s]#015 16%|█▌        | 143/897 [00:09<00:54, 13.91it/s]#015 16%|█▌        | 145/897 [00:09<00:53, 14.09it/s]#015 16%|█▋        | 147/897 [00:09<00:50, 14.72it/s]#015 17%|█▋        | 149/897 [00:09<00:48, 15.30it/s]#015 17%|█▋        | 151/897 [00:09<00:50, 14.83it/s]#015 17%|█▋        | 153/897 [00:10<00:50, 14.85it/s]#015 17%|█▋        | 155/897 [00:10<00:50, 14.73it/s]#015 18%|█▊        | 157/897 [00:10<00:48, 15.11it/s]#015 18%|█▊        | 159/897 [00:10<00:49, 15.06it/s]#015 18%|█▊        | 161/897 [00:10<00:48, 15.23it/s]#015 18%|█▊        | 163/897 [00:10<00:47, 15.61it/s]#015 18%|█▊        | 165/897 [00:10<00:46, 15.71it/s]#015 19%|█▊        | 167/897 [00:10<00:46, 15.77it/s]#015 19%|█▉        | 169/897 [00:11<00:47, 15.48it/s]#015 19%|█▉        | 171/897 [00:11<00:47, 15.14it/s]#015 19%|█▉        | 173/897 [00:11<00:47, 15.29it/s]#015 20%|█▉        | 175/897 [00:11<00:46, 15.41it/s]#015 20%|█▉        | 177/897 [00:11<00:49, 14.65it/s]#015 20%|█▉        | 179/897 [00:11<00:47, 15.03it/s]#015 20%|██        | 181/897 [00:11<00:47, 14.96it/s]#015 20%|██        | 183/897 [00:12<00:46, 15.50it/s]#015 21%|██        | 185/897 [00:12<00:45, 15.60it/s]#015 21%|██        | 187/897 [00:12<00:46, 15.41it/s]#015 21%|██        | 189/897 [00:12<00:45, 15.42it/s]#015 21%|██▏       | 191/897 [00:12<00:45, 15.49it/s]#015 22%|██▏       | 193/897 [00:12<00:44, 15.87it/s]#015 22%|██▏       | 195/897 [00:12<00:44, 15.88it/s]#015 22%|██▏       | 197/897 [00:12<00:43, 16.13it/s]#015 22%|██▏       | 199/897 [00:13<00:44, 15.66it/s]#015 22%|██▏       | 201/897 [00:13<00:47, 14.71it/s]#015 23%|██▎       | 203/897 [00:13<00:45, 15.15it/s]#015 23%|██▎       | 205/897 [00:13<00:45, 15.31it/s]#015 23%|██▎       | 207/897 [00:13<00:45, 15.17it/s]#015 23%|██▎       | 209/897 [00:13<00:46, 14.76it/s]#015 24%|██▎       | 211/897 [00:13<00:47, 14.44it/s]#015 24%|██▎       | 213/897 [00:14<00:46, 14.75it/s]#015 24%|██▍       | 215/897 [00:14<00:46, 14.57it/s]#015 24%|██▍       | 217/897 [00:14<00:46, 14.67it/s]#015 24%|██▍       | 219/897 [00:14<00:45, 14.90it/s]#015 25%|██▍       | 221/897 [00:14<00:45, 15.00it/s]#015 25%|██▍       | 223/897 [00:14<00:45, 14.89it/s]#015 25%|██▌       | 225/897 [00:14<00:45, 14.89it/s]#015 25%|██▌       | 227/897 [00:14<00:45, 14.84it/s]#015 26%|██▌       | 229/897 [00:15<00:43, 15.29it/s]#015 26%|██▌       | 231/897 [00:15<00:42, 15.59it/s]#015 26%|██▌       | 233/897 [00:15<00:42, 15.76it/s]#015 26%|██▌       | 235/897 [00:15<00:43, 15.35it/s]#015 26%|██▋       | 237/897 [00:15<00:42, 15.63it/s]#015 27%|██▋       | 239/897 [00:15<00:43, 15.15it/s]#015 27%|██▋       | 241/897 [00:15<00:45, 14.29it/s]#015 27%|██▋       | 243/897 [00:16<00:44, 14.54it/s]#015 27%|██▋       | 245/897 [00:16<00:44, 14.69it/s]#015 28%|██▊       | 247/897 [00:16<00:43, 14.87it/s]#015 28%|██▊       | 249/897 [00:16<00:42, 15.28it/s]#015 28%|██▊       | 251/897 [00:16<00:42, 15.13it/s]#015 28%|██▊       | 253/897 [00:16<00:42, 15.03it/s]#015 28%|██▊       | 255/897 [00:16<00:42, 15.13it/s]#015 29%|██▊       | 257/897 [00:16<00:41, 15.49it/s]#015 29%|██▉       | 259/897 [00:17<00:40, 15.67it/s]#015 29%|██▉       | 261/897 [00:17<00:40, 15.59it/s]#015 29%|██▉       | 263/897 [00:17<00:41, 15.16it/s]#015 30%|██▉       | 265/897 [00:17<00:42, 15.03it/s]#015 30%|██▉       | 267/897 [00:17<00:40, 15.45it/s]#015 30%|██▉       | 269/897 [00:17<00:39, 15.71it/s]#015 30%|███       | 271/897 [00:17<00:41, 15.12it/s]#015 30%|███       | 273/897 [00:17<00:42, 14.79it/s]#015 31%|███       | 275/897 [00:18<00:40, 15.29it/s]#015 31%|███       | 277/897 [00:18<00:41, 14.92it/s]#015 31%|███       | 279/897 [00:18<00:41, 14.86it/s]#015 31%|███▏      | 281/897 [00:18<00:40, 15.31it/s]#015 32%|███▏      | 283/897 [00:18<00:38, 15.76it/s]#015 32%|███▏      | 285/897 [00:18<00:37, 16.14it/s]#015 32%|███▏      | 287/897 [00:18<00:37, 16.30it/s]#015 32%|███▏      | 289/897 [00:18<00:38, 15.68it/s]#015 32%|███▏      | 291/897 [00:19<00:37, 15.96it/s]#015 33%|███▎      | 293/897 [00:19<00:38, 15.79it/s]#015 33%|███▎      | 295/897 [00:19<00:38, 15.48it/s]#015 33%|███▎      | 297/897 [00:19<00:39, 15.11it/s]#015 33%|███▎      | 299/897 [00:19<00:38, 15.39it/s]#015 34%|███▎      | 301/897 [00:19<00:38, 15.34it/s]#015 34%|███▍      | 303/897 [00:19<00:40, 14.81it/s]#015 34%|███▍      | 305/897 [00:20<00:42, 14.06it/s]#015 34%|███▍      | 307/897 [00:20<00:41, 14.27it/s]#015 34%|███▍      | 309/897 [00:20<00:42, 13.81it/s]#015 35%|███▍      | 311/897 [00:20<00:41, 14.15it/s]#015 35%|███▍      | 313/897 [00:20<00:40, 14.31it/s]#015 35%|███▌      | 315/897 [00:20<00:39, 14.74it/s]#015 35%|███▌      | 317/897 [00:20<00:40, 14.20it/s]#015 36%|███▌      | 319/897 [00:21<00:42, 13.58it/s]#015 36%|███▌      | 321/897 [00:21<00:40, 14.35it/s]#015 36%|███▌      | 323/897 [00:21<00:38, 15.06it/s]#015 36%|███▌      | 325/897 [00:21<00:37, 15.10it/s]#015 36%|███▋      | 327/897 [00:21<00:38, 14.65it/s]#015 37%|███▋      | 329/897 [00:21<00:39, 14.33it/s]#015 37%|███▋      | 331/897 [00:21<00:39, 14.39it/s]#015 37%|███▋      | 333/897 [00:22<00:38, 14.56it/s]#015 37%|███▋      | 335/897 [00:22<00:38, 14.57it/s]#015 38%|███▊      | 337/897 [00:22<00:38, 14.65it/s]#015 38%|███▊      | 339/897 [00:22<00:37, 14.79it/s]#015 38%|███▊      | 341/897 [00:22<00:36, 15.05it/s]#015 38%|███▊      | 343/897 [00:22<00:36, 15.15it/s]#015 38%|███▊      | 345/897 [00:22<00:37, 14.91it/s]#015 39%|███▊      | 347/897 [00:22<00:36, 14.98it/s]#015 39%|███▉      | 349/897 [00:23<00:36, 14.85it/s]#015 39%|███▉      | 351/897 [00:23<00:39, 13.99it/s]#015 39%|███▉      | 353/897 [00:23<00:40, 13.29it/s]#015 40%|███▉      | 355/897 [00:23<00:39, 13.57it/s]#015 40%|███▉      | 357/897 [00:23<00:38, 14.01it/s]#015 40%|████      | 359/897 [00:23<00:37, 14.42it/s]#015 40%|████      | 361/897 [00:23<00:36, 14.85it/s]#015 40%|████      | 363/897 [00:24<00:35, 14.86it/s]#015 41%|████      | 365/897 [00:24<00:35, 15.18it/s]#015 41%|████      | 367/897 [00:24<00:37, 14.31it/s]#015 41%|████      | 369/897 [00:24<00:36, 14.53it/s]#015 41%|████▏     | 371/897 [00:24<00:34, 15.05it/s]#015 42%|████▏     | 373/897 [00:24<00:34, 15.16it/s]#015 42%|████▏     | 375/897 [00:24<00:34, 15.22it/s]#015 42%|████▏     | 377/897 [00:25<00:34, 15.19it/s]#015 42%|████▏     | 379/897 [00:25<00:35, 14.78it/s]#015 42%|████▏     | 381/897 [00:25<00:37, 13.92it/s]#015 43%|████▎     | 383/897 [00:25<00:35, 14.56it/s]#015 43%|████▎     | 385/897 [00:25<00:36, 14.11it/s]#015 43%|████▎     | 387/897 [00:25<00:35, 14.54it/s]#015 43%|████▎     | 389/897 [00:25<00:34, 14.55it/s]#015 44%|████▎     | 391/897 [00:25<00:33, 15.14it/s]#015 44%|████▍     | 393/897 [00:26<00:33, 15.06it/s]#015 44%|████▍     | 395/897 [00:26<00:33, 15.05it/s]#015 44%|████▍     | 397/897 [00:26<00:32, 15.36it/s]#015 44%|████▍     | 399/897 [00:26<00:31, 15.77it/s]#015 45%|████▍     | 401/897 [00:26<00:32, 15.27it/s]#015 45%|████▍     | 403/897 [00:26<00:32, 15.01it/s]#015 45%|████▌     | 405/897 [00:26<00:31, 15.56it/s]#015 45%|████▌     | 407/897 [00:27<00:31, 15.58it/s]#015 46%|████▌     | 409/897 [00:27<00:31, 15.56it/s]#015 46%|████▌     | 411/897 [00:27<00:30, 15.83it/s]#015 46%|████▌     | 413/897 [00:27<00:30, 15.82it/s]#015 46%|████▋     | 415/897 [00:27<00:31, 15.30it/s]#015 46%|████▋     | 417/897 [00:27<00:31, 15.18it/s]#015 47%|████▋     | 419/897 [00:27<00:31, 15.36it/s]#015 47%|████▋     | 421/897 [00:27<00:30, 15.70it/s]#015 47%|████▋     | 423/897 [00:28<00:30, 15.75it/s]#015 47%|████▋     | 425/897 [00:28<00:30, 15.63it/s]#015 48%|████▊     | 427/897 [00:28<00:29, 15.93it/s]#015 48%|████▊     | 429/897 [00:28<00:30, 15.56it/s]#015 48%|████▊     | 431/897 [00:28<00:30, 15.53it/s]#015 48%|████▊     | 433/897 [00:28<00:29, 15.63it/s]#015 48%|████▊     | 435/897 [00:28<00:30, 15.22it/s]#015 49%|████▊     | 437/897 [00:28<00:30, 15.22it/s]#015 49%|████▉     | 439/897 [00:29<00:29, 15.32it/s]#015 49%|████▉     | 441/897 [00:29<00:29, 15.27it/s]#015 49%|████▉     | 443/897 [00:29<00:32, 14.01it/s]#015 50%|████▉     | 445/897 [00:29<00:32, 13.72it/s]#015 50%|████▉     | 447/897 [00:29<00:33, 13.33it/s]#015 50%|█████     | 449/897 [00:29<00:32, 13.95it/s]#015 50%|█████     | 451/897 [00:29<00:31, 14.26it/s]#015 51%|█████     | 453/897 [00:30<00:29, 14.86it/s]#015 51%|█████     | 455/897 [00:30<00:30, 14.34it/s]#015 51%|█████     | 457/897 [00:30<00:29, 15.03it/s]#015 51%|█████     | 459/897 [00:30<00:28, 15.49it/s]#015 51%|█████▏    | 461/897 [00:30<00:27, 15.96it/s]#015 52%|█████▏    | 463/897 [00:30<00:26, 16.32it/s]#015 52%|█████▏    | 465/897 [00:30<00:27, 15.97it/s]#015 52%|█████▏    | 467/897 [00:30<00:27, 15.83it/s]#015 52%|█████▏    | 469/897 [00:31<00:26, 16.10it/s]#015 53%|█████▎    | 471/897 [00:31<00:27, 15.43it/s]#015 53%|█████▎    | 473/897 [00:31<00:29, 14.38it/s]#015 53%|█████▎    | 475/897 [00:31<00:28, 14.74it/s]#015 53%|█████▎    | 477/897 [00:31<00:27, 15.39it/s]#015 53%|█████▎    | 479/897 [00:31<00:27, 15.41it/s]#015 54%|█████▎    | 481/897 [00:31<00:27, 15.02it/s]#015 54%|█████▍    | 483/897 [00:32<00:27, 15.14it/s]#015 54%|█████▍    | 485/897 [00:32<00:27, 14.74it/s]#015 54%|█████▍    | 487/897 [00:32<00:28, 14.36it/s]#015 55%|█████▍    | 489/897 [00:32<00:28, 14.52it/s]#015 55%|█████▍    | 491/897 [00:32<00:27, 14.59it/s]#015 55%|█████▍    | 493/897 [00:32<00:27, 14.74it/s]#015 55%|█████▌    | 495/897 [00:32<00:27, 14.58it/s]#015 55%|█████▌    | 497/897 [00:32<00:26, 14.97it/s]#015 56%|█████▌    | 499/897 [00:33<00:25, 15.36it/s]#015 56%|█████▌    | 501/897 [00:33<00:25, 15.44it/s]#015 56%|█████▌    | 503/897 [00:33<00:25, 15.74it/s]#015 56%|█████▋    | 505/897 [00:33<00:25, 15.32it/s]#015 57%|█████▋    | 507/897 [00:33<00:27, 14.05it/s]#015 57%|█████▋    | 509/897 [00:33<00:27, 13.90it/s]#015 57%|█████▋    | 511/897 [00:33<00:26, 14.49it/s]#015 57%|█████▋    | 513/897 [00:34<00:25, 14.87it/s]#015 57%|█████▋    | 515/897 [00:34<00:26, 14.65it/s]#015 58%|█████▊    | 517/897 [00:34<00:25, 15.03it/s]#015 58%|█████▊    | 519/897 [00:34<00:25, 14.94it/s]#015 58%|█████▊    | 521/897 [00:34<00:24, 15.26it/s]#015 58%|█████▊    | 523/897 [00:34<00:23, 15.69it/s]#015 59%|█████▊    | 525/897 [00:34<00:23, 15.95it/s]#015 59%|█████▉    | 527/897 [00:34<00:23, 15.48it/s]#015 59%|█████▉    | 529/897 [00:35<00:24, 14.95it/s]#015 59%|█████▉    | 531/897 [00:35<00:25, 14.49it/s]#015 59%|█████▉    | 533/897 [00:35<00:25, 14.47it/s]#015 60%|█████▉    | 535/897 [00:35<00:24, 14.57it/s]#015 60%|█████▉    | 537/897 [00:35<00:24, 14.56it/s]#015 60%|██████    | 539/897 [00:35<00:24, 14.42it/s]#015 60%|██████    | 541/897 [00:35<00:23, 14.96it/s]#015 61%|██████    | 543/897 [00:36<00:22, 15.46it/s]#015 61%|██████    | 545/897 [00:36<00:22, 15.53it/s]#015 61%|██████    | 547/897 [00:36<00:23, 15.00it/s]#015 61%|██████    | 549/897 [00:36<00:23, 14.66it/s]#015 61%|██████▏   | 551/897 [00:36<00:23, 14.42it/s]#015 62%|██████▏   | 553/897 [00:36<00:23, 14.45it/s]#015 62%|██████▏   | 555/897 [00:36<00:23, 14.55it/s]#015 62%|██████▏   | 557/897 [00:37<00:24, 14.05it/s]#015 62%|██████▏   | 559/897 [00:37<00:24, 13.82it/s]#015 63%|██████▎   | 561/897 [00:37<00:24, 13.51it/s]#015 63%|██████▎   | 563/897 [00:37<00:24, 13.72it/s]#015 63%|██████▎   | 565/897 [00:37<00:24, 13.51it/s]#015 63%|██████▎   | 567/897 [00:37<00:23, 14.02it/s]#015 63%|██████▎   | 569/897 [00:37<00:22, 14.57it/s]#015 64%|██████▎   | 571/897 [00:38<00:22, 14.78it/s]#015 64%|██████▍   | 573/897 [00:38<00:21, 15.33it/s]#015 64%|██████▍   | 575/897 [\u001b[0m\n",
      "\u001b[34m00:38<00:20, 15.64it/s]#015 64%|██████▍   | 577/897 [00:38<00:20, 15.43it/s]#015 65%|██████▍   | 579/897 [00:38<00:20, 15.64it/s]#015 65%|██████▍   | 581/897 [00:38<00:20, 15.10it/s]#015 65%|██████▍   | 583/897 [00:38<00:20, 14.99it/s]#015 65%|██████▌   | 585/897 [00:38<00:20, 15.48it/s]#015 65%|██████▌   | 587/897 [00:39<00:19, 15.54it/s]#015 66%|██████▌   | 589/897 [00:39<00:20, 15.25it/s]#015 66%|██████▌   | 591/897 [00:39<00:20, 15.21it/s]#015 66%|██████▌   | 593/897 [00:39<00:19, 15.56it/s]#015 66%|██████▋   | 595/897 [00:39<00:19, 15.86it/s]#015 67%|██████▋   | 597/897 [00:39<00:18, 16.06it/s]#015 67%|██████▋   | 599/897 [00:39<00:19, 15.47it/s]#015 67%|██████▋   | 601/897 [00:39<00:20, 14.46it/s]#015 67%|██████▋   | 603/897 [00:40<00:19, 14.78it/s]#015 67%|██████▋   | 605/897 [00:40<00:19, 14.86it/s]#015 68%|██████▊   | 607/897 [00:40<00:19, 15.09it/s]#015 68%|██████▊   | 609/897 [00:40<00:19, 15.04it/s]#015 68%|██████▊   | 611/897 [00:40<00:18, 15.48it/s]#015 68%|██████▊   | 613/897 [00:40<00:18, 15.28it/s]#015 69%|██████▊   | 615/897 [00:40<00:18, 14.92it/s]#015 69%|██████▉   | 617/897 [00:41<00:18, 15.10it/s]#015 69%|██████▉   | 619/897 [00:41<00:18, 14.96it/s]#015 69%|██████▉   | 621/897 [00:41<00:18, 15.30it/s]#015 69%|██████▉   | 623/897 [00:41<00:17, 15.44it/s]#015 70%|██████▉   | 625/897 [00:41<00:17, 15.77it/s]#015 70%|██████▉   | 627/897 [00:41<00:17, 15.88it/s]#015 70%|███████   | 629/897 [00:41<00:16, 16.03it/s]#015 70%|███████   | 631/897 [00:41<00:16, 15.85it/s]#015 71%|███████   | 633/897 [00:42<00:17, 15.16it/s]#015 71%|███████   | 635/897 [00:42<00:17, 15.06it/s]#015 71%|███████   | 637/897 [00:42<00:16, 15.44it/s]#015 71%|███████   | 639/897 [00:42<00:16, 15.60it/s]#015 71%|███████▏  | 641/897 [00:42<00:16, 15.42it/s]#015 72%|███████▏  | 643/897 [00:42<00:16, 15.25it/s]#015 72%|███████▏  | 645/897 [00:42<00:16, 15.26it/s]#015 72%|███████▏  | 647/897 [00:42<00:16, 15.56it/s]#015 72%|███████▏  | 649/897 [00:43<00:15, 15.81it/s]#015 73%|███████▎  | 651/897 [00:43<00:15, 15.90it/s]#015 73%|███████▎  | 653/897 [00:43<00:15, 15.99it/s]#015 73%|███████▎  | 655/897 [00:43<00:15, 15.96it/s]#015 73%|███████▎  | 657/897 [00:43<00:15, 15.80it/s]#015 73%|███████▎  | 659/897 [00:43<00:15, 15.73it/s]#015 74%|███████▎  | 661/897 [00:43<00:15, 15.38it/s]#015 74%|███████▍  | 663/897 [00:43<00:15, 14.99it/s]#015 74%|███████▍  | 665/897 [00:44<00:15, 14.58it/s]#015 74%|███████▍  | 667/897 [00:44<00:15, 15.00it/s]#015 75%|███████▍  | 669/897 [00:44<00:15, 15.07it/s]#015 75%|███████▍  | 671/897 [00:44<00:15, 15.06it/s]#015 75%|███████▌  | 673/897 [00:44<00:14, 15.36it/s]#015 75%|███████▌  | 675/897 [00:44<00:14, 15.33it/s]#015 75%|███████▌  | 677/897 [00:44<00:14, 15.12it/s]#015 76%|███████▌  | 679/897 [00:45<00:14, 15.11it/s]#015 76%|███████▌  | 681/897 [00:45<00:13, 15.43it/s]#015 76%|███████▌  | 683/897 [00:45<00:13, 15.85it/s]#015 76%|███████▋  | 685/897 [00:45<00:13, 16.03it/s]#015 77%|███████▋  | 687/897 [00:45<00:12, 16.18it/s]#015 77%|███████▋  | 689/897 [00:45<00:13, 15.77it/s]#015 77%|███████▋  | 691/897 [00:45<00:13, 15.36it/s]#015 77%|███████▋  | 693/897 [00:45<00:14, 14.21it/s]#015 77%|███████▋  | 695/897 [00:46<00:14, 14.28it/s]#015 78%|███████▊  | 697/897 [00:46<00:13, 14.76it/s]#015 78%|███████▊  | 699/897 [00:46<00:13, 15.14it/s]#015 78%|███████▊  | 701/897 [00:46<00:12, 15.20it/s]#015 78%|███████▊  | 703/897 [00:46<00:12, 15.07it/s]#015 79%|███████▊  | 705/897 [00:46<00:12, 15.81it/s]#015 79%|███████▉  | 707/897 [00:46<00:11, 16.11it/s]#015 79%|███████▉  | 709/897 [00:46<00:11, 16.16it/s]#015 79%|███████▉  | 711/897 [00:47<00:11, 15.91it/s]#015 79%|███████▉  | 713/897 [00:47<00:11, 15.66it/s]#015 80%|███████▉  | 715/897 [00:47<00:11, 15.68it/s]#015 80%|███████▉  | 717/897 [00:47<00:11, 15.93it/s]#015 80%|████████  | 719/897 [00:47<00:11, 15.53it/s]#015 80%|████████  | 721/897 [00:47<00:11, 15.96it/s]#015 81%|████████  | 723/897 [00:47<00:11, 15.58it/s]#015 81%|████████  | 725/897 [00:47<00:10, 16.07it/s]#015 81%|████████  | 727/897 [00:48<00:10, 15.79it/s]#015 81%|████████▏ | 729/897 [00:48<00:10, 16.00it/s]#015 81%|████████▏ | 731/897 [00:48<00:10, 16.24it/s]#015 82%|████████▏ | 733/897 [00:48<00:10, 16.11it/s]#015 82%|████████▏ | 735/897 [00:48<00:10, 16.08it/s]#015 82%|████████▏ | 737/897 [00:48<00:10, 14.72it/s]#015 82%|████████▏ | 739/897 [00:48<00:11, 13.95it/s]#015 83%|████████▎ | 741/897 [00:49<00:10, 14.46it/s]#015 83%|████████▎ | 743/897 [00:49<00:10, 15.13it/s]#015 83%|████████▎ | 745/897 [00:49<00:10, 15.10it/s]#015 83%|████████▎ | 747/897 [00:49<00:10, 14.60it/s]#015 84%|████████▎ | 749/897 [00:49<00:09, 15.16it/s]#015 84%|████████▎ | 751/897 [00:49<00:09, 15.55it/s]#015 84%|████████▍ | 753/897 [00:49<00:09, 15.80it/s]#015 84%|████████▍ | 755/897 [00:49<00:08, 16.15it/s]#015 84%|████████▍ | 757/897 [00:50<00:08, 16.11it/s]#015 85%|████████▍ | 759/897 [00:50<00:08, 16.31it/s]#015 85%|████████▍ | 761/897 [00:50<00:08, 16.16it/s]#015 85%|████████▌ | 763/897 [00:50<00:08, 16.20it/s]#015 85%|████████▌ | 765/897 [00:50<00:08, 16.31it/s]#015 86%|████████▌ | 767/897 [00:50<00:08, 16.24it/s]#015 86%|████████▌ | 769/897 [00:50<00:07, 16.12it/s]#015 86%|████████▌ | 771/897 [00:50<00:07, 16.22it/s]#015 86%|████████▌ | 773/897 [00:51<00:07, 15.98it/s]#015 86%|████████▋ | 775/897 [00:51<00:07, 15.54it/s]#015 87%|████████▋ | 777/897 [00:51<00:07, 15.65it/s]#015 87%|████████▋ | 779/897 [00:51<00:07, 14.94it/s]#015 87%|████████▋ | 781/897 [00:51<00:07, 15.30it/s]#015 87%|████████▋ | 783/897 [00:51<00:07, 15.69it/s]#015 88%|████████▊ | 785/897 [00:51<00:07, 15.83it/s]#015 88%|████████▊ | 787/897 [00:51<00:07, 15.64it/s]#015 88%|████████▊ | 789/897 [00:52<00:06, 15.73it/s]#015 88%|████████▊ | 791/897 [00:52<00:06, 15.79it/s]#015 88%|████████▊ | 793/897 [00:52<00:06, 15.26it/s]#015 89%|████████▊ | 795/897 [00:52<00:07, 14.44it/s]#015 89%|████████▉ | 797/897 [00:52<00:06, 14.40it/s]#015 89%|████████▉ | 799/897 [00:52<00:06, 14.84it/s]#015 89%|████████▉ | 801/897 [00:52<00:06, 15.31it/s]#015 90%|████████▉ | 803/897 [00:53<00:06, 15.14it/s]#015 90%|████████▉ | 805/897 [00:53<00:05, 15.39it/s]#015 90%|████████▉ | 807/897 [00:53<00:05, 15.65it/s]#015 90%|█████████ | 809/897 [00:53<00:05, 15.73it/s]#015 90%|█████████ | 811/897 [00:53<00:05, 15.26it/s]#015 91%|█████████ | 813/897 [00:53<00:05, 14.31it/s]#015 91%|█████████ | 815/897 [00:53<00:05, 14.07it/s]#015 91%|█████████ | 817/897 [00:53<00:05, 14.51it/s]#015 91%|█████████▏| 819/897 [00:54<00:05, 14.90it/s]#015 92%|█████████▏| 821/897 [00:54<00:05, 15.17it/s]#015 92%|█████████▏| 823/897 [00:54<00:04, 15.27it/s]#015 92%|█████████▏| 825/897 [00:54<00:04, 15.30it/s]#015 92%|█████████▏| 827/897 [00:54<00:04, 15.44it/s]#015 92%|█████████▏| 829/897 [00:54<00:04, 15.47it/s]#015 93%|█████████▎| 831/897 [00:54<00:04, 15.69it/s]#015 93%|█████████▎| 833/897 [00:54<00:04, 15.71it/s]#015 93%|█████████▎| 835/897 [00:55<00:03, 15.80it/s]#015 93%|█████████▎| 837/897 [00:55<00:03, 15.71it/s]#015 94%|█████████▎| 839/897 [00:55<00:03, 15.17it/s]#015 94%|█████████▍| 841/897 [00:55<00:03, 15.51it/s]#015 94%|█████████▍| 843/897 [00:55<00:03, 15.32it/s]#015 94%|█████████▍| 845/897 [00:55<00:03, 15.15it/s]#015 94%|█████████▍| 847/897 [00:55<00:03, 15.22it/s]#015 95%|█████████▍| 849/897 [00:56<00:03, 15.48it/s]#015 95%|█████████▍| 851/897 [00:56<00:02, 15.54it/s]#015 95%|█████████▌| 853/897 [00:56<00:02, 15.56it/s]#015 95%|█████████▌| 855/897 [00:56<00:02, 15.57it/s]#015 96%|█████████▌| 857/897 [00:56<00:02, 15.69it/s]#015 96%|█████████▌| 859/897 [00:56<00:02, 15.81it/s]#015 96%|█████████▌| 861/897 [00:56<00:02, 15.89it/s]#015 96%|█████████▌| 863/897 [00:56<00:02, 15.49it/s]#015 96%|█████████▋| 865/897 [00:57<00:02, 15.98it/s]#015 97%|█████████▋| 867/897 [00:57<00:01, 16.13it/s]#015 97%|█████████▋| 869/897 [00:57<00:01, 16.51it/s]#015 97%|█████████▋| 871/897 [00:57<00:01, 16.62it/s]#015 97%|█████████▋| 873/897 [00:57<00:01, 16.31it/s]#015 98%|█████████▊| 875/897 [00:57<00:01, 16.30it/s]#015 98%|█████████▊| 877/897 [00:57<00:01, 16.41it/s]#015 98%|█████████▊| 879/897 [00:57<00:01, 15.07it/s]#015 98%|█████████▊| 881/897 [00:58<00:01, 14.08it/s]#015 98%|█████████▊| 883/897 [00:58<00:01, 13.51it/s]#015 99%|█████████▊| 885/897 [00:58<00:00, 13.44it/s]#015 99%|█████████▉| 887/897 [00:58<00:00, 13.84it/s]#015 99%|█████████▉| 889/897 [00:58<00:00, 14.35it/s]#015 99%|█████████▉| 891/897 [00:58<00:00, 14.46it/s]#015100%|█████████▉| 893/897 [00:58<00:00, 15.14it/s]#015100%|█████████▉| 895/897 [00:59<00:00, 15.32it/s]#015100%|██████████| 897/897 [00:59<00:00, 15.35it/s]#015100%|██████████| 897/897 [00:59<00:00, 15.16it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:907] 2022-05-09 08:45:28,075 >> ***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,075 >>   epoch                     =        1.0\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,075 >>   eval_accuracy             =     0.5469\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_loss                 =     2.1974\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_mem_cpu_alloc_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_mem_cpu_peaked_delta =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_mem_gpu_alloc_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_mem_gpu_peaked_delta =       38MB\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_runtime              = 0:00:59.26\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_samples              =       7169\u001b[0m\n",
      "\u001b[34m[INFO|trainer_pt_utils.py:912] 2022-05-09 08:45:28,076 >>   eval_samples_per_second   =    120.968\u001b[0m\n",
      "\n",
      "2022-05-09 08:49:09 Completed - Training job completed\n",
      "Training seconds: 634\n",
      "Billable seconds: 634\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(\n",
    "  {'train': data_location+'/100014.csv',\n",
    "   'test': data_location+'/100014.csv',\n",
    "  'last_model': data_location2}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
